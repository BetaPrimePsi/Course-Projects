{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BetaPrimePsi/BetaPrimePsi/blob/main/CS686_Assignment_4_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odj1Coq5H080"
      },
      "outputs": [],
      "source": [
        "#@title ##### License { display-mode: \"form\" }\n",
        "# Copyright 2019 DeepMind Technologies Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOOzDGYAZcW3"
      },
      "source": [
        "# OpenSpiel: RRPS Example\n",
        "\n",
        "* This Colab gets you started with installing OpenSpiel and its dependencies.\n",
        "* OpenSpiel is a framework for reinforcement learning in games.\n",
        "* For a longer intro to OpenSpiel, see [the tutorial video](https://www.youtube.com/watch?v=8NCPqtPwlFQ), [documentation](https://openspiel.readthedocs.io/en/latest/), or [API reference](https://openspiel.readthedocs.io/en/latest/api_reference.html).\n",
        "* This colab also includes examples of how to get started with the Roshambo/RRPS environment and bots. It is based on [roshambo_population_example.py](https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/examples/roshambo_population_example.py)\n",
        "* For more info on Roshambo, see the [RRPS benchmark paper](https://openreview.net/pdf?id=gQnJ7ODIAx)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC6kQBzWahEF"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2_Vbijh4FlZ"
      },
      "source": [
        "Install OpenSpiel via pip:\n",
        "\n",
        "Note that if you are not using a colab, then you would use\n",
        "python3 -m pip install open_spiel\n",
        "Additional information about installing OpenSpiel can be found at the links above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQc12Xrn4CXU",
        "outputId": "c7c5a70b-5975-4745-a0d7-ff76f496432c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting open_spiel\n",
            "  Downloading open_spiel-1.6.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: pip>=20.0.2 in /usr/local/lib/python3.12/dist-packages (from open_spiel) (24.1.2)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.12/dist-packages (from open_spiel) (25.4.0)\n",
            "Requirement already satisfied: absl-py>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from open_spiel) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.21.5 in /usr/local/lib/python3.12/dist-packages (from open_spiel) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.12/dist-packages (from open_spiel) (1.16.3)\n",
            "Collecting ml-collections>=0.1.1 (from open_spiel)\n",
            "  Downloading ml_collections-1.1.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from ml-collections>=0.1.1->open_spiel) (6.0.3)\n",
            "Downloading open_spiel-1.6.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_collections-1.1.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml-collections, open_spiel\n",
            "Successfully installed ml-collections-1.1.0 open_spiel-1.6.9\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade open_spiel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUtlXZ8FBnAL"
      },
      "source": [
        "# Simple example of OpenSpiel API: uniform random trajectory on Tic-Tac-Toe\n",
        "\n",
        "This example is not used for RRPS, but it shows how to load a game in OpenSpiel, how to access a game state, and implements a random strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewMXCaUw8d9Q",
        "outputId": "c6bdf8c6-f28b-4a71-b7e6-d8d2335f64b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "...\n",
            "...\n",
            "x..\n",
            "\n",
            ".o.\n",
            "...\n",
            "x..\n",
            "\n",
            ".o.\n",
            ".x.\n",
            "x..\n",
            "\n",
            ".o.\n",
            ".x.\n",
            "xo.\n",
            "\n",
            ".o.\n",
            "xx.\n",
            "xo.\n",
            "\n",
            "oo.\n",
            "xx.\n",
            "xo.\n",
            "\n",
            "oo.\n",
            "xx.\n",
            "xox\n",
            "\n",
            "oo.\n",
            "xxo\n",
            "xox\n",
            "\n",
            "oox\n",
            "xxo\n",
            "xox\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pyspiel\n",
        "\n",
        "game = pyspiel.load_game(\"tic_tac_toe\")\n",
        "state = game.new_initial_state()\n",
        "\n",
        "while not state.is_terminal():\n",
        "  state.apply_action(np.random.choice(state.legal_actions()))\n",
        "  print(str(state) + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2hLkPIHLjjV"
      },
      "source": [
        "# Getting started with RRPS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF2eZXwmMJsa"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "\n",
        "from open_spiel.python import rl_agent\n",
        "from open_spiel.python import rl_environment\n",
        "import pyspiel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SavmNn1MG-N"
      },
      "outputs": [],
      "source": [
        "# Some helper classes and functions.\n",
        "# DO NOT CHANGE.\n",
        "\n",
        "class BotAgent(rl_agent.AbstractAgent):\n",
        "  \"\"\"Agent class that wraps a bot.\n",
        "\n",
        "  Note, the environment must include the OpenSpiel state in its observations,\n",
        "  which means it must have been created with use_full_state=True.\n",
        "\n",
        "  This is a simple wrapper that lets the RPS bots be interpreted as agents under\n",
        "  the RL API.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_actions, bot, name=\"bot_agent\"):\n",
        "    assert num_actions > 0\n",
        "    self._bot = bot\n",
        "    self._num_actions = num_actions\n",
        "\n",
        "  def restart(self):\n",
        "    self._bot.restart()\n",
        "\n",
        "  def step(self, time_step, is_evaluation=False):\n",
        "    # If it is the end of the episode, don't select an action.\n",
        "    if time_step.last():\n",
        "      return\n",
        "    _, state = pyspiel.deserialize_game_and_state(\n",
        "        time_step.observations[\"serialized_state\"])\n",
        "    action = self._bot.step(state)\n",
        "    probs = np.zeros(self._num_actions)\n",
        "    probs[action] = 1.0\n",
        "    return rl_agent.StepOutput(action=action, probs=probs)\n",
        "\n",
        "\n",
        "#  We will use this function to evaluate the agents. Do not change.\n",
        "\n",
        "def eval_agents(env, agents, num_players, num_episodes, verbose=False):\n",
        "  \"\"\"Evaluate the agent.\n",
        "\n",
        "  Runs a number of episodes and returns the average returns for each agent as\n",
        "  a numpy array.\n",
        "\n",
        "  Arguments:\n",
        "    env: the RL environment,\n",
        "    agents: a list of agents (size 2),\n",
        "    num_players: number of players in the game (for RRPS, this is 2),\n",
        "    num_episodes: number of evaluation episodes to run.\n",
        "    verbose: whether to print updates after each episode.\n",
        "  \"\"\"\n",
        "  sum_episode_rewards = np.zeros(num_players)\n",
        "  for ep in range(num_episodes):\n",
        "    for agent in agents:\n",
        "      # Bots need to be restarted at the start of the episode.\n",
        "      if hasattr(agent, \"restart\"):\n",
        "        agent.restart()\n",
        "    time_step = env.reset()\n",
        "    episode_rewards = np.zeros(num_players)\n",
        "    while not time_step.last():\n",
        "      agents_output = [\n",
        "          agent.step(time_step, is_evaluation=True) for agent in agents\n",
        "      ]\n",
        "      action_list = [agent_output.action for agent_output in agents_output]\n",
        "      time_step = env.step(action_list)\n",
        "      episode_rewards += time_step.rewards\n",
        "    sum_episode_rewards += episode_rewards\n",
        "    if verbose:\n",
        "      print(f\"Finished episode {ep}, \"\n",
        "            + f\"avg returns: {sum_episode_rewards / (ep+1)}\")\n",
        "\n",
        "  return sum_episode_rewards / num_episodes\n",
        "\n",
        "\n",
        "def print_roshambo_bot_names_and_ids(roshambo_bot_names):\n",
        "  print(\"Roshambo bot population:\")\n",
        "  for i in range(len(roshambo_bot_names)):\n",
        "    print(f\"{i}: {roshambo_bot_names[i]}\")\n",
        "\n",
        "def create_roshambo_bot_agent(player_id, num_actions, bot_names, pop_id):\n",
        "  name = bot_names[pop_id]\n",
        "  # Creates an OpenSpiel bot with the default number of throws\n",
        "  # (pyspiel.ROSHAMBO_NUM_THROWS). To create one for a different number of\n",
        "  # throws per episode, add the number as the third argument here.\n",
        "  bot = pyspiel.make_roshambo_bot(player_id, name)\n",
        "  return BotAgent(num_actions, bot, name=name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FZlLow5w5da"
      },
      "source": [
        "#The following functions are used to load the bots from the original RRPS competition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zimu2jMiNuYf",
        "outputId": "f897c2a6-7dde-4601-a942-77b5d3765c6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading bot population...\n",
            "Population size: 43\n",
            "Roshambo bot population:\n",
            "0: actr_lag2_decay\n",
            "1: adddriftbot2\n",
            "2: addshiftbot3\n",
            "3: antiflatbot\n",
            "4: antirotnbot\n",
            "5: biopic\n",
            "6: boom\n",
            "7: copybot\n",
            "8: debruijn81\n",
            "9: driftbot\n",
            "10: flatbot3\n",
            "11: foxtrotbot\n",
            "12: freqbot2\n",
            "13: granite\n",
            "14: greenberg\n",
            "15: halbot\n",
            "16: inocencio\n",
            "17: iocainebot\n",
            "18: marble\n",
            "19: markov5\n",
            "20: markovbails\n",
            "21: mixed_strategy\n",
            "22: mod1bot\n",
            "23: multibot\n",
            "24: peterbot\n",
            "25: phasenbott\n",
            "26: pibot\n",
            "27: piedra\n",
            "28: predbot\n",
            "29: r226bot\n",
            "30: randbot\n",
            "31: robertot\n",
            "32: rockbot\n",
            "33: rotatebot\n",
            "34: russrocker4\n",
            "35: shofar\n",
            "36: sunCrazybot\n",
            "37: sunNervebot\n",
            "38: sweetrock\n",
            "39: switchalot\n",
            "40: switchbot\n",
            "41: textbot\n",
            "42: zq_move\n"
          ]
        }
      ],
      "source": [
        "# Some basic info and initialize the population\n",
        "\n",
        "# print(pyspiel.ROSHAMBO_NUM_BOTS)    # 43 bots\n",
        "# print(pyspiel.ROSHAMBO_NUM_THROWS)  # 1000 steps per episode\n",
        "\n",
        "# The recall is how many of the most recent actions are presented to the RL\n",
        "# agents as part of their observations. Note: this is just for the RL agents\n",
        "# like DQN etc... every bot has access to the full history.\n",
        "RECALL = 20\n",
        "\n",
        "# The population of 43 bots. See the RRPS paper for high-level descriptions of\n",
        "# what each bot does.\n",
        "\n",
        "print(\"Loading bot population...\")\n",
        "pop_size = pyspiel.ROSHAMBO_NUM_BOTS\n",
        "print(f\"Population size: {pop_size}\")\n",
        "roshambo_bot_names = pyspiel.roshambo_bot_names()\n",
        "roshambo_bot_names.sort()\n",
        "print_roshambo_bot_names_and_ids(roshambo_bot_names)\n",
        "\n",
        "bot_id = 0\n",
        "roshambo_bot_ids = {}\n",
        "for name in roshambo_bot_names:\n",
        "  roshambo_bot_ids[name] = bot_id\n",
        "  bot_id += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_qAMW6p5btS"
      },
      "source": [
        "#Example showing how to load to agents from the RRPS bot population and evalute them against each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-u0LlkiOCD6",
        "outputId": "b2059c50-3473-4a8d-ec3b-20edf613cae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting eval run.\n",
            "Finished episode 0, avg returns: [ 39. -39.]\n",
            "Finished episode 1, avg returns: [ 22. -22.]\n",
            "Finished episode 2, avg returns: [ 20.33333333 -20.33333333]\n",
            "Finished episode 3, avg returns: [ 18. -18.]\n",
            "Finished episode 4, avg returns: [ 11.6 -11.6]\n",
            "Finished episode 5, avg returns: [ 12.5 -12.5]\n",
            "Finished episode 6, avg returns: [ 20.42857143 -20.42857143]\n",
            "Finished episode 7, avg returns: [ 15.875 -15.875]\n",
            "Finished episode 8, avg returns: [ 12.33333333 -12.33333333]\n",
            "Finished episode 9, avg returns: [ 12.1 -12.1]\n",
            "Avg return  [ 12.1 -12.1]\n"
          ]
        }
      ],
      "source": [
        "# Example: create an RL environment, and two agents from the bot population and\n",
        "# evaluate these two agents head-to-head.\n",
        "\n",
        "# Note that the include_full_state variable has to be enabled because the\n",
        "# BotAgent needs access to the full state.\n",
        "env = rl_environment.Environment(\n",
        "    \"repeated_game(stage_game=matrix_rps(),num_repetitions=\" +\n",
        "    f\"{pyspiel.ROSHAMBO_NUM_THROWS},\" +\n",
        "    f\"recall={RECALL})\",\n",
        "    include_full_state=True)\n",
        "num_players = 2\n",
        "num_actions = env.action_spec()[\"num_actions\"]\n",
        "# Learning agents might need this:\n",
        "info_state_size = env.observation_spec()[\"info_state\"][0]\n",
        "\n",
        "# Create two bot agents\n",
        "p0_pop_id = 0   # actr_lag2_decay\n",
        "p1_pop_id = 1   # adddriftbot2\n",
        "agents = [\n",
        "    create_roshambo_bot_agent(0, num_actions, roshambo_bot_names, p0_pop_id),\n",
        "    create_roshambo_bot_agent(1, num_actions, roshambo_bot_names, p1_pop_id)\n",
        "]\n",
        "\n",
        "print(\"Starting eval run.\")\n",
        "avg_eval_returns = eval_agents(env, agents, num_players, 10, verbose=True)\n",
        "\n",
        "print(\"Avg return \", avg_eval_returns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioOm4Wfvp1G5"
      },
      "source": [
        "# Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsATKfVsqLJi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def collect_rrps_data(env, bot_names, episodes_per_pair=2):\n",
        "    trajectories = []\n",
        "    pop_size = len(bot_names)\n",
        "\n",
        "    for i in range(pop_size):\n",
        "        for j in range(pop_size):\n",
        "            # Create agents for both bots\n",
        "            ag0 = create_roshambo_bot_agent(0, num_actions, bot_names, i)\n",
        "            ag1 = create_roshambo_bot_agent(1, num_actions, bot_names, j)\n",
        "            agents = [ag0, ag1]\n",
        "\n",
        "            for ep in range(episodes_per_pair):\n",
        "                for ag in agents:\n",
        "                    if hasattr(ag, \"restart\"):\n",
        "                        ag.restart()\n",
        "\n",
        "                ts = env.reset()\n",
        "                ep_hist = []\n",
        "\n",
        "                while not ts.last():\n",
        "                    outputs = [ag.step(ts) for ag in agents]\n",
        "                    acts = [out.action for out in outputs]\n",
        "                    ep_hist.append(tuple(acts))\n",
        "                    ts = env.step(acts)\n",
        "\n",
        "                trajectories.append(ep_hist)\n",
        "\n",
        "    return trajectories\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyQrl5sUvzLT"
      },
      "outputs": [],
      "source": [
        "def collect_pairs_data(env, bot_names, pairs, episodes_per_pair=1):\n",
        "    trajectories = []\n",
        "\n",
        "    for (i, j) in pairs:\n",
        "\n",
        "        # Agent 0\n",
        "        if i == 43:\n",
        "            ag0 = MyAgent(num_actions)\n",
        "        else:\n",
        "            ag0 = create_roshambo_bot_agent(0, num_actions, bot_names, i)\n",
        "\n",
        "        # Agent 1\n",
        "        if j == 43:\n",
        "            ag1 = MyAgent(num_actions)\n",
        "        else:\n",
        "            ag1 = create_roshambo_bot_agent(1, num_actions, bot_names, j)\n",
        "\n",
        "        agents = [ag0, ag1]\n",
        "\n",
        "        for ep in range(episodes_per_pair):\n",
        "            for ag in agents:\n",
        "                if hasattr(ag, \"restart\"):\n",
        "                    ag.restart()\n",
        "\n",
        "            ts = env.reset()\n",
        "            ep_hist = []\n",
        "\n",
        "            while not ts.last():\n",
        "                outputs = [ag.step(ts) for ag in agents]\n",
        "                acts = [out.action for out in outputs]\n",
        "                ep_hist.append(tuple(acts))\n",
        "                ts = env.step(acts)\n",
        "\n",
        "            trajectories.append(ep_hist)\n",
        "\n",
        "    return trajectories\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQrYlEaKrxav"
      },
      "source": [
        "# Turn Trajectories into Supervised Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9pCwgkvr1hf"
      },
      "outputs": [],
      "source": [
        "PAD = 9   # special pad token\n",
        "\n",
        "def make_training_samples(trajectories, window=100):\n",
        "    X = []\n",
        "    Y = []\n",
        "\n",
        "    for traj in trajectories:\n",
        "        for t in range(1, len(traj)):\n",
        "            past = traj[max(0, t-window):t]\n",
        "\n",
        "            tokens = [(a0*3 + a1) for (a0, a1) in past]\n",
        "\n",
        "            if len(tokens) < window:\n",
        "                tokens = [PAD] * (window - len(tokens)) + tokens\n",
        "            else:\n",
        "                tokens = tokens[-window:]\n",
        "\n",
        "            target_a0, target_a1 = traj[t]\n",
        "            target = target_a0*3 + target_a1\n",
        "\n",
        "            X.append(tokens)\n",
        "            Y.append(target)\n",
        "\n",
        "    return torch.tensor(X), torch.tensor(Y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ergvTOUH-k7h"
      },
      "outputs": [],
      "source": [
        "def collect_rrps_windows_from_pairs(pairs,              # list of (id0, id1)\n",
        "                                    env=env,\n",
        "                                    bot_names=roshambo_bot_names,\n",
        "                                    num_actions=num_actions,\n",
        "                                    episodes_per_pair=1,\n",
        "                                    window=100):\n",
        "    PAD = 9\n",
        "    def encode_joint(a0, a1): return 3*a0 + a1\n",
        "\n",
        "    X_list, Y_list = [], []\n",
        "    id0_list, id1_list = [], []\n",
        "\n",
        "    for (i, j) in pairs:\n",
        "\n",
        "        # agent 0\n",
        "        if i == 43:\n",
        "            ag0 = MyAgent(num_actions, name=\"trans_agent\")\n",
        "        else:\n",
        "            ag0 = create_roshambo_bot_agent(0, num_actions, bot_names, i)\n",
        "\n",
        "        # agent 1\n",
        "        if j == 43:\n",
        "            ag1 = MyAgent(num_actions, name=\"trans_agent\")\n",
        "        else:\n",
        "            ag1 = create_roshambo_bot_agent(1, num_actions, bot_names, j)\n",
        "\n",
        "        agents = [ag0, ag1]\n",
        "\n",
        "        for ep in range(episodes_per_pair):\n",
        "            for ag in agents:\n",
        "                if hasattr(ag, \"restart\"):\n",
        "                    ag.restart()\n",
        "\n",
        "            ts = env.reset()\n",
        "            ep_hist = []\n",
        "\n",
        "            while not ts.last():\n",
        "                outputs = [ag.step(ts) for ag in agents]\n",
        "                acts = [out.action for out in outputs]\n",
        "                ep_hist.append(tuple(acts))\n",
        "                ts = env.step(acts)\n",
        "\n",
        "            # convert episode to token sequence\n",
        "            tokens = [encode_joint(a0, a1) for (a0, a1) in ep_hist]\n",
        "\n",
        "            # sliding windows\n",
        "            for t in range(1, len(tokens)):\n",
        "                start = max(0, t - window)\n",
        "                seq = tokens[start:t]\n",
        "\n",
        "                if len(seq) < window:\n",
        "                    seq = [PAD] * (window - len(seq)) + seq\n",
        "\n",
        "                X_list.append(seq)\n",
        "                Y_list.append(tokens[t])\n",
        "                id0_list.append(i)\n",
        "                id1_list.append(j)\n",
        "\n",
        "    X = torch.tensor(X_list, dtype=torch.long)\n",
        "    Y = torch.tensor(Y_list, dtype=torch.long)\n",
        "    id0 = torch.tensor(id0_list, dtype=torch.long)\n",
        "    id1 = torch.tensor(id1_list, dtype=torch.long)\n",
        "\n",
        "    return X, Y, id0, id1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeXiMJ1m-nCk"
      },
      "outputs": [],
      "source": [
        "def collect_rrps_windows_two_lists(listA,\n",
        "                                   listB,\n",
        "                                   env=env,\n",
        "                                   bot_names=roshambo_bot_names,\n",
        "                                   num_actions=num_actions,\n",
        "                                   episodes_per_pair=1,\n",
        "                                   window=100):\n",
        "    pairs = [(i, j) for i in listA for j in listB]\n",
        "    return collect_rrps_windows_from_pairs(pairs,\n",
        "        episodes_per_pair=episodes_per_pair,\n",
        "        window=window\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaSrnP-hEM14"
      },
      "outputs": [],
      "source": [
        "pairs = [\n",
        "(0, 30),   # actr_lag2_decay vs randbot\n",
        "(1, 30),   # adddriftbot2 vs randbot\n",
        "(3, 30),   # antiflatbot vs randbot\n",
        "(4, 33),   # antirotnbot vs rotatebot\n",
        "(5, 30),   # biopic vs randbot\n",
        "(6, 30),   # boom vs randbot\n",
        "(7, 30),   # copybot vs randbot\n",
        "(8, 30),   # debruijn81 vs randbot\n",
        "(9, 30),   # driftbot vs randbot\n",
        "(10, 32),  # flatbot3 vs rockbot\n",
        "(11, 30),  # foxtrotbot vs randbot\n",
        "(13, 30),  # granite vs randbot\n",
        "(14, 30),  # greenberg vs randbot\n",
        "(15, 30),  # halbot vs randbot\n",
        "(16, 30),  # inocencio vs randbot\n",
        "(18, 30),  # marble vs randbot\n",
        "(20, 30),  # markovbails vs randbot\n",
        "(21, 32),  # mixed_strategy vs rockbot\n",
        "(23, 30),  # multibot vs randbot\n",
        "(24, 30),  # peterbot vs randbot\n",
        "(26, 30),  # pibot vs randbot\n",
        "(27, 32),  # piedra vs rockbot\n",
        "(31, 32),  # robertot vs rockbot\n",
        "(34, 30),  # russrocker4 vs randbot\n",
        "(35, 30),  # shofar vs randbot\n",
        "(38, 32),  # sweetrock vs rockbot\n",
        "(40, 30),  # switchbot vs randbot\n",
        "(41, 30),  # textbot vs randbot\n",
        "(42, 30),  # zq_move vs randbot\n",
        "\n",
        "(12, 30),  # vs randbot\n",
        "(12, 32),  # vs rockbot\n",
        "\n",
        "(12, 30),  # vs randbot\n",
        "(12, 32),  # vs rockbot\n",
        "\n",
        "(17, 30),  # vs randbot\n",
        "(17, 33),  # vs rotatebot\n",
        "\n",
        "(19, 30),  # vs randbot\n",
        "(19, 33),  # vs rotatebot\n",
        "\n",
        "(25, 33),  # vs rotatebot (reveals phase)\n",
        "(25, 30),  # vs randbot (tests stability)\n",
        "\n",
        "(22, 33),  # vs rotatebot (mod pattern emerges)\n",
        "(22, 12),  # vs freqbot2 (long-range pattern exposure)\n",
        "\n",
        "(28, 30),  # vs randbot\n",
        "(28, 12),  # vs freqbot2 (tests prediction ability)\n",
        "\n",
        "(29, 30),  # baseline\n",
        "(29, 33),  # high structure probe\n",
        "\n",
        "(36, 30),  # vs randbot\n",
        "(36, 33),  # vs rotatebot\n",
        "(36, 17),  # vs iocainebot (meta-chaos interaction)\n",
        "\n",
        "(37, 30),  # vs randbot\n",
        "(37, 12),  # vs freqbot2\n",
        "(37, 33),  # vs rotatebot\n",
        "\n",
        "(39, 30),  # vs randbot\n",
        "(39, 33),  # vs rotatebot\n",
        "(39, 12),  # vs freqbot2\n",
        "\n",
        "(32, 33),  # rockbot vs rotatebot â€” contrast of static vs periodic\n",
        "(12, 19),  # freqbot2 vs markov5 â€” predictor vs memory-based\n",
        "(17, 19)  # iocainebot vs markov5 â€” meta-strategy vs long-horizon\n",
        "]\n",
        "\n",
        "Xnew, Ynew, id0, id1 = collect_rrps_windows_from_pairs(pairs=pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1nzQEjXBa8o"
      },
      "outputs": [],
      "source": [
        "def merge_datasets_with_ids(X_old, Y_old,\n",
        "                            X_new, Y_new,\n",
        "                            id0_new, id1_new):\n",
        "    \"\"\"\n",
        "    Old data has no id labels -> we pad with -1.\n",
        "    \"\"\"\n",
        "    N0 = X_old.size(0)\n",
        "    N1 = X_new.size(0)\n",
        "\n",
        "    X_all = torch.cat([X_old, X_new], dim=0)\n",
        "    Y_all = torch.cat([Y_old, Y_new], dim=0)\n",
        "\n",
        "    # id0/id1: -1 for old samples\n",
        "    id0_old = -torch.ones(N0, dtype=torch.long)\n",
        "    id1_old = -torch.ones(N0, dtype=torch.long)\n",
        "\n",
        "    id0_all = torch.cat([id0_old, id0_new], dim=0)\n",
        "    id1_all = torch.cat([id1_old, id1_new], dim=0)\n",
        "\n",
        "    return X_all, Y_all, id0_all, id1_all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HG5R9FsbQpMO"
      },
      "outputs": [],
      "source": [
        "Xall, Yall, id0all, id1all = merge_datasets_with_ids(Xc, Yc, Xnew, Ynew, id0, id1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbS6sxTwNY1x"
      },
      "outputs": [],
      "source": [
        "def mirror_augment(X, Y, id0, id1):\n",
        "    \"\"\"\n",
        "    X:   (N, L) joint token sequences where each token âˆˆ {0..8, 9=PAD}\n",
        "    Y:   (N,)   next-token labels\n",
        "    id0: (N,)   seat 0 ids\n",
        "    id1: (N,)   seat 1 ids\n",
        "\n",
        "    Returns concatenated original + mirrored.\n",
        "    \"\"\"\n",
        "    device = X.device\n",
        "    PAD = 9\n",
        "\n",
        "    # mapping for token swap: encode_joint(a0,a1) â†’ encode_joint(a1,a0)\n",
        "    swap_map = torch.tensor([0, 3, 6, 1, 4, 7, 2, 5, 8, 9], device=device)\n",
        "    # indices: 0..8 joint tokens, 9 = PAD (maps to itself)\n",
        "\n",
        "    X_m = swap_map[X]  # swap a0<->a1 in every token\n",
        "    Y_m = swap_map[Y]\n",
        "\n",
        "    id0_m = id1.clone()\n",
        "    id1_m = id0.clone()\n",
        "\n",
        "    X_all  = torch.cat([X, X_m], dim=0)\n",
        "    Y_all  = torch.cat([Y, Y_m], dim=0)\n",
        "    id0_all = torch.cat([id0, id0_m], dim=0)\n",
        "    id1_all = torch.cat([id1, id1_m], dim=0)\n",
        "\n",
        "    return X_all, Y_all, id0_all, id1_all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iu8ps4d4RKDS"
      },
      "outputs": [],
      "source": [
        "Xm, Ym, id0m, id1m = mirror_augment(Xall, Yall, id0all, id1all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4qmrQPaRbXY",
        "outputId": "618b98d5-fa58-4a05-db04-b9e17d9fc722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([995004, 100]) torch.Size([995004]) torch.Size([995004]) torch.Size([995004])\n"
          ]
        }
      ],
      "source": [
        "print(Xm.shape, Ym.shape, id0m.shape, id1m.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-tThOLKt4Id"
      },
      "source": [
        "# Predictor Model (Transformer+Memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OH68Zgqt-6A"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PredictorModel(nn.Module):\n",
        "    def __init__(self, d_model=64, n_heads=2, n_layers=3,\n",
        "                 vocab=10, num_bots=43):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_bots = num_bots\n",
        "\n",
        "        # token + position embeddings\n",
        "        self.token_emb = nn.Embedding(vocab, d_model)\n",
        "        self.pos_emb = nn.Embedding(200, d_model)\n",
        "\n",
        "        # transformer encoder\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=4 * d_model,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
        "\n",
        "        # main head: next joint action (9 classes)\n",
        "        self.head = nn.Linear(d_model, 9)\n",
        "\n",
        "        # NEW: identity head â€“ predicts opponent bot id (0..num_bots-1)\n",
        "        self.id_head = nn.Linear(d_model, num_bots)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        B, L = tokens.shape\n",
        "\n",
        "        pos = torch.arange(L, device=tokens.device).unsqueeze(0).expand(B, L)\n",
        "        x = self.token_emb(tokens) + self.pos_emb(pos)\n",
        "\n",
        "        h = self.transformer(x)\n",
        "                               # (B, L, d_model)\n",
        "        hpred = h[:,-1,:]\n",
        "        h0 = h[:,0,:]\n",
        "        h1 = h[:,1,:]\n",
        "\n",
        "        # prediction uses the last time-step token\n",
        "        pred_logits = self.head(hpred)          # (B, 9)\n",
        "\n",
        "\n",
        "        # identity prediction uses the first and second time-step token\n",
        "        id0_logits = self.id_head(h0)          # (B, num_bots)\n",
        "        id1_logits = self.id_head(h1)\n",
        "\n",
        "        return pred_logits, id0_logits, id1_logits, hpred, h0, h1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSGcwhajud_z",
        "outputId": "312fda27-fb24-430e-9092-770b5b68ee89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "400\n"
          ]
        }
      ],
      "source": [
        "subset = list(range(20))   # use bots 0â€“19 (covers deterministic + reactive bots)\n",
        "trajectories = collect_rrps_data(env, roshambo_bot_names[:20], episodes_per_pair=1)\n",
        "print(len(trajectories))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHLEkUohcFsF"
      },
      "outputs": [],
      "source": [
        "pairs = [\n",
        "    (43, 40),\n",
        "    (40, 43),\n",
        "    (43, 20),\n",
        "    (28,43),\n",
        "    (43,28),\n",
        "    (42,40),\n",
        "    (40,42),\n",
        "    (42,20),\n",
        "    (20,42),\n",
        "    (40,20),\n",
        "    (20,40),\n",
        "    (28,40),\n",
        "    (40,28),\n",
        "    (28,20),\n",
        "    (20,28),\n",
        "    (28,42),\n",
        "    (42,28),\n",
        "    (20, 43),\n",
        "    (43, 42),\n",
        "]\n",
        "\n",
        "trajectories2 = collect_pairs_data(env, roshambo_bot_names, pairs, episodes_per_pair=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp82BxB3glhO",
        "outputId": "c2541e1b-90fb-4000-fd3f-429e6128ca9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38\n"
          ]
        }
      ],
      "source": [
        "print(len(trajectories2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8WlzUY4uoU1",
        "outputId": "10287acd-e33c-44db-9437-420984566f22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([399600, 100]) torch.Size([399600])\n"
          ]
        }
      ],
      "source": [
        "X, Y = make_training_samples(trajectories, window=100)\n",
        "print(X.shape, Y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOI4vj-Dga1D"
      },
      "outputs": [],
      "source": [
        "X2, Y2 = make_training_samples(trajectories2+selftrajectory, window=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw_Vl4w_iQ2d",
        "outputId": "6631f2d3-923b-431b-e412-1202d9cfbcb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([440559, 100]) torch.Size([440559])\n"
          ]
        }
      ],
      "source": [
        "print(Xc.shape,Yc.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKKkVQdhhwb1",
        "outputId": "149b3dbe-3feb-4c70-f223-3aa9ea7f1c39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([40959, 100]) torch.Size([40959])\n"
          ]
        }
      ],
      "source": [
        "print(X2.shape,Y2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gCgKFGAh0Az",
        "outputId": "f17f3ec0-1872-4a71-f38a-f250d652c4de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([399600, 100]) torch.Size([399600])\n"
          ]
        }
      ],
      "source": [
        "print(X.shape,Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDt_KBEggYil",
        "outputId": "f8b910f1-3570-48b4-984b-527c8469b2fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9eLmss-guFZ"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/MyDrive/rrpsm_data.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZedl34v_mt5",
        "outputId": "5cc69e44-c4db-43b8-f3cc-0549f3dbd922"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded: torch.Size([995004, 100]) torch.Size([995004]) torch.Size([995004]) torch.Size([995004])\n",
            "Window: 100\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Replace the filename below if yours differs\n",
        "data = torch.load(file_path, map_location=\"cpu\")\n",
        "Xm = data[\"Xm\"]\n",
        "Ym = data[\"Ym\"]\n",
        "id0m = data[\"id0m\"]\n",
        "id1m = data[\"id1m\"]\n",
        "window = data.get(\"window\", 100)\n",
        "\n",
        "print(\"Loaded:\", Xm.shape, Ym.shape, id0m.shape, id1m.shape)\n",
        "print(\"Window:\", window)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0dDJWpOuCLl"
      },
      "source": [
        "# Symmetry loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuGlCXk0HeRA"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "It_w-TpRuECI"
      },
      "outputs": [],
      "source": [
        "def swap_tokens_batch(batch_tokens):\n",
        "    PAD = 9\n",
        "    a0 = batch_tokens // 3\n",
        "    a1 = batch_tokens % 3\n",
        "    swapped = a1 * 3 + a0\n",
        "    swapped[batch_tokens == PAD] = PAD\n",
        "    return swapped\n",
        "\n",
        "swap_map = torch.tensor([0,3,6,1,4,7,2,5,8], device=device)\n",
        "\n",
        "def symmetry_loss(pred, pred_sw):\n",
        "    pred_swapped = pred[:, swap_map]\n",
        "    return ((pred_sw - pred_swapped)**2).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KipgMbFYuGyp"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku0njcrvSEkd"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ObpiRkMSLyV"
      },
      "outputs": [],
      "source": [
        "model = PredictorModel(num_bots=len(roshambo_bot_names), n_layers=3).to(device)\n",
        "model2 = PredictorModel(num_bots=len(roshambo_bot_names), n_layers=3).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "opt2 = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_ce  = nn.CrossEntropyLoss()\n",
        "loss_idc = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub7FtsS5SZl1"
      },
      "outputs": [],
      "source": [
        "for g in opt.param_groups:\n",
        "    g[\"lr\"] = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-WS0ellSNPz"
      },
      "outputs": [],
      "source": [
        "beta_id = 0.0   # weight for identity loss\n",
        "lambda_sym = 0.00\n",
        "batch_size = 512\n",
        "num_epochs = 10\n",
        "save_every = 200          # save every 200 batches\n",
        "show_every = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "teCHzQ3LSPLy",
        "outputId": "890c89c5-5411-4f66-c4eb-6a58dfbbcb21"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Xm' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2827258877.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mXm_dev\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mXm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mYm_dev\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mYm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mid0m_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid0m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mid1m_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid1m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Xm' is not defined"
          ]
        }
      ],
      "source": [
        "Xm_dev  = Xm.to(device)\n",
        "Ym_dev  = Ym.to(device)\n",
        "id0m_dev = id0m.to(device)\n",
        "id1m_dev = id1m.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Odl658gmtd4O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "ckpt_path_latest = \"/content/drive/MyDrive/pred_checkpoint_latest.pt\"\n",
        "ckpt_path_latest2 = \"/content/drive/MyDrive/pred_checkpoint_latest2.pt\"\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "if os.path.exists(ckpt_path_latest):\n",
        "    ckpt = torch.load(ckpt_path_latest, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    opt.load_state_dict(ckpt[\"opt\"])\n",
        "    start_epoch = ckpt[\"epoch\"]\n",
        "if os.path.exists(ckpt_path_latest2):\n",
        "    ckpt = torch.load(ckpt_path_latest2, map_location=device)\n",
        "    model2.load_state_dict(ckpt[\"model\"])\n",
        "    opt2.load_state_dict(ckpt[\"opt\"])\n",
        "    start_epoch = ckpt[\"epoch\"]\n",
        "else:\n",
        "    model2.load_state_dict(torch.load(\"/content/drive/MyDrive/online_pred_weights.pt\", map_location=device))\n",
        "\n",
        "    print(f\"Resuming from epoch {start_epoch}, batch {ckpt['idx']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHcEGiOoC4i4",
        "outputId": "ef8cec83-7d80-4e52-bbcd-15931236035f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on: cuda\n",
            "[epoch 0, batch 0.0] loss_main=1.5246, loss_sym=0.6108, loss_id=4.0096\n",
            "ðŸ’¾ Saved checkpoint at epoch 0, batch 0\n",
            "[epoch 0, batch 200.0] loss_main=1.5391, loss_sym=0.4341, loss_id=4.0884\n",
            "ðŸ’¾ Saved checkpoint at epoch 0, batch 200\n",
            "[epoch 0, batch 400.0] loss_main=1.4916, loss_sym=0.4707, loss_id=4.1628\n",
            "ðŸ’¾ Saved checkpoint at epoch 0, batch 400\n",
            "ðŸ’¾ Saved epoch 0 checkpoint\n",
            "Epoch 0: CE Loss = 1.5049\n",
            "[epoch 1, batch 0.0] loss_main=1.5155, loss_sym=0.4410, loss_id=3.9379\n",
            "ðŸ’¾ Saved checkpoint at epoch 1, batch 0\n",
            "[epoch 1, batch 200.0] loss_main=1.4668, loss_sym=0.4507, loss_id=4.3647\n",
            "ðŸ’¾ Saved checkpoint at epoch 1, batch 200\n",
            "[epoch 1, batch 400.0] loss_main=1.5167, loss_sym=0.4407, loss_id=4.1701\n",
            "ðŸ’¾ Saved checkpoint at epoch 1, batch 400\n",
            "ðŸ’¾ Saved epoch 1 checkpoint\n",
            "Epoch 1: CE Loss = 1.4979\n",
            "[epoch 2, batch 0.0] loss_main=1.5442, loss_sym=0.4245, loss_id=4.1437\n",
            "ðŸ’¾ Saved checkpoint at epoch 2, batch 0\n",
            "[epoch 2, batch 200.0] loss_main=1.4970, loss_sym=0.4309, loss_id=4.1200\n",
            "ðŸ’¾ Saved checkpoint at epoch 2, batch 200\n",
            "[epoch 2, batch 400.0] loss_main=1.4977, loss_sym=0.4480, loss_id=4.3215\n",
            "ðŸ’¾ Saved checkpoint at epoch 2, batch 400\n",
            "ðŸ’¾ Saved epoch 2 checkpoint\n",
            "Epoch 2: CE Loss = 1.4947\n",
            "[epoch 3, batch 0.0] loss_main=1.4994, loss_sym=0.4840, loss_id=4.1481\n",
            "ðŸ’¾ Saved checkpoint at epoch 3, batch 0\n",
            "[epoch 3, batch 200.0] loss_main=1.4925, loss_sym=0.5131, loss_id=4.2395\n",
            "ðŸ’¾ Saved checkpoint at epoch 3, batch 200\n",
            "[epoch 3, batch 400.0] loss_main=1.4800, loss_sym=0.4529, loss_id=4.2572\n",
            "ðŸ’¾ Saved checkpoint at epoch 3, batch 400\n",
            "ðŸ’¾ Saved epoch 3 checkpoint\n",
            "Epoch 3: CE Loss = 1.4919\n",
            "[epoch 4, batch 0.0] loss_main=1.4902, loss_sym=0.4695, loss_id=4.1802\n",
            "ðŸ’¾ Saved checkpoint at epoch 4, batch 0\n",
            "[epoch 4, batch 200.0] loss_main=1.4911, loss_sym=0.5066, loss_id=4.2616\n",
            "ðŸ’¾ Saved checkpoint at epoch 4, batch 200\n"
          ]
        }
      ],
      "source": [
        "print(\"Training on:\", device)\n",
        "\n",
        "N = Xm_dev.size(0)\n",
        "\n",
        "\n",
        "ckpt_path_latest = \"/content/drive/MyDrive/pred_checkpoint_latest.pt\"\n",
        "ckpt_path_epoch  = \"/content/drive/MyDrive/pred_checkpoint_epoch{}.pt\"\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    perm = torch.randperm(N, device=device)\n",
        "    running_ce = 0.0\n",
        "\n",
        "    for idx in range(0, N, batch_size):\n",
        "        batch_ix = perm[idx:idx + batch_size]\n",
        "        inp = Xm_dev[batch_ix]\n",
        "        tgt = Ym_dev[batch_ix]\n",
        "        id0b = id0m_dev[batch_ix]\n",
        "        id1b = id1m_dev[batch_ix]\n",
        "\n",
        "        B = inp.size(0)\n",
        "\n",
        "        pred, id0_logits, id1_logits, hpred, h0, h1 = model(inp)\n",
        "        loss_main = loss_ce(pred, tgt)\n",
        "\n",
        "        # identity loss only on samples with valid labels (>=0)\n",
        "        mask = (id0b >= 0) & (id1b >= 0)\n",
        "        if mask.any():\n",
        "            loss_id0 = loss_idc(id0_logits[mask], id0b[mask])\n",
        "            loss_id1 = loss_idc(id1_logits[mask], id1b[mask])\n",
        "            loss_id = 0.5 * (loss_id0 + loss_id1)\n",
        "\n",
        "            frac = mask.float().mean().item()\n",
        "            scaled_beta = beta_id / max(frac, 1e-6)\n",
        "        else:\n",
        "            loss_id = 0.0\n",
        "\n",
        "        # symmetry loss every few batches\n",
        "        if idx % (4 * batch_size) == 0:\n",
        "            swapped_inp = swap_tokens_batch(inp)\n",
        "            pred_sw, _, _, _, _, _ = model(swapped_inp)\n",
        "            loss_sym = symmetry_loss(pred, pred_sw)\n",
        "        else:\n",
        "            loss_sym = 0.0\n",
        "\n",
        "        loss = loss_main + lambda_sym * loss_sym + scaled_beta * loss_id\n",
        "\n",
        "        if idx % (show_every*batch_size) == 0:\n",
        "            print(f\"[epoch {epoch}, batch {idx/batch_size}] \"\n",
        "                  f\"loss_main={loss_main.item():.4f}, \"\n",
        "                  f\"loss_sym={loss_sym if isinstance(loss_sym, float) else loss_sym.item():.4f}, \"\n",
        "                  f\"loss_id={loss_id if isinstance(loss_id, float) else loss_id.item():.4f}\")\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        running_ce += loss_main.item()\n",
        "\n",
        "        # ðŸ”¥ periodic checkpointing (won't kill performance)\n",
        "        if idx % (save_every*batch_size) == 0:\n",
        "            checkpoint = {\n",
        "                \"epoch\": epoch,\n",
        "                \"idx\": idx,\n",
        "                \"model\": model.state_dict(),\n",
        "                \"opt\": opt.state_dict(),\n",
        "                \"beta_id\": beta_id,\n",
        "                \"lambda_sym\": lambda_sym,\n",
        "                \"batch_size\": batch_size,\n",
        "            }\n",
        "            torch.save(checkpoint, ckpt_path_latest)\n",
        "            print(f\"ðŸ’¾ Saved checkpoint at epoch {epoch}, batch {idx//batch_size}\")\n",
        "\n",
        "    # ðŸ”¥ save at end of each epoch\n",
        "    epoch_ckpt = {\n",
        "        \"epoch\": epoch,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"opt\": opt.state_dict(),\n",
        "    }\n",
        "    torch.save(epoch_ckpt, ckpt_path_epoch.format(epoch))\n",
        "    print(f\"ðŸ’¾ Saved epoch {epoch} checkpoint\")\n",
        "\n",
        "    print(f\"Epoch {epoch}: CE Loss = {running_ce / max(1, N//batch_size):.4f}\")\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/pred_weights.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "ayWL6_gB4tww",
        "outputId": "5b1f2116-c7bb-4fe9-f054-b8e42d8ee527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on: cpu\n",
            "[epoch 0, batch 0.0] loss_main=1.4095, loss_sym=0.5027, loss_id=4.1506\n",
            "ðŸ’¾ Saved checkpoint at epoch 0, batch 0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2873460092.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid0_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid1_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mloss_main\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_ce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-114284776.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                                \u001b[0;31m# (B, L, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mhpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m             output = mod(\n\u001b[0m\u001b[1;32m    525\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    935\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             )\n\u001b[0;32m--> 937\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;31m# feed forward block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    963\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \"\"\"\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"dropout probability has to be between 0 and 1, but got {p}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     return (\n\u001b[0;32m-> 1418\u001b[0;31m         \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1419\u001b[0m     )\n\u001b[1;32m   1420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"Training on:\", device)\n",
        "\n",
        "N = Xm_dev.size(0)\n",
        "\n",
        "\n",
        "ckpt_path_latest2 = \"/content/drive/MyDrive/pred_checkpoint_latest2.pt\"\n",
        "ckpt_path_epoch  = \"/content/drive/MyDrive/pred_checkpoint_epoch{}.pt\"\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    perm = torch.randperm(N, device=device)\n",
        "    running_ce = 0.0\n",
        "\n",
        "    for idx in range(0, N, batch_size):\n",
        "        batch_ix = perm[idx:idx + batch_size]\n",
        "        inp = Xm_dev[batch_ix]\n",
        "        tgt = Ym_dev[batch_ix]\n",
        "        id0b = id0m_dev[batch_ix]\n",
        "        id1b = id1m_dev[batch_ix]\n",
        "\n",
        "        B = inp.size(0)\n",
        "\n",
        "        pred, id0_logits, id1_logits, hpred, h0, h1 = model2(inp)\n",
        "        loss_main = loss_ce(pred, tgt)\n",
        "\n",
        "        # identity loss only on samples with valid labels (>=0)\n",
        "        mask = (id0b >= 0) & (id1b >= 0)\n",
        "        if mask.any():\n",
        "            loss_id0 = loss_idc(id0_logits[mask], id0b[mask])\n",
        "            loss_id1 = loss_idc(id1_logits[mask], id1b[mask])\n",
        "            loss_id = 0.5 * (loss_id0 + loss_id1)\n",
        "\n",
        "            frac = mask.float().mean().item()\n",
        "            scaled_beta = beta_id / max(frac, 1e-6)\n",
        "        else:\n",
        "            loss_id = 0.0\n",
        "\n",
        "        # symmetry loss every few batches\n",
        "        if idx % (4 * batch_size) == 0:\n",
        "            swapped_inp = swap_tokens_batch(inp)\n",
        "            pred_sw, _, _, _, _, _ = model2(swapped_inp)\n",
        "            loss_sym = symmetry_loss(pred, pred_sw)\n",
        "        else:\n",
        "            loss_sym = 0.0\n",
        "\n",
        "        loss = loss_main + lambda_sym * loss_sym + scaled_beta * loss_id\n",
        "\n",
        "        if idx % (show_every*batch_size) == 0:\n",
        "            print(f\"[epoch {epoch}, batch {idx/batch_size}] \"\n",
        "                  f\"loss_main={loss_main.item():.4f}, \"\n",
        "                  f\"loss_sym={loss_sym if isinstance(loss_sym, float) else loss_sym.item():.4f}, \"\n",
        "                  f\"loss_id={loss_id if isinstance(loss_id, float) else loss_id.item():.4f}\")\n",
        "\n",
        "        opt2.zero_grad()\n",
        "        loss.backward()\n",
        "        opt2.step()\n",
        "\n",
        "        running_ce += loss_main.item()\n",
        "\n",
        "        # ðŸ”¥ periodic checkpointing (won't kill performance)\n",
        "        if idx % (save_every*batch_size) == 0:\n",
        "            checkpoint = {\n",
        "                \"epoch\": epoch,\n",
        "                \"idx\": idx,\n",
        "                \"model\": model2.state_dict(),\n",
        "                \"opt\": opt2.state_dict(),\n",
        "                \"beta_id\": beta_id,\n",
        "                \"lambda_sym\": lambda_sym,\n",
        "                \"batch_size\": batch_size,\n",
        "            }\n",
        "            torch.save(checkpoint, ckpt_path_latest2)\n",
        "            print(f\"ðŸ’¾ Saved checkpoint at epoch {epoch}, batch {idx//batch_size}\")\n",
        "\n",
        "    # ðŸ”¥ save at end of each epoch\n",
        "    epoch_ckpt = {\n",
        "        \"epoch\": epoch,\n",
        "        \"model\": model2.state_dict(),\n",
        "        \"opt\": opt2.state_dict(),\n",
        "    }\n",
        "    torch.save(epoch_ckpt, ckpt_path_epoch.format(epoch))\n",
        "    print(f\"ðŸ’¾ Saved epoch {epoch} checkpoint\")\n",
        "\n",
        "    print(f\"Epoch {epoch}: CE Loss = {running_ce / max(1, N//batch_size):.4f}\")\n",
        "torch.save(model2.state_dict(), \"/content/drive/MyDrive/pred_weights.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neCcstdootKR"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "zayS-IFjQCM-",
        "outputId": "5fde3cd2-d2ce-4a60-820e-d430bd3eec73"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_45be0034-4e85-4e45-bab4-d5af46cda091\", \"predictor_weights.pt\", 297259)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"predictor_weights.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "pbtKbihswc25",
        "outputId": "6c391cc5-325b-4415-e201-781cbc54472d"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_548b9a1c-5f3d-4a36-954a-71362e8d7311\", \"predictor_weights2.pt\", 297285)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "files.download(\"predictor_weights2.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKedmD2G3Axh"
      },
      "source": [
        "#Basic Template for an RL agent\n",
        "\n",
        "Below is a template of a basic RL agent to get you started. You will be implementing the step() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY_w_Qv8A315"
      },
      "outputs": [],
      "source": [
        "class IdentityBelief():\n",
        "    def __init__(self, num_bots=43, alpha=0.15, device='cpu'):\n",
        "        self.alpha = alpha\n",
        "        self.belief = torch.ones(num_bots).to(device) / num_bots\n",
        "\n",
        "    def update(self, logits):\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        self.belief = (1 - self.alpha) * self.belief + self.alpha * probs\n",
        "        return self.belief"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3oxFSPMA5AI"
      },
      "outputs": [],
      "source": [
        "class IdentityConditionedPolicy(nn.Module):\n",
        "    def __init__(self, d_model, num_bots=43):\n",
        "        super().__init__()\n",
        "\n",
        "        # each identity dist is one \"token\" -> shape (2, num_bots)\n",
        "        self.id_to_Q = nn.Linear(num_bots, d_model)\n",
        "        self.id_to_K = nn.Linear(num_bots, d_model)\n",
        "        self.id_to_V = nn.Linear(num_bots, d_model)\n",
        "\n",
        "        # base latent features: h0, h1, hpred, pred_logits (9-dim)\n",
        "        self.feature_proj = nn.Linear(d_model*3 + 9, d_model)\n",
        "\n",
        "        # final action head\n",
        "        self.out = nn.Linear(d_model, 3)\n",
        "\n",
        "    def forward(self, h0, h1, hpred, pred_logits, my_belief, opp_belief):\n",
        "        # identity tokens: shape (2, num_bots)\n",
        "        tokens = torch.stack([my_belief, opp_belief], dim=0)  # (2, 43)\n",
        "\n",
        "        Q = self.id_to_Q(tokens)  # (2, d)\n",
        "        K = self.id_to_K(tokens)  # (2, d)\n",
        "        V = self.id_to_V(tokens)  # (2, d)\n",
        "\n",
        "        # attention scores: (2,2)\n",
        "        scores = (Q @ K.t()) / (Q.size(-1)**0.5)\n",
        "        attn = torch.softmax(scores, dim=-1)  # (2,2)\n",
        "\n",
        "        # identity-conditioned context: (2,d) -> pooled -> (1,d)\n",
        "        W = attn @ V\n",
        "        context = W.mean(dim=0, keepdim=True)\n",
        "\n",
        "        # base features\n",
        "        base = torch.cat([h0, h1, hpred, pred_logits], dim=-1)\n",
        "        base = self.feature_proj(base.unsqueeze(0))  # (1,d)\n",
        "\n",
        "        # combine context\n",
        "        fused = torch.tanh(base + context)  # (1,d)\n",
        "\n",
        "        # action logits\n",
        "        return self.out(fused)  # (1,3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls9ranG1ddSW"
      },
      "outputs": [],
      "source": [
        "class IdentityTemperature(nn.Module):\n",
        "    def __init__(self, num_bots=43):\n",
        "        super().__init__()\n",
        "        self.w = nn.Parameter(torch.zeros(num_bots))\n",
        "        self.a = nn.Parameter(torch.tensor(1.0))   # gain\n",
        "        self.b = nn.Parameter(torch.tensor(1.0))   # base temp\n",
        "\n",
        "    def forward(self, id0_logits, id1_logits):\n",
        "        # identity difference\n",
        "        d = id0_logits - id1_logits   # shape (43)\n",
        "\n",
        "        # advantage score\n",
        "        s = torch.dot(self.w, d)\n",
        "\n",
        "        # map to temperature\n",
        "        tau = F.softplus(self.a * s + self.b)\n",
        "\n",
        "        # clamp tau to reasonable range\n",
        "        tau = torch.clamp(tau, 0.3, 2.5)\n",
        "\n",
        "        return tau\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBVDlrLhj-IE"
      },
      "outputs": [],
      "source": [
        "class TemperatureHead(nn.Module):\n",
        "    def __init__(self, num_bots=43):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(num_bots, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, my_belief, opp_belief):\n",
        "        # difference tells us how much \"we are like X\" vs \"they are like X\"\n",
        "        diff = my_belief - opp_belief      # (num_bots,)\n",
        "        raw = self.mlp(diff)               # (1,)\n",
        "\n",
        "        tau = F.softplus(raw)[0]           # positive scalar\n",
        "        tau = torch.clamp(tau, 0.1, 5.0)   # keep it sane\n",
        "        return tau\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg9JaB9jNiOK"
      },
      "outputs": [],
      "source": [
        "class DynamicPayoffHead(nn.Module):\n",
        "    def __init__(self, d_model=64, hidden=128, max_delta=0.5):\n",
        "        super().__init__()\n",
        "        self.max_delta = max_delta   # clamp magnitude\n",
        "\n",
        "        # Combine the three latent vectors -> one fused representation\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(d_model * 3, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Output 9 numbers (3Ã—3 payoff adjustments)\n",
        "        self.out = nn.Linear(hidden, 9)\n",
        "\n",
        "    def forward(self, h0, h1, hpred):\n",
        "        \"\"\"\n",
        "        h0, h1, hpred are each (d_model,)\n",
        "        Output: Î”R of shape (3,3)\n",
        "        \"\"\"\n",
        "\n",
        "        x = torch.cat([h0, h1, hpred], dim=-1)\n",
        "        z = self.fuse(x)\n",
        "\n",
        "        raw = self.out(z)                 # (9,)\n",
        "        delta = torch.tanh(raw) * self.max_delta\n",
        "        delta = delta.view(3, 3)          # reshape\n",
        "\n",
        "        # OPTIONAL: zero the diagonal\n",
        "        # delta = delta * (1 - torch.eye(3))\n",
        "\n",
        "        return delta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sxAkTX6L_vl"
      },
      "outputs": [],
      "source": [
        "class QHead(nn.Module):\n",
        "    def __init__(self, d_model=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 3)   # Q-values for R,P,S\n",
        "        )\n",
        "\n",
        "    def forward(self, hpred):\n",
        "        return self.net(hpred)   # (3,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYqajc8ddij9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#      1)   ANALYTICAL BEST-RESPONSE FROM pred_logits\n",
        "# ============================================================\n",
        "\n",
        "def compute_expected_reward_from_pred(pred_logits):\n",
        "    \"\"\"\n",
        "    pred_logits: shape (9,)\n",
        "    Returns: tensor (3,) expected reward for actions R,P,S\n",
        "    \"\"\"\n",
        "\n",
        "    probs = torch.softmax(pred_logits, dim=-1)  # (9,)\n",
        "    M = probs.view(3, 3)                        # joint matrix\n",
        "\n",
        "    Rmat = torch.tensor([\n",
        "        [ 0, -1,  1],\n",
        "        [ 1,  0, -1],\n",
        "        [-1,  1,  0],\n",
        "    ], dtype=torch.float32, device=pred_logits.device)\n",
        "\n",
        "    ER = torch.zeros(3, device=pred_logits.device)\n",
        "    for a0 in range(3):\n",
        "        row = M[a0]\n",
        "        s = row.sum()\n",
        "        if s <= 1e-9:\n",
        "            ER[a0] = 0\n",
        "        else:\n",
        "            cond = row / s\n",
        "            ER[a0] = torch.sum(Rmat[a0] * cond)\n",
        "\n",
        "    return ER   # shape (3,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLqHW2jDCV4Y"
      },
      "outputs": [],
      "source": [
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, d_model=64, num_bots=43, max_delta=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dynamic_R = DynamicPayoffHead(d_model=d_model, max_delta=max_delta)\n",
        "\n",
        "        # identity â†’ temperature\n",
        "        self.w = nn.Parameter(torch.zeros(num_bots))\n",
        "        self.b = nn.Parameter(torch.tensor(1.0))\n",
        "\n",
        "        # final fusion\n",
        "        self.head = nn.Linear(3, 3)\n",
        "\n",
        "        # static base RPS matrix\n",
        "        self.R_static = torch.tensor([\n",
        "            [ 0, -1,  1],\n",
        "            [ 1,  0, -1],\n",
        "            [-1,  1,  0]\n",
        "        ], dtype=torch.float32)\n",
        "\n",
        "    def compute_temperature(self, my_b, opp_b):\n",
        "        diff = my_b - opp_b\n",
        "        tau_raw = (self.w * diff).sum() + self.b\n",
        "        tau = F.softplus(tau_raw).clamp(0.1, 5.0)\n",
        "        return tau\n",
        "\n",
        "    def forward(self, h0, h1, hpred, pred_logits, my_b, opp_b):\n",
        "        # --- predicted opponent distribution ---\n",
        "        probs = torch.softmax(pred_logits, dim=-1).view(3,3)\n",
        "\n",
        "        # --- compute dynamic payoff ---\n",
        "        delta = self.dynamic_R(h0, h1, hpred)  # (3,3)\n",
        "        R_eff = self.R_static.to(delta.device) + delta\n",
        "\n",
        "        expected = torch.matmul(R_eff, probs.sum(dim=1))\n",
        "\n",
        "        # --- temperature scaling ---\n",
        "        tau = self.compute_temperature(my_b, opp_b)\n",
        "        scaled = expected / tau\n",
        "\n",
        "        # --- little linear head for flexibility ---\n",
        "        logits = self.head(scaled)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGt-IdoW6Ole"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "\n",
        "class MyAgent(rl_agent.AbstractAgent):\n",
        "    def __init__(self, num_actions, model,\n",
        "                 weights=\"pred_weights.pt\",\n",
        "                 d_model=64,\n",
        "                 num_bots=43, update_pred=False, device=\"cpu\", player_id=0, name='predynamic'):\n",
        "\n",
        "        self.device = device\n",
        "        self._num_actions = num_actions\n",
        "        self.name = name\n",
        "        self.update_pred = update_pred\n",
        "        self.num_bots = num_bots\n",
        "        self.player_id = player_id\n",
        "\n",
        "        # ---------------- FROZEN PREDICTOR ----------------\n",
        "        if model is None:\n",
        "            self.predictor = PredictorModel(\n",
        "                d_model=d_model, n_layers=3, num_bots=num_bots\n",
        "            ).to(device)\n",
        "            self.predictor.load_state_dict(\n",
        "                torch.load(weights, map_location=\"cpu\"), strict=False\n",
        "            )\n",
        "        else:\n",
        "            self.predictor = copy.deepcopy(model).to(device)\n",
        "\n",
        "        self.predictor.eval()\n",
        "        for p in self.predictor.parameters():\n",
        "            p.requires_grad = update_pred\n",
        "\n",
        "        # ---------------- POLICY (TRAINED HEADS) ----------------\n",
        "        self.policy = PolicyNet(d_model=d_model, num_bots=num_bots).to(device)\n",
        "\n",
        "        # ---------------- BELIEFS + HISTORY ---------------\n",
        "        self.my_belief = IdentityBelief(num_bots,device=device)\n",
        "        self.opp_belief = IdentityBelief(num_bots, device=device)\n",
        "        self.history_tokens = []\n",
        "        self.saved_last = None\n",
        "\n",
        "\n",
        "    # ------------ helpers ------------\n",
        "\n",
        "    def restart(self):\n",
        "        self.history_tokens = []\n",
        "        self.my_belief = IdentityBelief(self.num_bots, device=self.device)\n",
        "        self.opp_belief = IdentityBelief(self.num_bots, device=self.device)\n",
        "        self.saved_last = None\n",
        "\n",
        "    def _encode_move(self, a0, a1):\n",
        "        return 3 * a0 + a1\n",
        "\n",
        "    def _make_window(self):\n",
        "        win = self.history_tokens[-100:]\n",
        "        if len(win) < 100:\n",
        "            win = [9] * (100 - len(win)) + win\n",
        "        return torch.tensor(win).long().unsqueeze(0).to(self.device)\n",
        "\n",
        "\n",
        "    # ------------ main step ------------\n",
        "\n",
        "    def _step(self, time_step):\n",
        "\n",
        "        # FIRST STEP\n",
        "        if time_step.first():\n",
        "            self.restart()\n",
        "            probs = np.ones(3) / 3\n",
        "            action = np.random.choice(3, p=probs)\n",
        "\n",
        "            self.saved_last = (\n",
        "                torch.zeros(64), torch.zeros(64), torch.zeros(64),\n",
        "                torch.zeros(9),\n",
        "                torch.zeros(self.num_bots),\n",
        "                torch.zeros(self.num_bots),\n",
        "                torch.log(torch.tensor(1/3.0)),\n",
        "            )\n",
        "            return rl_agent.StepOutput(action=action, probs=probs)\n",
        "\n",
        "        # DECODE LAST MOVE\n",
        "        game, state = pyspiel.deserialize_game_and_state(\n",
        "            time_step.observations[\"serialized_state\"]\n",
        "        )\n",
        "        h = state.history()\n",
        "\n",
        "        if len(h) >= 2:\n",
        "            a0 = h[-2]\n",
        "            a1 = h[-1]\n",
        "        if self.player_id == 0:\n",
        "            my_a = a0\n",
        "            opp_a = a1\n",
        "        else:\n",
        "            my_a = a1\n",
        "            opp_a = a0\n",
        "        self.history_tokens.append(self._encode_move(my_a, opp_a))\n",
        "\n",
        "        # predictor forward\n",
        "        window = self._make_window()\n",
        "\n",
        "        if self.update_pred:\n",
        "            pred_logits, id0_logits, id1_logits, hpred, h0, h1 = \\\n",
        "                self.predictor(window)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                pred_logits, id0_logits, id1_logits, hpred, h0, h1 = \\\n",
        "                    self.predictor(window)\n",
        "\n",
        "        # update beliefs\n",
        "        my_belief = self.my_belief.update(id0_logits[0])\n",
        "        opp_belief = self.opp_belief.update(id1_logits[0])\n",
        "\n",
        "        # --- policy forward ---\n",
        "        logits = self.policy(\n",
        "            h0[0], h1[0], hpred[0],\n",
        "            pred_logits[0],\n",
        "            my_belief,\n",
        "            opp_belief,\n",
        "        )\n",
        "\n",
        "        action_probs = torch.softmax(logits, dim=-1)\n",
        "        action = torch.multinomial(action_probs, 1).item()\n",
        "        logp = torch.log(action_probs[action] + 1e-12)\n",
        "\n",
        "        # store step\n",
        "        self.saved_last = (\n",
        "            h0[0],\n",
        "            h1[0],\n",
        "            hpred[0],\n",
        "            pred_logits[0],\n",
        "            my_belief.clone(),\n",
        "            opp_belief.clone(),\n",
        "            logp,\n",
        "        )\n",
        "\n",
        "        return rl_agent.StepOutput(\n",
        "            action=action,\n",
        "            probs=action_probs.detach().cpu().numpy()\n",
        "        )\n",
        "    def step(self, time_step, is_evaluation=True):\n",
        "        if is_evaluation:\n",
        "            with torch.no_grad():\n",
        "                return self._step(time_step)\n",
        "        else:\n",
        "            return self._step(time_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC2FDgnwjQ2I"
      },
      "source": [
        "# RL Training (with REINFORCE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsfrxhGnjTVO"
      },
      "outputs": [],
      "source": [
        "def collect_episode(env, agent, opponent):\n",
        "    trajectory = []\n",
        "\n",
        "    ts = env.reset()\n",
        "    agent.restart()\n",
        "    opponent.restart()\n",
        "\n",
        "    while not ts.last():\n",
        "        a_out = agent.step(ts,is_evaluation=False)\n",
        "        o_out = opponent.step(ts)\n",
        "        acts = [a_out.action, o_out.action]\n",
        "\n",
        "        ts_next = env.step(acts)\n",
        "        reward = ts_next.rewards[0]\n",
        "\n",
        "        if agent.saved_last is None:\n",
        "            # safety fallback (should not happen normally)\n",
        "            h0 = torch.zeros(64)\n",
        "            h1 = torch.zeros(64)\n",
        "            hpred = torch.zeros(64)\n",
        "            pred_logits = torch.zeros(9)\n",
        "            myb = torch.zeros(agent.num_bots)\n",
        "            oppb = torch.zeros(agent.num_bots)\n",
        "            logp = torch.log(torch.tensor(1/3.0))\n",
        "        else:\n",
        "            h0, h1, hpred, pred_logits, myb, oppb, logp = agent.saved_last\n",
        "\n",
        "        step = (h0, h1, hpred, pred_logits, myb, oppb, logp, reward)\n",
        "        trajectory.append(step)\n",
        "\n",
        "        ts = ts_next\n",
        "\n",
        "    return trajectory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWGiE1A5jgap"
      },
      "outputs": [],
      "source": [
        "def reinforce_update(agent_policy, optimizer, episode, gamma=0.99):\n",
        "    # Remove empty steps\n",
        "    episode = [step for step in episode if len(step) > 0]\n",
        "\n",
        "    if len(episode) == 0:\n",
        "        return 0.0  # nothing to learn from\n",
        "\n",
        "    # discounted rewards\n",
        "    rewards = [step[-1] for step in episode]\n",
        "    G = []\n",
        "    R = 0\n",
        "    for r in reversed(rewards):\n",
        "        R = r + gamma * R\n",
        "        G.insert(0, R)\n",
        "    returns = torch.tensor(G, dtype=torch.float32)\n",
        "\n",
        "    # normalize\n",
        "    adv = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "    loss = 0\n",
        "    for (step, A) in zip(episode, adv):\n",
        "        logprob = step[-2]      # second last = log Ï€(a|s)\n",
        "        loss += -(logprob * A)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwY-tPDsCjoL"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch.nn.utils\n",
        "\n",
        "def reinforce_update_round(policy, optimizer, pred_opt, episodes_round, bot_strength,\n",
        "                           gamma=0.99, diff_scale_strength=0.4,\n",
        "                           max_grad_norm=1.0, update_pred = False):\n",
        "    \"\"\"\n",
        "    episodes_round: list of (opponent_id, episode)\n",
        "      - episode is list of steps: (..., logp, reward)\n",
        "    bot_strength: numpy array of current strengths\n",
        "    \"\"\"\n",
        "    total_loss_out = 0.0\n",
        "    any_data = False\n",
        "\n",
        "    # Get the device of the policy network\n",
        "    policy_device = next(policy.parameters()).device\n",
        "\n",
        "    for opponent_id, episode in episodes_round:\n",
        "        if len(episode) == 0:\n",
        "            continue\n",
        "\n",
        "        any_data = True\n",
        "\n",
        "        # Move returns to the correct device\n",
        "        rewards = [step[-1] for step in episode]\n",
        "        G = []\n",
        "        R = 0\n",
        "        for r in reversed(rewards):\n",
        "            R = r + gamma * R\n",
        "            G.insert(0, R)\n",
        "        returns = torch.tensor(G, dtype=torch.float32, device=policy_device)\n",
        "\n",
        "        # advantage normalize per-episode\n",
        "        adv = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "        # difficulty-based scale (bounded via tanh)\n",
        "        s = bot_strength[opponent_id]\n",
        "        scale = 1.0 + diff_scale_strength * np.tanh(s)\n",
        "\n",
        "        ep_loss = 0.0\n",
        "        for (step, A) in zip(episode, adv):\n",
        "            # Move step tensors to the correct device before using them\n",
        "            h0, h1, hpred, pred_logits, myb, oppb, logp, reward = step\n",
        "\n",
        "            h0_dev = h0.to(policy_device)\n",
        "            h1_dev = h1.to(policy_device)\n",
        "            hpred_dev = hpred.to(policy_device)\n",
        "            pred_logits_dev = pred_logits.to(policy_device)\n",
        "            myb_dev = myb.to(policy_device)\n",
        "            oppb_dev = oppb.to(policy_device)\n",
        "            logp_dev = logp.to(policy_device)\n",
        "\n",
        "            logits = policy(h0_dev, h1_dev, hpred_dev, pred_logits_dev, myb_dev, oppb_dev)\n",
        "\n",
        "            # logprob and adv are already on policy_device\n",
        "            ep_loss += -(logp_dev * A) * scale / len(episodes_round)\n",
        "\n",
        "        total_loss_out += ep_loss\n",
        "\n",
        "    if not any_data:\n",
        "        return 0.0\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    pred_opt.zero_grad()\n",
        "    total_loss_out.backward()\n",
        "    if max_grad_norm is not None:\n",
        "        torch.nn.utils.clip_grad_norm_(policy.parameters(), max_grad_norm)\n",
        "\n",
        "    optimizer.step()\n",
        "    if update_pred:\n",
        "        pred_opt.step()\n",
        "\n",
        "    return total_loss_out.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPEEqvRcoBgD"
      },
      "outputs": [],
      "source": [
        "save_path = \"/content/drive/MyDrive/temp_weights.pt\"\n",
        "pred_path = \"/content/drive/MyDrive/online_pred_weights.pt\"\n",
        "save_every = 1\n",
        "update_pred = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6TzXbp3jrOL"
      },
      "outputs": [],
      "source": [
        "agent = MyAgent(3, model=None, weights=pred_path, update_pred=update_pred, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9PmNyQIygiv",
        "outputId": "f6118488-32e7-4d7d-eb06-0a3617f992f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "print(agent.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-DNR1xBn_a4"
      },
      "outputs": [],
      "source": [
        "policy = agent.policy.to(device)\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-4)\n",
        "pred_opt = torch.optim.Adam(agent.predictor.parameters(), lr=5e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHKw1KavYzmZ"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxfsi1fWUlmw"
      },
      "outputs": [],
      "source": [
        "policy.load_state_dict(torch.load(save_path,map_location='cpu'))\n",
        "if os.path.exists(pred_path):\n",
        "    agent.predictor.load_state_dict(torch.load(pred_path, map_location='cpu'))\n",
        "else:\n",
        "    torch.save(agent.predictor.state_dict(), pred_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPEdE5PypIkm"
      },
      "outputs": [],
      "source": [
        "pure_noise = {\"randbot\", \"pibot\", \"textbot\", \"debruijn81\"}\n",
        "\n",
        "hard = {\"mixed_strategy\", \"robertot\", \"foxtrotbot\", \"markov5\", \"markovbails\" \"russrocker4\",\n",
        "        \"shofar\", \"peterbot\", \"phasenbott\", \"sunNervebot\", \"greenberg\", \"iocainebot\"}\n",
        "\n",
        "special = {\"greenberg\", \"iocainebot\", \"markovbails\", \"phasenbott\", \"markov5\"}\n",
        "\n",
        "superspec = {\"markovbails\", \"phasenbott\"}\n",
        "\n",
        "def sample_opponent(bot_strength, names):\n",
        "    strengths = np.array(bot_strength, dtype=np.float32)\n",
        "\n",
        "    # convert â€œeasierâ€ (very negative) â†’ smaller positive weight\n",
        "    # and â€œharderâ€ (less negative / positive) â†’ larger weight\n",
        "    base = (strengths - strengths.mean())/strengths.std()\n",
        "\n",
        "    # squash to keep extremes reasonable\n",
        "    base = np.exp(base)  # gentle compression\n",
        "\n",
        "    # manual downweight for pure random-ish bots\n",
        "    for i, name in enumerate(names):\n",
        "        if name in pure_noise:\n",
        "            base[i] = 0.0\n",
        "        if name in hard:\n",
        "            base[i] * 100.0\n",
        "        else:\n",
        "            base[i] *= 0.1\n",
        "        if name in special:\n",
        "            base[i] *= 10.0\n",
        "        if name in superspec:\n",
        "            base[i] *= 2.0\n",
        "    probs = base / base.sum()\n",
        "    return np.random.choice(len(names), p=probs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbljieKSw3zx",
        "outputId": "11ac1d5f-8ae2-497f-b5e0-43a3cb31eb8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Loaded saved bot strengths.\n"
          ]
        }
      ],
      "source": [
        "num_bots = 43\n",
        "if os.path.exists(\"/content/drive/MyDrive/bot_strength.npy\"):\n",
        "    bot_strength = np.load(\"/content/drive/MyDrive/bot_strength.npy\")\n",
        "    print(\"ðŸ”„ Loaded saved bot strengths.\")\n",
        "else:\n",
        "    bot_strength = np.full(43, 0.0, dtype=np.float32)\n",
        "    print(\"âœ¨ Initialized new bot strengths.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWTZXBJ33SyS"
      },
      "outputs": [],
      "source": [
        "rounds = 200\n",
        "bots_per_round = 5  # number of different bots per round\n",
        "episodes_per_bot = 1\n",
        "gamma = 0.977237221\n",
        "k = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANf9yiIKik-X"
      },
      "outputs": [],
      "source": [
        "ema_alpha = 0.2\n",
        "ema_perf = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_FfF2So-U_g",
        "outputId": "954a80ed-0f4e-4a46-9bc4-897d263cec0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('phasenbott', np.float32(-0.38462558)), ('randbot', np.float32(-0.55250835)), ('markovbails', np.float32(-0.8252956)), ('pibot', np.float32(-0.9155607)), ('iocainebot', np.float32(-1.1841633)), ('debruijn81', np.float32(-1.7767735)), ('actr_lag2_decay', np.float32(-3.0306308)), ('markov5', np.float32(-3.954097)), ('textbot', np.float32(-4.3785696)), ('greenberg', np.float32(-4.7941546)), ('russrocker4', np.float32(-6.012001)), ('shofar', np.float32(-9.743508)), ('biopic', np.float32(-9.896416)), ('mod1bot', np.float32(-10.075775)), ('boom', np.float32(-10.432556)), ('flatbot3', np.float32(-12.9398365)), ('adddriftbot2', np.float32(-15.487335)), ('robertot', np.float32(-16.168736)), ('sunNervebot', np.float32(-16.579884)), ('switchalot', np.float32(-17.066345)), ('driftbot', np.float32(-17.140211)), ('inocencio', np.float32(-17.680489)), ('foxtrotbot', np.float32(-18.978935)), ('piedra', np.float32(-18.981367)), ('halbot', np.float32(-19.838905)), ('sweetrock', np.float32(-20.445265)), ('predbot', np.float32(-21.70043)), ('marble', np.float32(-22.019934)), ('granite', np.float32(-22.461527)), ('r226bot', np.float32(-23.604647)), ('peterbot', np.float32(-25.901909)), ('sunCrazybot', np.float32(-26.902584)), ('switchbot', np.float32(-27.695913)), ('mixed_strategy', np.float32(-29.12715)), ('zq_move', np.float32(-32.78715)), ('antirotnbot', np.float32(-34.362442)), ('addshiftbot3', np.float32(-36.032745)), ('multibot', np.float32(-37.398514)), ('antiflatbot', np.float32(-49.269066)), ('copybot', np.float32(-50.70533)), ('rotatebot', np.float32(-51.311413)), ('freqbot2', np.float32(-55.277615)), ('rockbot', np.float32(-87.03013))]\n"
          ]
        }
      ],
      "source": [
        "print(sorted(zip(roshambo_bot_names, bot_strength), key=lambda x: x[1], reverse=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPPm9dfwpIla",
        "outputId": "bc455bce-b7b4-4474-c64f-9859371acec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'actr_lag2_decay': 0, 'adddriftbot2': 1, 'addshiftbot3': 2, 'antiflatbot': 3, 'antirotnbot': 4, 'biopic': 5, 'boom': 6, 'copybot': 7, 'debruijn81': 8, 'driftbot': 9, 'flatbot3': 10, 'foxtrotbot': 11, 'freqbot2': 12, 'granite': 13, 'greenberg': 14, 'halbot': 15, 'inocencio': 16, 'iocainebot': 17, 'marble': 18, 'markov5': 19, 'markovbails': 20, 'mixed_strategy': 21, 'mod1bot': 22, 'multibot': 23, 'peterbot': 24, 'phasenbott': 25, 'pibot': 26, 'piedra': 27, 'predbot': 28, 'r226bot': 29, 'randbot': 30, 'robertot': 31, 'rockbot': 32, 'rotatebot': 33, 'russrocker4': 34, 'shofar': 35, 'sunCrazybot': 36, 'sunNervebot': 37, 'sweetrock': 38, 'switchalot': 39, 'switchbot': 40, 'textbot': 41, 'zq_move': 42}\n"
          ]
        }
      ],
      "source": [
        "print(roshambo_bot_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3V1Zo69yvNE",
        "outputId": "8ac54025-1701-43e0-9b3e-d445a443bbf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bot=25, name=phasenbott strength=2.42\n",
            "bot=30, name=randbot strength=-0.55\n",
            "bot=26, name=pibot strength=-0.92\n",
            "bot=20, name=markovbails strength=-1.27\n",
            "bot=08, name=debruijn81 strength=-1.78\n",
            "bot=19, name=markov5 strength=-3.13\n",
            "bot=14, name=greenberg strength=-3.24\n",
            "bot=41, name=textbot strength=-4.38\n",
            "bot=00, name=actr_lag2_decay strength=-5.44\n",
            "bot=17, name=iocainebot strength=-5.88\n",
            "bot=34, name=russrocker4 strength=-6.11\n",
            "bot=35, name=shofar strength=-9.28\n",
            "bot=05, name=biopic strength=-10.03\n",
            "bot=06, name=boom strength=-10.86\n",
            "bot=22, name=mod1bot strength=-12.89\n",
            "bot=10, name=flatbot3 strength=-14.33\n",
            "bot=01, name=adddriftbot2 strength=-16.01\n",
            "bot=31, name=robertot strength=-16.24\n",
            "bot=39, name=switchalot strength=-17.65\n",
            "bot=16, name=inocencio strength=-19.58\n",
            "bot=09, name=driftbot strength=-19.70\n",
            "bot=27, name=piedra strength=-20.00\n",
            "bot=37, name=sunNervebot strength=-20.08\n",
            "bot=15, name=halbot strength=-20.31\n",
            "bot=38, name=sweetrock strength=-21.80\n",
            "bot=11, name=foxtrotbot strength=-24.06\n",
            "bot=29, name=r226bot strength=-24.18\n",
            "bot=13, name=granite strength=-24.33\n",
            "bot=28, name=predbot strength=-25.03\n",
            "bot=18, name=marble strength=-25.61\n",
            "bot=36, name=sunCrazybot strength=-27.84\n",
            "bot=24, name=peterbot strength=-28.20\n",
            "bot=04, name=antirotnbot strength=-34.53\n",
            "bot=40, name=switchbot strength=-34.67\n",
            "bot=42, name=zq_move strength=-35.55\n",
            "bot=21, name=mixed_strategy strength=-35.83\n",
            "bot=02, name=addshiftbot3 strength=-36.77\n",
            "bot=23, name=multibot strength=-37.78\n",
            "bot=07, name=copybot strength=-50.71\n",
            "bot=03, name=antiflatbot strength=-52.66\n",
            "bot=33, name=rotatebot strength=-53.56\n",
            "bot=12, name=freqbot2 strength=-57.18\n",
            "bot=32, name=rockbot strength=-87.03\n"
          ]
        }
      ],
      "source": [
        "for (i, name, strength) in sorted(zip(roshambo_bot_ids.values(), roshambo_bot_names, bot_strength), key=lambda x: x[2], reverse=True):\n",
        "    print(f\"bot={i:02}, name={name:02} strength={strength:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkwPiVWpw23O",
        "outputId": "e7b7ab8c-d0a0-489e-db21-882650720d32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25\n"
          ]
        }
      ],
      "source": [
        "print(np.argmax(bot_strength))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9TxWiw4eW5s",
        "outputId": "db5190cd-1e8d-4bc9-9fd1-f2b44ff44720"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32\n"
          ]
        }
      ],
      "source": [
        "print(np.argmin(bot_strength))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOdL1mT6mVZh"
      },
      "outputs": [],
      "source": [
        "for g in optimizer.param_groups:\n",
        "            g[\"lr\"] = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeqiOikxD1Y_"
      },
      "outputs": [],
      "source": [
        "for g in pred_opt.param_groups:\n",
        "      g['lr'] = 1e-7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMnDCcmC8Wem"
      },
      "outputs": [],
      "source": [
        "for id in range(num_bots):\n",
        "    if roshambo_bot_names[id] in pure_noise:\n",
        "        bot_strength[id] -= 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ziaJX-c_ji_c",
        "outputId": "0972c28f-b194-4250-8f91-3b88e739fc4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¾ Saved weights at iter 0\n",
            "[round 0000] loss=-0.224 avgR=-17.00 EMA_R=-17.00 |\n",
            "hardest=08(debruijn81), strength=-1.78\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.07\n",
            "sampled=19, markov5, prev_strength=-4.21, curr_strength=-3.77\n",
            "sampled=25, phasenbott, prev_strength=1.10, curr_strength=1.07\n",
            "sampled=19, markov5, prev_strength=-4.09, curr_strength=-3.77\n",
            "sampled=14, greenberg, prev_strength=-2.28, curr_strength=-2.02\n",
            "sampled=17, iocainebot, prev_strength=-3.90, curr_strength=-3.60\n",
            "ðŸ’¾ Saved weights at iter 1\n",
            "[round 0001] loss=8.390 avgR=-6.20 EMA_R=-14.84 |\n",
            "hardest=08(debruijn81), strength=-1.78\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.22\n",
            "sampled=25, phasenbott, prev_strength=1.07, curr_strength=1.22\n",
            "sampled=25, phasenbott, prev_strength=1.16, curr_strength=1.22\n",
            "sampled=19, markov5, prev_strength=-3.77, curr_strength=-3.72\n",
            "sampled=14, greenberg, prev_strength=-2.02, curr_strength=-1.84\n",
            "sampled=14, greenberg, prev_strength=-2.40, curr_strength=-1.84\n",
            "ðŸ’¾ Saved weights at iter 2\n",
            "[round 0002] loss=-0.194 avgR=-5.60 EMA_R=-12.99 |\n",
            "hardest=20(markovbails), strength=-1.72\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.32\n",
            "sampled=19, markov5, prev_strength=-3.72, curr_strength=-3.90\n",
            "sampled=25, phasenbott, prev_strength=1.22, curr_strength=1.32\n",
            "sampled=25, phasenbott, prev_strength=1.33, curr_strength=1.32\n",
            "sampled=20, markovbails, prev_strength=-2.15, curr_strength=-1.72\n",
            "sampled=19, markov5, prev_strength=-3.74, curr_strength=-3.90\n",
            "ðŸ’¾ Saved weights at iter 3\n",
            "[round 0003] loss=5.110 avgR=-15.40 EMA_R=-13.47 |\n",
            "hardest=20(markovbails), strength=-1.72\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.60\n",
            "sampled=19, markov5, prev_strength=-3.90, curr_strength=-3.81\n",
            "sampled=17, iocainebot, prev_strength=-3.60, curr_strength=-3.11\n",
            "sampled=17, iocainebot, prev_strength=-3.36, curr_strength=-3.11\n",
            "sampled=14, greenberg, prev_strength=-1.84, curr_strength=-1.83\n",
            "sampled=25, phasenbott, prev_strength=1.32, curr_strength=1.60\n",
            "ðŸ’¾ Saved weights at iter 4\n",
            "[round 0004] loss=2.546 avgR=25.00 EMA_R=-5.78 |\n",
            "hardest=20(markovbails), strength=-1.45\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.31\n",
            "sampled=16, inocencio, prev_strength=-18.92, curr_strength=-19.58\n",
            "sampled=17, iocainebot, prev_strength=-3.11, curr_strength=-3.42\n",
            "sampled=20, markovbails, prev_strength=-1.72, curr_strength=-1.45\n",
            "sampled=17, iocainebot, prev_strength=-3.28, curr_strength=-3.42\n",
            "sampled=25, phasenbott, prev_strength=1.60, curr_strength=1.31\n",
            "ðŸ’¾ Saved weights at iter 5\n",
            "[round 0005] loss=0.037 avgR=9.80 EMA_R=-2.66 |\n",
            "hardest=20(markovbails), strength=-1.45\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.08\n",
            "sampled=17, iocainebot, prev_strength=-3.42, curr_strength=-3.26\n",
            "sampled=17, iocainebot, prev_strength=-3.26, curr_strength=-3.26\n",
            "sampled=19, markov5, prev_strength=-3.81, curr_strength=-3.70\n",
            "sampled=25, phasenbott, prev_strength=1.31, curr_strength=1.08\n",
            "sampled=14, greenberg, prev_strength=-1.83, curr_strength=-2.25\n",
            "ðŸ’¾ Saved weights at iter 6\n",
            "[round 0006] loss=1.024 avgR=0.60 EMA_R=-2.01 |\n",
            "hardest=20(markovbails), strength=-1.14\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.09\n",
            "sampled=17, iocainebot, prev_strength=-3.26, curr_strength=-3.53\n",
            "sampled=25, phasenbott, prev_strength=1.08, curr_strength=1.09\n",
            "sampled=20, markovbails, prev_strength=-1.45, curr_strength=-1.14\n",
            "sampled=14, greenberg, prev_strength=-2.25, curr_strength=-2.23\n",
            "sampled=17, iocainebot, prev_strength=-3.19, curr_strength=-3.53\n",
            "ðŸ’¾ Saved weights at iter 7\n",
            "[round 0007] loss=2.480 avgR=6.00 EMA_R=-0.41 |\n",
            "hardest=20(markovbails), strength=-1.14\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.19\n",
            "sampled=25, phasenbott, prev_strength=1.09, curr_strength=1.19\n",
            "sampled=19, markov5, prev_strength=-3.70, curr_strength=-3.71\n",
            "sampled=19, markov5, prev_strength=-3.48, curr_strength=-3.71\n",
            "sampled=25, phasenbott, prev_strength=1.28, curr_strength=1.19\n",
            "sampled=14, greenberg, prev_strength=-2.23, curr_strength=-2.55\n",
            "ðŸ’¾ Saved weights at iter 8\n",
            "[round 0008] loss=0.476 avgR=53.00 EMA_R=10.27 |\n",
            "hardest=20(markovbails), strength=-1.14\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=0.40\n",
            "sampled=40, switchbot, prev_strength=-32.42, curr_strength=-33.77\n",
            "sampled=25, phasenbott, prev_strength=1.19, curr_strength=0.40\n",
            "sampled=19, markov5, prev_strength=-3.71, curr_strength=-3.83\n",
            "sampled=25, phasenbott, prev_strength=0.75, curr_strength=0.40\n",
            "sampled=19, markov5, prev_strength=-3.86, curr_strength=-3.83\n",
            "ðŸ’¾ Saved weights at iter 9\n",
            "[round 0009] loss=8.702 avgR=-13.00 EMA_R=5.62 |\n",
            "hardest=20(markovbails), strength=-1.14\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=0.72\n",
            "sampled=25, phasenbott, prev_strength=0.40, curr_strength=0.72\n",
            "sampled=14, greenberg, prev_strength=-2.55, curr_strength=-2.54\n",
            "sampled=17, iocainebot, prev_strength=-3.53, curr_strength=-3.17\n",
            "sampled=25, phasenbott, prev_strength=0.50, curr_strength=0.72\n",
            "sampled=19, markov5, prev_strength=-3.83, curr_strength=-3.78\n",
            "ðŸ’¾ Saved weights at iter 10\n",
            "[round 0010] loss=-0.021 avgR=17.20 EMA_R=7.93 |\n",
            "hardest=20(markovbails), strength=-1.14\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=0.29\n",
            "sampled=14, greenberg, prev_strength=-2.54, curr_strength=-3.00\n",
            "sampled=14, greenberg, prev_strength=-2.73, curr_strength=-3.00\n",
            "sampled=17, iocainebot, prev_strength=-3.17, curr_strength=-3.06\n",
            "sampled=25, phasenbott, prev_strength=0.72, curr_strength=0.29\n",
            "sampled=25, phasenbott, prev_strength=0.67, curr_strength=0.29\n",
            "ðŸ’¾ Saved weights at iter 11\n",
            "[round 0011] loss=-1.272 avgR=4.20 EMA_R=7.19 |\n",
            "hardest=20(markovbails), strength=-1.14\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=-0.14\n",
            "sampled=25, phasenbott, prev_strength=0.29, curr_strength=-0.14\n",
            "sampled=25, phasenbott, prev_strength=0.37, curr_strength=-0.14\n",
            "sampled=14, greenberg, prev_strength=-3.00, curr_strength=-3.03\n",
            "sampled=19, markov5, prev_strength=-3.78, curr_strength=-3.48\n",
            "sampled=25, phasenbott, prev_strength=0.07, curr_strength=-0.14\n",
            "ðŸ’¾ Saved weights at iter 12\n",
            "[round 0012] loss=-2.608 avgR=7.00 EMA_R=7.15 |\n",
            "hardest=20(markovbails), strength=-1.14\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=-0.42\n",
            "sampled=25, phasenbott, prev_strength=-0.14, curr_strength=-0.42\n",
            "sampled=17, iocainebot, prev_strength=-3.06, curr_strength=-2.90\n",
            "sampled=14, greenberg, prev_strength=-3.03, curr_strength=-2.43\n",
            "sampled=9, driftbot, prev_strength=-19.12, curr_strength=-19.70\n",
            "sampled=25, phasenbott, prev_strength=-0.26, curr_strength=-0.42\n",
            "ðŸ’¾ Saved weights at iter 13\n",
            "[round 0013] loss=-3.661 avgR=13.40 EMA_R=8.40 |\n",
            "hardest=20(markovbails), strength=-1.11\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=25(phasenbott), strength=-0.80\n",
            "hardest=30(randbot), strength=-0.55\n",
            "sampled=25, phasenbott, prev_strength=-0.42, curr_strength=-0.80\n",
            "sampled=17, iocainebot, prev_strength=-2.90, curr_strength=-3.14\n",
            "sampled=17, iocainebot, prev_strength=-2.78, curr_strength=-3.14\n",
            "sampled=20, markovbails, prev_strength=-1.14, curr_strength=-1.11\n",
            "sampled=25, phasenbott, prev_strength=-0.60, curr_strength=-0.80\n",
            "ðŸ’¾ Saved weights at iter 14\n",
            "[round 0014] loss=0.591 avgR=22.20 EMA_R=11.16 |\n",
            "hardest=20(markovbails), strength=-1.11\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=25(phasenbott), strength=-0.80\n",
            "hardest=30(randbot), strength=-0.55\n",
            "sampled=17, iocainebot, prev_strength=-3.14, curr_strength=-3.86\n",
            "sampled=17, iocainebot, prev_strength=-3.37, curr_strength=-3.86\n",
            "sampled=19, markov5, prev_strength=-3.48, curr_strength=-3.62\n",
            "sampled=14, greenberg, prev_strength=-2.43, curr_strength=-2.52\n",
            "sampled=17, iocainebot, prev_strength=-3.19, curr_strength=-3.86\n",
            "ðŸ’¾ Saved weights at iter 15\n",
            "[round 0015] loss=1.572 avgR=4.60 EMA_R=9.85 |\n",
            "hardest=20(markovbails), strength=-1.11\n",
            "hardest=25(phasenbott), strength=-0.96\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "sampled=0, actr_lag2_decay, prev_strength=-5.26, curr_strength=-5.44\n",
            "sampled=31, robertot, prev_strength=-16.64, curr_strength=-16.24\n",
            "sampled=25, phasenbott, prev_strength=-0.80, curr_strength=-0.96\n",
            "sampled=19, markov5, prev_strength=-3.62, curr_strength=-3.39\n",
            "sampled=17, iocainebot, prev_strength=-3.86, curr_strength=-4.08\n",
            "ðŸ’¾ Saved weights at iter 16\n",
            "[round 0016] loss=-0.587 avgR=-9.00 EMA_R=6.08 |\n",
            "hardest=25(phasenbott), strength=-1.05\n",
            "hardest=20(markovbails), strength=-0.93\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "sampled=20, markovbails, prev_strength=-1.11, curr_strength=-0.93\n",
            "sampled=19, markov5, prev_strength=-3.39, curr_strength=-2.94\n",
            "sampled=19, markov5, prev_strength=-3.19, curr_strength=-2.94\n",
            "sampled=25, phasenbott, prev_strength=-0.96, curr_strength=-1.05\n",
            "sampled=25, phasenbott, prev_strength=-0.85, curr_strength=-1.05\n",
            "ðŸ’¾ Saved weights at iter 17\n",
            "[round 0017] loss=2.760 avgR=11.80 EMA_R=7.22 |\n",
            "hardest=25(phasenbott), strength=-1.55\n",
            "hardest=20(markovbails), strength=-0.93\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "sampled=19, markov5, prev_strength=-2.94, curr_strength=-2.88\n",
            "sampled=25, phasenbott, prev_strength=-1.05, curr_strength=-1.55\n",
            "sampled=37, sunNervebot, prev_strength=-19.86, curr_strength=-19.90\n",
            "sampled=25, phasenbott, prev_strength=-1.52, curr_strength=-1.55\n",
            "sampled=17, iocainebot, prev_strength=-4.08, curr_strength=-3.88\n",
            "ðŸ’¾ Saved weights at iter 18\n",
            "[round 0018] loss=2.473 avgR=-4.00 EMA_R=4.98 |\n",
            "hardest=20(markovbails), strength=-0.93\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=25(phasenbott), strength=-0.71\n",
            "hardest=30(randbot), strength=-0.55\n",
            "sampled=17, iocainebot, prev_strength=-3.88, curr_strength=-4.13\n",
            "sampled=25, phasenbott, prev_strength=-1.55, curr_strength=-0.71\n",
            "sampled=25, phasenbott, prev_strength=-1.42, curr_strength=-0.71\n",
            "sampled=19, markov5, prev_strength=-2.88, curr_strength=-3.18\n",
            "sampled=25, phasenbott, prev_strength=-0.80, curr_strength=-0.71\n",
            "ðŸ’¾ Saved weights at iter 19\n",
            "[round 0019] loss=-1.360 avgR=2.60 EMA_R=4.50 |\n",
            "hardest=20(markovbails), strength=-1.06\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=-0.41\n",
            "sampled=17, iocainebot, prev_strength=-4.13, curr_strength=-4.42\n",
            "sampled=14, greenberg, prev_strength=-2.52, curr_strength=-2.42\n",
            "sampled=25, phasenbott, prev_strength=-0.71, curr_strength=-0.41\n",
            "sampled=25, phasenbott, prev_strength=-0.47, curr_strength=-0.41\n",
            "sampled=20, markovbails, prev_strength=-0.93, curr_strength=-1.06\n",
            "ðŸ’¾ Saved weights at iter 20\n",
            "[round 0020] loss=-4.463 avgR=-5.40 EMA_R=2.52 |\n",
            "hardest=20(markovbails), strength=-1.06\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=0.04\n",
            "sampled=25, phasenbott, prev_strength=-0.41, curr_strength=0.04\n",
            "sampled=25, phasenbott, prev_strength=-0.22, curr_strength=0.04\n",
            "sampled=25, phasenbott, prev_strength=0.62, curr_strength=0.04\n",
            "sampled=25, phasenbott, prev_strength=0.30, curr_strength=0.04\n",
            "sampled=19, markov5, prev_strength=-3.18, curr_strength=-3.34\n",
            "ðŸ’¾ Saved weights at iter 21\n",
            "[round 0021] loss=-4.956 avgR=8.20 EMA_R=3.66 |\n",
            "hardest=20(markovbails), strength=-1.06\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=0.08\n",
            "sampled=14, greenberg, prev_strength=-2.42, curr_strength=-2.46\n",
            "sampled=14, greenberg, prev_strength=-2.41, curr_strength=-2.46\n",
            "sampled=35, shofar, prev_strength=-9.03, curr_strength=-9.31\n",
            "sampled=25, phasenbott, prev_strength=0.04, curr_strength=0.08\n",
            "sampled=25, phasenbott, prev_strength=-0.10, curr_strength=0.08\n",
            "ðŸ’¾ Saved weights at iter 22\n",
            "[round 0022] loss=1.048 avgR=7.80 EMA_R=4.49 |\n",
            "hardest=20(markovbails), strength=-1.06\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=0.31\n",
            "sampled=25, phasenbott, prev_strength=0.08, curr_strength=0.31\n",
            "sampled=17, iocainebot, prev_strength=-4.42, curr_strength=-4.62\n",
            "sampled=35, shofar, prev_strength=-9.31, curr_strength=-9.34\n",
            "sampled=19, markov5, prev_strength=-3.34, curr_strength=-3.56\n",
            "sampled=25, phasenbott, prev_strength=0.10, curr_strength=0.31\n",
            "ðŸ’¾ Saved weights at iter 23\n",
            "[round 0023] loss=-6.240 avgR=12.80 EMA_R=6.15 |\n",
            "hardest=20(markovbails), strength=-1.06\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=0.21\n",
            "sampled=17, iocainebot, prev_strength=-4.62, curr_strength=-4.89\n",
            "sampled=19, markov5, prev_strength=-3.56, curr_strength=-3.16\n",
            "sampled=25, phasenbott, prev_strength=0.31, curr_strength=0.21\n",
            "sampled=25, phasenbott, prev_strength=0.24, curr_strength=0.21\n",
            "sampled=14, greenberg, prev_strength=-2.46, curr_strength=-3.02\n",
            "ðŸ’¾ Saved weights at iter 24\n",
            "[round 0024] loss=3.882 avgR=-23.60 EMA_R=0.20 |\n",
            "hardest=20(markovbails), strength=-1.06\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=0.96\n",
            "sampled=25, phasenbott, prev_strength=0.21, curr_strength=0.96\n",
            "sampled=25, phasenbott, prev_strength=0.27, curr_strength=0.96\n",
            "sampled=17, iocainebot, prev_strength=-4.89, curr_strength=-4.43\n",
            "sampled=25, phasenbott, prev_strength=0.59, curr_strength=0.96\n",
            "sampled=25, phasenbott, prev_strength=0.66, curr_strength=0.96\n",
            "ðŸ’¾ Saved weights at iter 25\n",
            "[round 0025] loss=1.487 avgR=6.80 EMA_R=1.52 |\n",
            "hardest=20(markovbails), strength=-0.98\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.47\n",
            "sampled=17, iocainebot, prev_strength=-4.43, curr_strength=-4.36\n",
            "sampled=19, markov5, prev_strength=-3.16, curr_strength=-3.78\n",
            "sampled=25, phasenbott, prev_strength=0.96, curr_strength=1.47\n",
            "sampled=14, greenberg, prev_strength=-3.02, curr_strength=-3.30\n",
            "sampled=20, markovbails, prev_strength=-1.06, curr_strength=-0.98\n",
            "ðŸ’¾ Saved weights at iter 26\n",
            "[round 0026] loss=1.646 avgR=1.00 EMA_R=1.42 |\n",
            "hardest=20(markovbails), strength=-0.98\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.64\n",
            "sampled=25, phasenbott, prev_strength=1.47, curr_strength=1.64\n",
            "sampled=25, phasenbott, prev_strength=1.39, curr_strength=1.64\n",
            "sampled=14, greenberg, prev_strength=-3.30, curr_strength=-3.45\n",
            "sampled=17, iocainebot, prev_strength=-4.36, curr_strength=-4.40\n",
            "sampled=25, phasenbott, prev_strength=1.24, curr_strength=1.64\n",
            "ðŸ’¾ Saved weights at iter 27\n",
            "[round 0027] loss=-1.567 avgR=-14.40 EMA_R=-1.75 |\n",
            "hardest=20(markovbails), strength=-0.98\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.49\n",
            "sampled=14, greenberg, prev_strength=-3.45, curr_strength=-3.01\n",
            "sampled=25, phasenbott, prev_strength=1.64, curr_strength=1.49\n",
            "sampled=19, markov5, prev_strength=-3.78, curr_strength=-3.32\n",
            "sampled=25, phasenbott, prev_strength=1.60, curr_strength=1.49\n",
            "sampled=25, phasenbott, prev_strength=1.23, curr_strength=1.49\n",
            "ðŸ’¾ Saved weights at iter 28\n",
            "[round 0028] loss=-5.869 avgR=35.40 EMA_R=5.68 |\n",
            "hardest=20(markovbails), strength=-0.98\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.09\n",
            "sampled=25, phasenbott, prev_strength=1.49, curr_strength=1.09\n",
            "sampled=17, iocainebot, prev_strength=-4.40, curr_strength=-4.52\n",
            "sampled=19, markov5, prev_strength=-3.32, curr_strength=-3.68\n",
            "sampled=21, mixed_strategy, prev_strength=-34.48, curr_strength=-34.98\n",
            "sampled=25, phasenbott, prev_strength=1.37, curr_strength=1.09\n",
            "ðŸ’¾ Saved weights at iter 29\n",
            "[round 0029] loss=1.715 avgR=15.60 EMA_R=7.67 |\n",
            "hardest=20(markovbails), strength=-1.36\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=0.74\n",
            "sampled=17, iocainebot, prev_strength=-4.52, curr_strength=-3.97\n",
            "sampled=20, markovbails, prev_strength=-0.98, curr_strength=-1.36\n",
            "sampled=25, phasenbott, prev_strength=1.09, curr_strength=0.74\n",
            "sampled=25, phasenbott, prev_strength=1.07, curr_strength=0.74\n",
            "sampled=14, greenberg, prev_strength=-3.01, curr_strength=-3.55\n",
            "ðŸ’¾ Saved weights at iter 30\n",
            "[round 0030] loss=1.286 avgR=28.80 EMA_R=11.89 |\n",
            "hardest=20(markovbails), strength=-1.36\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=0.34\n",
            "sampled=11, foxtrotbot, prev_strength=-24.15, curr_strength=-24.00\n",
            "sampled=17, iocainebot, prev_strength=-3.97, curr_strength=-4.73\n",
            "sampled=14, greenberg, prev_strength=-3.55, curr_strength=-3.63\n",
            "sampled=17, iocainebot, prev_strength=-3.83, curr_strength=-4.73\n",
            "sampled=25, phasenbott, prev_strength=0.74, curr_strength=0.34\n",
            "ðŸ’¾ Saved weights at iter 31\n",
            "[round 0031] loss=-0.990 avgR=-18.20 EMA_R=5.87 |\n",
            "hardest=20(markovbails), strength=-1.36\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.10\n",
            "sampled=17, iocainebot, prev_strength=-4.73, curr_strength=-4.50\n",
            "sampled=25, phasenbott, prev_strength=0.34, curr_strength=1.10\n",
            "sampled=17, iocainebot, prev_strength=-4.80, curr_strength=-4.50\n",
            "sampled=25, phasenbott, prev_strength=0.33, curr_strength=1.10\n",
            "sampled=25, phasenbott, prev_strength=0.61, curr_strength=1.10\n",
            "ðŸ’¾ Saved weights at iter 32\n",
            "[round 0032] loss=-1.404 avgR=18.40 EMA_R=8.38 |\n",
            "hardest=20(markovbails), strength=-1.36\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.13\n",
            "sampled=19, markov5, prev_strength=-3.68, curr_strength=-4.03\n",
            "sampled=19, markov5, prev_strength=-3.60, curr_strength=-4.03\n",
            "sampled=25, phasenbott, prev_strength=1.10, curr_strength=1.13\n",
            "sampled=35, shofar, prev_strength=-9.34, curr_strength=-9.56\n",
            "sampled=14, greenberg, prev_strength=-3.63, curr_strength=-3.81\n",
            "ðŸ’¾ Saved weights at iter 33\n",
            "[round 0033] loss=-0.484 avgR=22.40 EMA_R=11.18 |\n",
            "hardest=20(markovbails), strength=-1.36\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.25\n",
            "sampled=25, phasenbott, prev_strength=1.13, curr_strength=1.25\n",
            "sampled=19, markov5, prev_strength=-4.03, curr_strength=-4.27\n",
            "sampled=19, markov5, prev_strength=-4.23, curr_strength=-4.27\n",
            "sampled=17, iocainebot, prev_strength=-4.50, curr_strength=-4.97\n",
            "sampled=22, mod1bot, prev_strength=-12.59, curr_strength=-12.89\n",
            "ðŸ’¾ Saved weights at iter 34\n",
            "[round 0034] loss=-0.180 avgR=10.00 EMA_R=10.95 |\n",
            "hardest=20(markovbails), strength=-1.36\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.42\n",
            "sampled=17, iocainebot, prev_strength=-4.97, curr_strength=-5.58\n",
            "sampled=17, iocainebot, prev_strength=-5.03, curr_strength=-5.58\n",
            "sampled=25, phasenbott, prev_strength=1.25, curr_strength=1.42\n",
            "sampled=25, phasenbott, prev_strength=1.55, curr_strength=1.42\n",
            "sampled=25, phasenbott, prev_strength=1.21, curr_strength=1.42\n",
            "ðŸ’¾ Saved weights at iter 35\n",
            "[round 0035] loss=-2.582 avgR=15.00 EMA_R=11.76 |\n",
            "hardest=20(markovbails), strength=-1.18\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.51\n",
            "sampled=40, switchbot, prev_strength=-33.77, curr_strength=-34.67\n",
            "sampled=25, phasenbott, prev_strength=1.42, curr_strength=1.51\n",
            "sampled=20, markovbails, prev_strength=-1.36, curr_strength=-1.18\n",
            "sampled=14, greenberg, prev_strength=-3.81, curr_strength=-3.58\n",
            "sampled=25, phasenbott, prev_strength=1.56, curr_strength=1.51\n",
            "ðŸ’¾ Saved weights at iter 36\n",
            "[round 0036] loss=9.233 avgR=-1.40 EMA_R=9.13 |\n",
            "hardest=20(markovbails), strength=-1.18\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.34\n",
            "sampled=19, markov5, prev_strength=-4.27, curr_strength=-3.97\n",
            "sampled=14, greenberg, prev_strength=-3.58, curr_strength=-3.62\n",
            "sampled=25, phasenbott, prev_strength=1.51, curr_strength=1.34\n",
            "sampled=25, phasenbott, prev_strength=1.82, curr_strength=1.34\n",
            "sampled=25, phasenbott, prev_strength=1.46, curr_strength=1.34\n",
            "ðŸ’¾ Saved weights at iter 37\n",
            "[round 0037] loss=5.340 avgR=14.80 EMA_R=10.26 |\n",
            "hardest=20(markovbails), strength=-1.18\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.12\n",
            "sampled=25, phasenbott, prev_strength=1.34, curr_strength=1.12\n",
            "sampled=19, markov5, prev_strength=-3.97, curr_strength=-3.78\n",
            "sampled=25, phasenbott, prev_strength=1.40, curr_strength=1.12\n",
            "sampled=24, peterbot, prev_strength=-28.01, curr_strength=-28.20\n",
            "sampled=14, greenberg, prev_strength=-3.62, curr_strength=-3.80\n",
            "ðŸ’¾ Saved weights at iter 38\n",
            "[round 0038] loss=-3.186 avgR=32.40 EMA_R=14.69 |\n",
            "hardest=20(markovbails), strength=-1.18\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.01\n",
            "sampled=25, phasenbott, prev_strength=1.12, curr_strength=1.01\n",
            "sampled=21, mixed_strategy, prev_strength=-34.98, curr_strength=-35.83\n",
            "sampled=11, foxtrotbot, prev_strength=-24.00, curr_strength=-24.06\n",
            "sampled=17, iocainebot, prev_strength=-5.58, curr_strength=-5.56\n",
            "sampled=25, phasenbott, prev_strength=1.05, curr_strength=1.01\n",
            "ðŸ’¾ Saved weights at iter 39\n",
            "[round 0039] loss=7.633 avgR=7.60 EMA_R=13.27 |\n",
            "hardest=20(markovbails), strength=-1.18\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=0.87\n",
            "sampled=25, phasenbott, prev_strength=1.01, curr_strength=0.87\n",
            "sampled=19, markov5, prev_strength=-3.78, curr_strength=-3.90\n",
            "sampled=25, phasenbott, prev_strength=0.55, curr_strength=0.87\n",
            "sampled=17, iocainebot, prev_strength=-5.56, curr_strength=-5.61\n",
            "sampled=25, phasenbott, prev_strength=1.06, curr_strength=0.87\n",
            "ðŸ’¾ Saved weights at iter 40\n",
            "[round 0040] loss=-3.805 avgR=-11.00 EMA_R=8.42 |\n",
            "hardest=20(markovbails), strength=-1.18\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=0.97\n",
            "sampled=25, phasenbott, prev_strength=0.87, curr_strength=0.97\n",
            "sampled=14, greenberg, prev_strength=-3.80, curr_strength=-3.46\n",
            "sampled=25, phasenbott, prev_strength=0.85, curr_strength=0.97\n",
            "sampled=14, greenberg, prev_strength=-3.27, curr_strength=-3.46\n",
            "sampled=19, markov5, prev_strength=-3.90, curr_strength=-3.70\n",
            "ðŸ’¾ Saved weights at iter 41\n",
            "[round 0041] loss=-0.979 avgR=0.80 EMA_R=6.89 |\n",
            "hardest=20(markovbails), strength=-1.18\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=0.74\n",
            "sampled=17, iocainebot, prev_strength=-5.61, curr_strength=-5.40\n",
            "sampled=25, phasenbott, prev_strength=0.97, curr_strength=0.74\n",
            "sampled=25, phasenbott, prev_strength=0.81, curr_strength=0.74\n",
            "sampled=25, phasenbott, prev_strength=0.65, curr_strength=0.74\n",
            "sampled=14, greenberg, prev_strength=-3.46, curr_strength=-3.41\n",
            "ðŸ’¾ Saved weights at iter 42\n",
            "[round 0042] loss=-2.478 avgR=-8.80 EMA_R=3.75 |\n",
            "hardest=20(markovbails), strength=-1.18\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.03\n",
            "sampled=20, markovbails, prev_strength=-1.18, curr_strength=-1.18\n",
            "sampled=25, phasenbott, prev_strength=0.74, curr_strength=1.03\n",
            "sampled=20, markovbails, prev_strength=-1.30, curr_strength=-1.18\n",
            "sampled=19, markov5, prev_strength=-3.70, curr_strength=-3.51\n",
            "sampled=25, phasenbott, prev_strength=0.78, curr_strength=1.03\n",
            "ðŸ’¾ Saved weights at iter 43\n",
            "[round 0043] loss=-4.706 avgR=9.40 EMA_R=4.88 |\n",
            "hardest=20(markovbails), strength=-1.18\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=0.87\n",
            "sampled=14, greenberg, prev_strength=-3.41, curr_strength=-3.68\n",
            "sampled=14, greenberg, prev_strength=-3.26, curr_strength=-3.68\n",
            "sampled=25, phasenbott, prev_strength=1.03, curr_strength=0.87\n",
            "sampled=25, phasenbott, prev_strength=0.89, curr_strength=0.87\n",
            "sampled=25, phasenbott, prev_strength=1.10, curr_strength=0.87\n",
            "ðŸ’¾ Saved weights at iter 44\n",
            "[round 0044] loss=-3.907 avgR=9.20 EMA_R=5.75 |\n",
            "hardest=20(markovbails), strength=-1.18\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=0.98\n",
            "sampled=14, greenberg, prev_strength=-3.68, curr_strength=-4.05\n",
            "sampled=25, phasenbott, prev_strength=0.87, curr_strength=0.98\n",
            "sampled=17, iocainebot, prev_strength=-5.40, curr_strength=-5.49\n",
            "sampled=14, greenberg, prev_strength=-4.06, curr_strength=-4.05\n",
            "sampled=25, phasenbott, prev_strength=1.13, curr_strength=0.98\n",
            "ðŸ’¾ Saved weights at iter 45\n",
            "[round 0045] loss=7.535 avgR=-5.80 EMA_R=3.44 |\n",
            "hardest=20(markovbails), strength=-1.18\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=0.88\n",
            "sampled=25, phasenbott, prev_strength=0.98, curr_strength=0.88\n",
            "sampled=19, markov5, prev_strength=-3.51, curr_strength=-3.35\n",
            "sampled=25, phasenbott, prev_strength=0.97, curr_strength=0.88\n",
            "sampled=25, phasenbott, prev_strength=0.74, curr_strength=0.88\n",
            "sampled=14, greenberg, prev_strength=-4.05, curr_strength=-3.77\n",
            "ðŸ’¾ Saved weights at iter 46\n",
            "[round 0046] loss=-13.214 avgR=2.40 EMA_R=3.23 |\n",
            "hardest=20(markovbails), strength=-1.18\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.13\n",
            "sampled=14, greenberg, prev_strength=-3.77, curr_strength=-3.72\n",
            "sampled=25, phasenbott, prev_strength=0.88, curr_strength=1.13\n",
            "sampled=17, iocainebot, prev_strength=-5.49, curr_strength=-5.84\n",
            "sampled=25, phasenbott, prev_strength=1.18, curr_strength=1.13\n",
            "sampled=25, phasenbott, prev_strength=1.22, curr_strength=1.13\n",
            "ðŸ’¾ Saved weights at iter 47\n",
            "[round 0047] loss=-4.240 avgR=-9.80 EMA_R=0.62 |\n",
            "hardest=20(markovbails), strength=-1.20\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=1.24\n",
            "sampled=20, markovbails, prev_strength=-1.18, curr_strength=-1.20\n",
            "sampled=25, phasenbott, prev_strength=1.13, curr_strength=1.24\n",
            "sampled=25, phasenbott, prev_strength=1.01, curr_strength=1.24\n",
            "sampled=14, greenberg, prev_strength=-3.72, curr_strength=-3.26\n",
            "sampled=14, greenberg, prev_strength=-3.61, curr_strength=-3.26\n",
            "ðŸ’¾ Saved weights at iter 48\n",
            "[round 0048] loss=9.341 avgR=-21.00 EMA_R=-3.70 |\n",
            "hardest=20(markovbails), strength=-1.20\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=2.24\n",
            "sampled=19, markov5, prev_strength=-3.35, curr_strength=-3.10\n",
            "sampled=17, iocainebot, prev_strength=-5.84, curr_strength=-5.99\n",
            "sampled=25, phasenbott, prev_strength=1.24, curr_strength=2.24\n",
            "sampled=25, phasenbott, prev_strength=1.56, curr_strength=2.24\n",
            "sampled=25, phasenbott, prev_strength=1.58, curr_strength=2.24\n",
            "ðŸ’¾ Saved weights at iter 49\n",
            "[round 0049] loss=0.223 avgR=-11.80 EMA_R=-5.32 |\n",
            "hardest=20(markovbails), strength=-1.20\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=2.46\n",
            "sampled=25, phasenbott, prev_strength=2.24, curr_strength=2.46\n",
            "sampled=25, phasenbott, prev_strength=2.16, curr_strength=2.46\n",
            "sampled=14, greenberg, prev_strength=-3.26, curr_strength=-2.87\n",
            "sampled=25, phasenbott, prev_strength=2.18, curr_strength=2.46\n",
            "sampled=19, markov5, prev_strength=-3.10, curr_strength=-3.13\n",
            "ðŸ’¾ Saved weights at iter 50\n",
            "[round 0050] loss=4.205 avgR=5.40 EMA_R=-3.18 |\n",
            "hardest=20(markovbails), strength=-1.27\n",
            "hardest=26(pibot), strength=-0.92\n",
            "hardest=30(randbot), strength=-0.55\n",
            "hardest=25(phasenbott), strength=2.42\n",
            "sampled=35, shofar, prev_strength=-9.56, curr_strength=-9.28\n",
            "sampled=25, phasenbott, prev_strength=2.46, curr_strength=2.42\n",
            "sampled=37, sunNervebot, prev_strength=-19.90, curr_strength=-20.08\n",
            "sampled=20, markovbails, prev_strength=-1.20, curr_strength=-1.27\n",
            "sampled=25, phasenbott, prev_strength=2.26, curr_strength=2.42\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3343911844.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroshambo_bot_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopponent_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         )\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mepisodes_round\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopponent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2619198721.py\u001b[0m in \u001b[0;36mcollect_episode\u001b[0;34m(env, agent, opponent)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0ma_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_evaluation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mo_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0macts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2314446399.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, time_step, is_evaluation)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_evaluation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;31m# If it is the end of the episode, don't select an action.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for it in range(rounds):\n",
        "\n",
        "    episodes_round = []\n",
        "    sampled_bot_ids = []\n",
        "    prev_bot_strength = []\n",
        "    # ---- collect episodes vs multiple bots ----\n",
        "    for _ in range(bots_per_round):\n",
        "        opponent_id = sample_opponent(bot_strength, roshambo_bot_names)\n",
        "        sampled_bot_ids.append(opponent_id)\n",
        "        prev_bot_strength.append(bot_strength[opponent_id])\n",
        "        opp = create_roshambo_bot_agent(\n",
        "            1, num_actions, roshambo_bot_names, opponent_id\n",
        "        )\n",
        "        ep = collect_episode(env, agent, opp)\n",
        "        episodes_round.append((opponent_id, ep))\n",
        "\n",
        "        # update difficulty EMA per-episode\n",
        "        ep_return = sum(step[-1] for step in ep) # Summing over collected CPU tensors\n",
        "        bot_strength[opponent_id] = (\n",
        "            0.99 * bot_strength[opponent_id] + 0.01 * (-ep_return)\n",
        "        )\n",
        "\n",
        "    sampled_bot_ids = np.array(sampled_bot_ids, dtype=np.int32)\n",
        "    prev_bot_strength = np.array(prev_bot_strength, dtype=np.float32)\n",
        "\n",
        "    # ---- REINFORCE update over the whole round ----\n",
        "    loss = reinforce_update_round(policy=policy,\n",
        "        optimizer=optimizer,\n",
        "        pred_opt=pred_opt,\n",
        "        episodes_round=episodes_round,\n",
        "        bot_strength=bot_strength,\n",
        "        gamma=gamma,\n",
        "        diff_scale_strength=0.4,\n",
        "        max_grad_norm=1.0,\n",
        "        update_pred=update_pred\n",
        "    )\n",
        "\n",
        "\n",
        "    # track a simple overall EMA performance metric (unweighted)\n",
        "    round_returns = [sum(step[-1] for step in ep) for _, ep in episodes_round]\n",
        "    avg_return = np.mean(round_returns) if round_returns else 0.0\n",
        "\n",
        "    if ema_perf is None:\n",
        "        ema_perf = avg_return\n",
        "    else:\n",
        "        ema_perf = (1 - ema_alpha) * ema_perf + ema_alpha * avg_return\n",
        "\n",
        "    # ---- LR schedule ----\n",
        "    if it == 500:\n",
        "        for g in optimizer.param_groups:\n",
        "            g[\"lr\"] = 1e-4\n",
        "    if it == 1500:\n",
        "        for g in optimizer.param_groups:\n",
        "            g[\"lr\"] = 5e-5\n",
        "\n",
        "    # ---- periodic save ----\n",
        "    if it % save_every == 0:\n",
        "        torch.save(policy.state_dict(), save_path)\n",
        "        # Save the agent's predictor which might still be on CPU\n",
        "        torch.save(agent.predictor.state_dict(), pred_path)\n",
        "        torch.save(optimizer.state_dict, save_path.replace(\".pt\", \"_opt.pt\"))\n",
        "        np.save(\"/content/drive/MyDrive/bot_strength.npy\", bot_strength)\n",
        "        print(f\"ðŸ’¾ Saved weights at iter {it}\")\n",
        "\n",
        "    # ---- debug print ----\n",
        "    # show hardest bot this round\n",
        "    hardest_id = np.argpartition(bot_strength, -k)[-k:]\n",
        "    print(\n",
        "    f\"[round {it:04}] \"\n",
        "    f\"loss={loss:.3f} \"\n",
        "    f\"avgR={avg_return:.2f} EMA_R={ema_perf:.2f} |\")\n",
        "    for id in hardest_id:\n",
        "        print(f\"hardest={id:02}({roshambo_bot_names[id]}), \"\n",
        "        f\"strength={bot_strength[id]:.2f}\"\n",
        "        )\n",
        "    for i, id in enumerate(sampled_bot_ids):\n",
        "        print(f\"sampled={id}, {roshambo_bot_names[id]}, \"\n",
        "              f\"prev_strength={prev_bot_strength[i]:.2f}, \"\n",
        "              f\"curr_strength={bot_strength[id]:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPMIIMZlK6g2"
      },
      "source": [
        "# RL Training with PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7h5NlhlV6fp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class IdentityConditionedValue(nn.Module):\n",
        "    def __init__(self, d_model=64, num_bots=43):\n",
        "        super().__init__()\n",
        "\n",
        "        # mirror the structure of IdentityConditionedPolicy\n",
        "        self.id_to_query = nn.Linear(num_bots * 2, d_model)\n",
        "        self.id_to_key   = nn.Linear(num_bots * 2, d_model)\n",
        "        self.id_to_value = nn.Linear(num_bots * 2, d_model)\n",
        "\n",
        "        self.feature_proj = nn.Linear(d_model * 3 + 9, d_model)\n",
        "        self.out = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, h0, h1, hpred, pred_logits, my_belief, opp_belief):\n",
        "        # beliefs: (num_bots,), turn into (1, num_bots*2)\n",
        "        id_feat = torch.cat([my_belief, opp_belief], dim=-1).unsqueeze(0)\n",
        "\n",
        "        Q = self.id_to_query(id_feat)\n",
        "        K = self.id_to_key(id_feat)\n",
        "        V = self.id_to_value(id_feat)\n",
        "\n",
        "        attn = (Q @ K.transpose(-2, -1)) / (Q.size(-1)**0.5)\n",
        "        W = torch.softmax(attn, dim=-1) @ V   # (1, d)\n",
        "\n",
        "        base = torch.cat([h0, h1, hpred, pred_logits], dim=-1)\n",
        "        base = self.feature_proj(base.unsqueeze(0))  # (1, d)\n",
        "\n",
        "        ctx = torch.tanh(base + W)   # (1, d)\n",
        "        v = self.out(ctx)            # (1, 1)\n",
        "\n",
        "        # return shape (1,)\n",
        "        return v.view(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43_gZT74K5Lb"
      },
      "outputs": [],
      "source": [
        "def collect_online_episodes(env, agent, opponent_bot_id, num_episodes=10):\n",
        "    episodes = []\n",
        "\n",
        "    opp = create_roshambo_bot_agent(1, num_actions, roshambo_bot_names, opponent_bot_id)\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        agent.restart()\n",
        "        opp.restart()\n",
        "\n",
        "        ts = env.reset()\n",
        "        episode = []\n",
        "\n",
        "        while not ts.last():\n",
        "\n",
        "            out_agent = agent.step(ts)\n",
        "            out_opp   = opp.step(ts)\n",
        "\n",
        "            action = out_agent.action\n",
        "            opp_action = out_opp.action\n",
        "            old_probs = np.array(out_agent.probs, dtype=np.float32)\n",
        "\n",
        "            ts_next = env.step([action, opp_action])\n",
        "            reward = ts_next.rewards[0]\n",
        "\n",
        "            # --- SKIP until saved_last is populated ---\n",
        "            if agent.saved_last is None:\n",
        "                ts = ts_next\n",
        "                continue\n",
        "\n",
        "            h0, h1, hpred, pred_logits, my_b, opp_b, last_probs = agent.saved_last\n",
        "\n",
        "            last_probs = np.array(last_probs, dtype=np.float32)\n",
        "\n",
        "            episode.append(\n",
        "                (h0, h1, hpred, pred_logits,\n",
        "                 my_b, opp_b,\n",
        "                 action, opp_action,\n",
        "                 last_probs, reward)\n",
        "            )\n",
        "\n",
        "            ts = ts_next\n",
        "\n",
        "        episodes.append(episode)\n",
        "\n",
        "    return episodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaDCZlBDK-TL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def online_ppo_update(\n",
        "        policy,\n",
        "        value_net,\n",
        "        optimizer,\n",
        "        episodes,\n",
        "        save_path=\"/content/drive/MyDrive/policy_value_ckpt.pt\",\n",
        "        gamma=0.99,\n",
        "        clip_eps=0.2,\n",
        "        aux_weight=0.05,\n",
        "        entropy_weight=0.005,\n",
        "        max_kl=0.05,\n",
        "        save_every=10,\n",
        "        iteration=0\n",
        "    ):\n",
        "\n",
        "    total_loss_out = 0.0\n",
        "\n",
        "    for ep in episodes:\n",
        "\n",
        "        # ---- compute discounted return ----\n",
        "        rewards = [step[-1] for step in ep]\n",
        "        G = []\n",
        "        R = 0\n",
        "        for r in reversed(rewards):\n",
        "            R = r + gamma * R\n",
        "            G.insert(0, R)\n",
        "        G = torch.tensor(G, dtype=torch.float32)\n",
        "\n",
        "        # ---- value network baseline ----\n",
        "        values = []\n",
        "        for step in ep:\n",
        "            h0, h1, hpred, pred_logits, my_b, opp_b, *_ = step\n",
        "            v = value_net(h0, h1, hpred, pred_logits, my_b, opp_b)\n",
        "\n",
        "            # ensure shape [1]\n",
        "            if v.dim() == 0:\n",
        "                  v = v.unsqueeze(0)\n",
        "\n",
        "            values.append(v)\n",
        "\n",
        "        values = torch.cat(values).squeeze()\n",
        "\n",
        "        # advantages\n",
        "        A = (G - values.detach())\n",
        "        A = (A - A.mean()) / (A.std() + 1e-8)\n",
        "        A = A / 2.0   # extra stabilization\n",
        "\n",
        "        # ---- PPO update ----\n",
        "        for (step, A_step, R_target) in zip(ep, A, G):\n",
        "\n",
        "            R_target = torch.clamp(R_target, -50, 50)\n",
        "\n",
        "\n",
        "            (h0, h1, hpred, pred_logits, my_b, opp_b,\n",
        "             action, opp_action, old_probs, reward) = step\n",
        "\n",
        "            logits = policy(h0, h1, hpred, pred_logits, my_b, opp_b)\n",
        "            logits = torch.clamp(logits, -10, 10)  # logit clamp\n",
        "\n",
        "            logprobs = F.log_softmax(logits, dim=-1)[0]\n",
        "            logp = logprobs[action]\n",
        "            old_logp = torch.log(torch.tensor(old_probs[action]) + 1e-12)\n",
        "\n",
        "            # PPO ratio\n",
        "            ratio = torch.exp(logp - old_logp)\n",
        "            surr1 = ratio * A_step\n",
        "            surr2 = torch.clamp(ratio, 1-clip_eps, 1+clip_eps) * A_step\n",
        "            ppo_loss = -torch.min(surr1, surr2)\n",
        "\n",
        "            # Value loss\n",
        "            value_pred = value_net(h0, h1, hpred, pred_logits, my_b, opp_b)\n",
        "            value_loss = 0.5 * (value_pred.squeeze() - R_target)**2\n",
        "            value_loss = 0.01 * value_loss\n",
        "\n",
        "\n",
        "            # Best/worst auxiliary loss\n",
        "            best_action = (opp_action + 1) % 3\n",
        "            worst_action = (best_action + 1) % 3\n",
        "\n",
        "            best_loss  = -logprobs[best_action]\n",
        "            worst_loss = +logprobs[worst_action]\n",
        "            aux_loss = best_loss + 0.25 * worst_loss\n",
        "\n",
        "            # Entropy bonus\n",
        "            entropy = -(logprobs * torch.exp(logprobs)).sum()\n",
        "            entropy_loss = -entropy_weight * entropy\n",
        "\n",
        "            # Full loss\n",
        "            loss = ppo_loss + value_loss + aux_weight * aux_loss + entropy_loss\n",
        "            total_loss_out += loss\n",
        "\n",
        "    # ---- apply gradients ----\n",
        "    optimizer.zero_grad()\n",
        "    total_loss_out.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
        "    torch.nn.utils.clip_grad_norm_(value_net.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    # ---- Save both networks ----\n",
        "    if iteration % save_every == 0:\n",
        "        checkpoint = {\n",
        "            \"policy\": policy.state_dict(),\n",
        "            \"value_net\": value_net.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "            \"iteration\": iteration\n",
        "        }\n",
        "        torch.save(checkpoint, save_path)\n",
        "        print(f\"ðŸ’¾ Saved policy+value checkpoint at iter {iteration}\")\n",
        "\n",
        "    return total_loss_out.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbCSVmz3WSMW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def choose_opponent(perf_estimates,\n",
        "                    temperature=1.0,\n",
        "                    floor=0.02,           # minimum probability for ANY bot\n",
        "                    mix_uniform=0.20):    # % of uniform randomness\n",
        "    \"\"\"\n",
        "    perf_estimates: array of running performance estimates vs each bot\n",
        "\n",
        "    Returns a bot index, avoiding collapse to a single opponent.\n",
        "    \"\"\"\n",
        "\n",
        "    perf = np.array(perf_estimates, dtype=np.float64)\n",
        "\n",
        "    # difficulty = hardness = lower perf = harder bots\n",
        "    difficulty = -perf\n",
        "\n",
        "    # temperature scaling (smooths out distribution)\n",
        "    scaled = difficulty / max(1e-6, temperature)\n",
        "\n",
        "    # safe exponentiation\n",
        "    scaled = np.clip(scaled, -50, 50)   # avoid overflow\n",
        "    weights = np.exp(scaled)\n",
        "\n",
        "    # normalize safely\n",
        "    probs = weights / (weights.sum() + 1e-12)\n",
        "\n",
        "    # apply probability floor so no bot hits 0\n",
        "    probs = np.maximum(probs, floor)\n",
        "    probs = probs / probs.sum()  # renormalize\n",
        "\n",
        "    # mix in some uniform randomness (encourages exploration)\n",
        "    if mix_uniform > 0:\n",
        "        uniform = np.ones_like(probs) / len(probs)\n",
        "        probs = (1 - mix_uniform) * probs + mix_uniform * uniform\n",
        "\n",
        "    # final renormalization\n",
        "    probs = probs / probs.sum()\n",
        "\n",
        "    # safe choice\n",
        "    return np.random.choice(len(probs), p=probs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vXilI0JWWPb"
      },
      "outputs": [],
      "source": [
        "perf = np.zeros(43)  # running avg return vs each bot\n",
        "counts = np.zeros(43) + 1e-6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79UTEOeWBrrm"
      },
      "outputs": [],
      "source": [
        "agent = MyAgent(3, model=model, weights=\"pred_weights.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLHNDpYhYCBc"
      },
      "outputs": [],
      "source": [
        "# ---- initialize policy + value ----\n",
        "policy = agent.policy\n",
        "value_net = IdentityConditionedValue(d_model=64, num_bots=43)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q8dXbO2ZCTc"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(\n",
        "    list(policy.parameters()) + list(value_net.parameters()),\n",
        "    lr=1e-4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ2_JeaIeCVI"
      },
      "outputs": [],
      "source": [
        "ckpt = torch.load(\"policy_value_ckpt.pt\")\n",
        "policy.load_state_dict(ckpt[\"policy\"])\n",
        "value_net.load_state_dict(ckpt[\"value_net\"])\n",
        "optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "start_iteration = ckpt[\"iteration\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "-aa50EH4K_Ia",
        "outputId": "0b5ec8c6-b818-443c-887b-1e46b0285954"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¾ Saved policy+value checkpoint at iter 0\n",
            "[RL] iter=0000 bot=09 driftbot           loss=1390.335 return=7.600\n",
            "[RL] iter=0001 bot=27 piedra             loss=3604.765 return=-104.800\n",
            "[RL] iter=0002 bot=27 piedra             loss=3742.040 return=-112.800\n",
            "[RL] iter=0003 bot=27 piedra             loss=1985.492 return=-59.600\n",
            "[RL] iter=0004 bot=27 piedra             loss=2403.616 return=-70.600\n",
            "[RL] iter=0005 bot=40 switchbot          loss=2206.570 return=61.400\n",
            "[RL] iter=0006 bot=40 switchbot          loss=1507.684 return=55.200\n",
            "[RL] iter=0007 bot=27 piedra             loss=2768.951 return=-93.200\n",
            "[RL] iter=0008 bot=32 rockbot            loss=8381.351 return=192.000\n",
            "[RL] iter=0009 bot=04 antirotnbot        loss=3245.907 return=-108.000\n",
            "ðŸ’¾ Saved policy+value checkpoint at iter 10\n",
            "[RL] iter=0010 bot=07 copybot            loss=47294.137 return=701.800\n",
            "[RL] iter=0011 bot=04 antirotnbot        loss=3285.820 return=-99.400\n",
            "[RL] iter=0012 bot=38 sweetrock          loss=2818.983 return=-89.000\n",
            "[RL] iter=0013 bot=14 greenberg          loss=4188.659 return=-111.000\n",
            "[RL] iter=0014 bot=38 sweetrock          loss=2787.465 return=-71.000\n",
            "[RL] iter=0015 bot=40 switchbot          loss=1766.809 return=50.000\n",
            "[RL] iter=0016 bot=10 flatbot3           loss=950.465 return=11.200\n",
            "[RL] iter=0017 bot=27 piedra             loss=2127.979 return=-71.000\n",
            "[RL] iter=0018 bot=13 granite            loss=2582.100 return=-81.000\n",
            "[RL] iter=0019 bot=35 shofar             loss=1458.994 return=-50.200\n",
            "ðŸ’¾ Saved policy+value checkpoint at iter 20\n",
            "[RL] iter=0020 bot=16 inocencio          loss=2680.116 return=-86.000\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2021501075.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# collect several episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_online_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbot_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# compute average return achieved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1977747022.py\u001b[0m in \u001b[0;36mcollect_online_episodes\u001b[0;34m(env, agent, opponent_bot_id, num_episodes)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mout_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mout_opp\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mopp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3629255141.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, time_step, is_evaluation)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# --- decode previous move from state ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         game, state = pyspiel.deserialize_game_and_state(\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"serialized_state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# curriculum performance stats\n",
        "perf = np.zeros(43, dtype=np.float32)\n",
        "counts = np.zeros(43, dtype=np.float32) + 1e-6\n",
        "\n",
        "train_loss_hist = []\n",
        "avg_return_hist = []\n",
        "\n",
        "for it in range(2000):\n",
        "\n",
        "    # choose opponent using curriculum\n",
        "    bot_id = choose_opponent(perf, temperature=1.0)\n",
        "\n",
        "    # collect several episodes\n",
        "    episodes = collect_online_episodes(env, agent, bot_id, num_episodes=5)\n",
        "\n",
        "    # compute average return achieved\n",
        "    ep_returns = [sum(step[-1] for step in ep) for ep in episodes]\n",
        "    avg_return = float(np.mean(ep_returns))\n",
        "\n",
        "    # PPO update\n",
        "    loss = online_ppo_update(\n",
        "        policy,\n",
        "        value_net,\n",
        "        optimizer,\n",
        "        episodes,\n",
        "        iteration=it\n",
        "    )\n",
        "\n",
        "    # update curriculum stats\n",
        "    counts[bot_id] += 1\n",
        "    perf[bot_id] += (avg_return - perf[bot_id]) / counts[bot_id]\n",
        "\n",
        "    # track learning curves\n",
        "    train_loss_hist.append(loss)\n",
        "    avg_return_hist.append(avg_return)\n",
        "\n",
        "    print(f\"[RL] iter={it:04d} \"\n",
        "          f\"bot={bot_id:02d} {roshambo_bot_names[bot_id]:18s} \"\n",
        "          f\"loss={loss:.3f} return={avg_return:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV45suDDLGm-"
      },
      "source": [
        "# Offline RL with PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ka41o4CLBe_"
      },
      "outputs": [],
      "source": [
        "def build_offline_rl_dataset(predictor, X, Y, id0, id1, num_bots=43):\n",
        "    offline_data = []\n",
        "\n",
        "    # create belief trackers for offline pass\n",
        "    my_belief = IdentityBelief(num_bots)\n",
        "    opp_belief = IdentityBelief(num_bots)\n",
        "\n",
        "    R = torch.tensor([\n",
        "        [ 0, -1,  1],   # R vs R,P,S\n",
        "        [ 1,  0, -1],\n",
        "        [-1,  1,  0],\n",
        "    ], dtype=torch.float32)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(X)):\n",
        "            window = X[i].unsqueeze(0)\n",
        "            pred_logits, id0_logits, id1_logits, hpred, h0, h1 = predictor(window)\n",
        "\n",
        "            # update identity EMA\n",
        "            my_belief.update(id0_logits[0])\n",
        "            opp_belief.update(id1_logits[0])\n",
        "\n",
        "            # decode the true action pair from label Y\n",
        "            joint = Y[i].item()\n",
        "            a0 = joint // 3\n",
        "            a1 = joint % 3\n",
        "\n",
        "            reward = R[a0, a1].item()\n",
        "\n",
        "            offline_data.append((\n",
        "                h0[0], h1[0], hpred[0],\n",
        "                pred_logits[0],\n",
        "                my_belief.belief.clone(),\n",
        "                opp_belief.belief.clone(),\n",
        "                a0,                        # true action\n",
        "                (a1+1) % 3,                 # best action, reward = 1\n",
        "                reward                     # true reward\n",
        "            ))\n",
        "\n",
        "    return offline_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NA4rJA4WK52m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def offline_ppo_update(\n",
        "        policy,\n",
        "        data,\n",
        "        optimizer,\n",
        "        epochs=3,\n",
        "        aux_weight=0.05,\n",
        "        entropy_weight=0.01,\n",
        "        save_path=\"/content/drive/MyDrive/policy_weights_offline.pt\",\n",
        "        save_every=1,\n",
        "        iteration=0\n",
        "    ):\n",
        "    \"\"\"\n",
        "    data: list of tuples\n",
        "      (h0, h1, hpred, pred_logits, my_b, opp_b,\n",
        "       action, best_action, reward)\n",
        "\n",
        "    No old_probs stored â†’ treat as advantage regression.\n",
        "    \"\"\"\n",
        "\n",
        "    total_loss_out = 0.0\n",
        "\n",
        "    for ep in range(epochs):\n",
        "\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Compute reward normalization (advantage normalization)\n",
        "        rewards = torch.tensor([d[-1] for d in data], dtype=torch.float32)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
        "\n",
        "        for i, (h0, h1, hpred, pred_logits,\n",
        "                my_b, opp_b,\n",
        "                action, best_action, reward) in enumerate(data):\n",
        "\n",
        "            logits = policy(h0, h1, hpred, pred_logits, my_b, opp_b)\n",
        "            logprobs = F.log_softmax(logits, -1)[0]\n",
        "\n",
        "            # --- Reinforcement (advantage-weighted regression) ---\n",
        "            A = rewards[i]\n",
        "            reinforce_loss = -logprobs[action] * A\n",
        "\n",
        "            # --- Auxiliary best-move loss ---\n",
        "            # best action comes from dataset (precomputed)\n",
        "            best_loss = -logprobs[best_action]\n",
        "\n",
        "            # correct worst action (loses to best)\n",
        "            worst_action = (best_action + 1) % 3\n",
        "            worst_loss = +logprobs[worst_action]\n",
        "\n",
        "            aux_loss = best_loss + 0.25 * worst_loss\n",
        "\n",
        "            # --- Entropy bonus ---\n",
        "            entropy = -(logprobs * torch.exp(logprobs)).sum()\n",
        "            entropy_loss = -entropy_weight * entropy\n",
        "\n",
        "            # --- Combine ---\n",
        "            loss = reinforce_loss + aux_weight * aux_loss + entropy_loss\n",
        "            total_loss += loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss_out += float(total_loss.item())\n",
        "\n",
        "        # Save weights\n",
        "        if iteration % save_every == 0 and ep == epochs - 1:\n",
        "            torch.save(policy.state_dict(), save_path)\n",
        "            print(f\"ðŸ’¾ Saved offline policy to {save_path} at iter {iteration}\")\n",
        "\n",
        "    return total_loss_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "JWQNCTE7oPIx",
        "outputId": "68df9430-821c-4f6b-8207-9b3bc294029f"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1944634886.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moffline_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_offline_rl_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXm_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYm_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid0m_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid1m_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2565098980.py\u001b[0m in \u001b[0;36mbuild_offline_rl_dataset\u001b[0;34m(predictor, X, Y, id0, id1, num_bots)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mpred_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid0_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid1_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# update identity EMA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-114284776.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                                \u001b[0;31m# (B, L, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mhpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m             output = mod(\n\u001b[0m\u001b[1;32m    525\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m                 \u001b[0mwhy_not_sparsity_fast_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"some Tensor argument has_torch_function\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             elif not all(\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_supported_device_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             ):\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    884\u001b[0m                 \u001b[0mwhy_not_sparsity_fast_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"some Tensor argument has_torch_function\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             elif not all(\n\u001b[0;32m--> 886\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_supported_device_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m             ):\n\u001b[1;32m    888\u001b[0m                 why_not_sparsity_fast_path = (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "offline_data = build_offline_rl_dataset(agent.predictor, Xm_dev, Ym_dev, id0m_dev, id1m_dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "468sewvlLLOX"
      },
      "outputs": [],
      "source": [
        "policy = agent.policy\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=3e-4)\n",
        "\n",
        "for it in range(50):\n",
        "    loss = offline_ppo_update(policy, offline_data, optimizer)\n",
        "    print(f\"[offline RL] iter {it}, loss={loss:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYHeqXy6PAG4"
      },
      "source": [
        "# Template Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpbyfKe2SV1s"
      },
      "outputs": [],
      "source": [
        "# Template to get you started. Pick one of these to implement as your starting\n",
        "# point.\n",
        "\n",
        "# Template : Basic RL agent.\n",
        "#\n",
        "#\n",
        "class MyAgent(rl_agent.AbstractAgent):\n",
        "  \"\"\"Agent class that learns to play RRPS.\n",
        "\n",
        "  You fill this in to create your RRPS agent.\n",
        "\n",
        "  See the superclass for more info: https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/rl_agent.py\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_actions, name=\"bot_agent\"):\n",
        "    assert num_actions > 0\n",
        "    self._num_actions = num_actions  # 3\n",
        "\n",
        "  def step(self, time_step, is_evaluation=False):\n",
        "    # If it is the end of the episode, don't select an action.\n",
        "    if time_step.last():\n",
        "      return\n",
        "    # Note: If the environment was created with include_full_state=True, then\n",
        "    # game and state can be obtained as follows:\n",
        "\n",
        "    game, state = pyspiel.deserialize_game_and_state(time_step.observations[\"serialized_state\"])\n",
        "\n",
        "    # A useful piece of information is the history (previous actions taken by agents).\n",
        "    # You can access this by state.history()\n",
        "\n",
        "    # Do something here that selects an action and computes a policy\n",
        "    # distribution in probs.\n",
        "    if len(state.history())==0:\n",
        "      action = 0\n",
        "    else:\n",
        "      action = state.history()[-1]\n",
        "    probs = np.ones(self._num_actions) / self._num_actions\n",
        "    return rl_agent.StepOutput(action=action, probs=probs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63PMnhw95GQ3"
      },
      "source": [
        "#A short example to show how to evaluate your agent againt an RRPS bot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wzodWjXPElN"
      },
      "outputs": [],
      "source": [
        "def evaluate_across_population(env, agent, num_eps=5):\n",
        "    scores = []\n",
        "    for bot_id in range(43):\n",
        "        _, avg_ret, _ = collect_online_episodes(env, agent, bot_id, num_eps)\n",
        "        scores.append(avg_ret)\n",
        "    return scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "gtjPe7O4as2V",
        "outputId": "f77bd2de-563b-4e43-c849-13ea87f97720"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for PredictorModel:\n\tMissing key(s) in state_dict: \"id_head.weight\", \"id_head.bias\". ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4105896947.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Just trying an example out.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmy_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"trans_agent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(my_agent._num_actions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1045107044.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_actions, name, weights)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# load your trained predictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictorModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2629\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2630\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2631\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PredictorModel:\n\tMissing key(s) in state_dict: \"id_head.weight\", \"id_head.bias\". "
          ]
        }
      ],
      "source": [
        "# Just trying an example out.\n",
        "\n",
        "my_agent = MyAgent(3, name=\"trans_agent\")\n",
        "# print(my_agent._num_actions)\n",
        "\n",
        "\n",
        "p1_pop_id = 17   # adddriftbot2\n",
        "agents = [\n",
        "    my_agent,\n",
        "    create_roshambo_bot_agent(1, num_actions, roshambo_bot_names, p1_pop_id)\n",
        "]\n",
        "\n",
        "\n",
        "print(\"Starting eval run.\")\n",
        "avg_eval_returns = eval_agents(env, agents, num_players, 4, verbose=True)\n",
        "\n",
        "print(\"Avg return \", avg_eval_returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL5wfH2CpPkX"
      },
      "outputs": [],
      "source": [
        "myagent = MyAgent(3, name=\"trans\", weights = \"predictor_weights.pt\")\n",
        "myagent2 = MyAgent(3, name=\"trans2\", weights = \"predictor_weights2.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnN8RK2JpYBA",
        "outputId": "11813139-86aa-4735-d4ee-def4a886f9ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished episode 0, avg returns: [ 52. -52.]\n",
            "Finished episode 1, avg returns: [ 22.5 -22.5]\n",
            "Finished episode 2, avg returns: [ 20.66666667 -20.66666667]\n",
            "Finished episode 3, avg returns: [ 6.75 -6.75]\n",
            "Finished episode 4, avg returns: [ 7. -7.]\n",
            "Finished episode 5, avg returns: [-2.33333333  2.33333333]\n",
            "Finished episode 6, avg returns: [-1.  1.]\n",
            "Finished episode 7, avg returns: [-3.125  3.125]\n",
            "Avg return [-3.125  3.125]\n"
          ]
        }
      ],
      "source": [
        "avg_eval_self = eval_agents(env, [my_agent,myagent2], num_players, 8, verbose=True)\n",
        "print(\"Avg return\", avg_eval_self)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4594qUzYfGb",
        "outputId": "c670e3ff-eebc-4adb-a278-957c2c352e94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 25: phasenbott ===\n",
            "Finished episode 0, avg returns: [ 30. -30.]\n",
            "Finished episode 1, avg returns: [ 7. -7.]\n",
            "Finished episode 2, avg returns: [-6.66666667  6.66666667]\n",
            "Finished episode 3, avg returns: [-3.  3.]\n",
            "Finished episode 4, avg returns: [-0.4  0.4]\n",
            "Finished episode 5, avg returns: [ 2.16666667 -2.16666667]\n",
            "Finished episode 6, avg returns: [ 4.57142857 -4.57142857]\n",
            "Finished episode 7, avg returns: [ 6.625 -6.625]\n",
            "Finished episode 8, avg returns: [ 3.22222222 -3.22222222]\n",
            "Finished episode 9, avg returns: [ 0.4 -0.4]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 30: randbot ===\n",
            "Finished episode 0, avg returns: [-4.  4.]\n",
            "Finished episode 1, avg returns: [-2.5  2.5]\n",
            "Finished episode 2, avg returns: [-4.66666667  4.66666667]\n",
            "Finished episode 3, avg returns: [-0.25  0.25]\n",
            "Finished episode 4, avg returns: [ 6.2 -6.2]\n",
            "Finished episode 5, avg returns: [-0.16666667  0.16666667]\n",
            "Finished episode 6, avg returns: [ 1.71428571 -1.71428571]\n",
            "Finished episode 7, avg returns: [ 2.875 -2.875]\n",
            "Finished episode 8, avg returns: [ 3.77777778 -3.77777778]\n",
            "Finished episode 9, avg returns: [ 4.6 -4.6]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 26: pibot ===\n",
            "Finished episode 0, avg returns: [ 9. -9.]\n",
            "Finished episode 1, avg returns: [ 4. -4.]\n",
            "Finished episode 2, avg returns: [-2.66666667  2.66666667]\n",
            "Finished episode 3, avg returns: [-1.5  1.5]\n",
            "Finished episode 4, avg returns: [-3.4  3.4]\n",
            "Finished episode 5, avg returns: [-5.5  5.5]\n",
            "Finished episode 6, avg returns: [-3.14285714  3.14285714]\n",
            "Finished episode 7, avg returns: [0. 0.]\n",
            "Finished episode 8, avg returns: [ 3.88888889 -3.88888889]\n",
            "Finished episode 9, avg returns: [ 3.3 -3.3]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 20: markovbails ===\n",
            "Finished episode 0, avg returns: [-12.  12.]\n",
            "Finished episode 1, avg returns: [-3.5  3.5]\n",
            "Finished episode 2, avg returns: [0. 0.]\n",
            "Finished episode 3, avg returns: [ 2.25 -2.25]\n",
            "Finished episode 4, avg returns: [ 2.4 -2.4]\n",
            "Finished episode 5, avg returns: [ 1. -1.]\n",
            "Finished episode 6, avg returns: [-7.28571429  7.28571429]\n",
            "Finished episode 7, avg returns: [-7.125  7.125]\n",
            "Finished episode 8, avg returns: [-1.44444444  1.44444444]\n",
            "Finished episode 9, avg returns: [-2.  2.]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 8: debruijn81 ===\n",
            "Finished episode 0, avg returns: [-1.  1.]\n",
            "Finished episode 1, avg returns: [ 10.5 -10.5]\n",
            "Finished episode 2, avg returns: [ 13.66666667 -13.66666667]\n",
            "Finished episode 3, avg returns: [ 14.5 -14.5]\n",
            "Finished episode 4, avg returns: [ 7.8 -7.8]\n",
            "Finished episode 5, avg returns: [ 12.66666667 -12.66666667]\n",
            "Finished episode 6, avg returns: [ 6.85714286 -6.85714286]\n",
            "Finished episode 7, avg returns: [ 6.75 -6.75]\n",
            "Finished episode 8, avg returns: [-0.22222222  0.22222222]\n",
            "Finished episode 9, avg returns: [-0.9  0.9]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 19: markov5 ===\n",
            "Finished episode 0, avg returns: [ 10. -10.]\n",
            "Finished episode 1, avg returns: [ 4. -4.]\n",
            "Finished episode 2, avg returns: [ 2.33333333 -2.33333333]\n",
            "Finished episode 3, avg returns: [ 7.75 -7.75]\n",
            "Finished episode 4, avg returns: [ 8.6 -8.6]\n",
            "Finished episode 5, avg returns: [ 13. -13.]\n",
            "Finished episode 6, avg returns: [ 17. -17.]\n",
            "Finished episode 7, avg returns: [ 21.625 -21.625]\n",
            "Finished episode 8, avg returns: [ 18.66666667 -18.66666667]\n",
            "Finished episode 9, avg returns: [ 15. -15.]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 14: greenberg ===\n",
            "Finished episode 0, avg returns: [ 2. -2.]\n",
            "Finished episode 1, avg returns: [-7.  7.]\n",
            "Finished episode 2, avg returns: [ 4.66666667 -4.66666667]\n",
            "Finished episode 3, avg returns: [ 7.25 -7.25]\n",
            "Finished episode 4, avg returns: [ 7.8 -7.8]\n",
            "Finished episode 5, avg returns: [ 6.5 -6.5]\n",
            "Finished episode 6, avg returns: [ 8. -8.]\n",
            "Finished episode 7, avg returns: [ 4.25 -4.25]\n",
            "Finished episode 8, avg returns: [ 1. -1.]\n",
            "Finished episode 9, avg returns: [ 3.1 -3.1]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 41: textbot ===\n",
            "Finished episode 0, avg returns: [ 26. -26.]\n",
            "Finished episode 1, avg returns: [ 24. -24.]\n",
            "Finished episode 2, avg returns: [ 34. -34.]\n",
            "Finished episode 3, avg returns: [ 22.75 -22.75]\n",
            "Finished episode 4, avg returns: [ 24.8 -24.8]\n",
            "Finished episode 5, avg returns: [ 22. -22.]\n",
            "Finished episode 6, avg returns: [ 17.71428571 -17.71428571]\n",
            "Finished episode 7, avg returns: [ 19.75 -19.75]\n",
            "Finished episode 8, avg returns: [ 15.77777778 -15.77777778]\n",
            "Finished episode 9, avg returns: [ 16. -16.]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 0: actr_lag2_decay ===\n",
            "Finished episode 0, avg returns: [-33.  33.]\n",
            "Finished episode 1, avg returns: [-23.5  23.5]\n",
            "Finished episode 2, avg returns: [-11.66666667  11.66666667]\n",
            "Finished episode 3, avg returns: [-10.5  10.5]\n",
            "Finished episode 4, avg returns: [-9.4  9.4]\n",
            "Finished episode 5, avg returns: [-6.66666667  6.66666667]\n",
            "Finished episode 6, avg returns: [-9.  9.]\n",
            "Finished episode 7, avg returns: [-9.625  9.625]\n",
            "Finished episode 8, avg returns: [-6.  6.]\n",
            "Finished episode 9, avg returns: [-4.  4.]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 17: iocainebot ===\n",
            "Finished episode 0, avg returns: [ 21. -21.]\n",
            "Finished episode 1, avg returns: [ 27.5 -27.5]\n",
            "Finished episode 2, avg returns: [ 20.66666667 -20.66666667]\n",
            "Finished episode 3, avg returns: [ 14.75 -14.75]\n",
            "Finished episode 4, avg returns: [ 8.6 -8.6]\n",
            "Finished episode 5, avg returns: [ 9.16666667 -9.16666667]\n",
            "Finished episode 6, avg returns: [ 10. -10.]\n",
            "Finished episode 7, avg returns: [ 8.75 -8.75]\n",
            "Finished episode 8, avg returns: [ 8.88888889 -8.88888889]\n",
            "Finished episode 9, avg returns: [ 7.9 -7.9]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 34: russrocker4 ===\n",
            "Finished episode 0, avg returns: [ 43. -43.]\n",
            "Finished episode 1, avg returns: [ 16. -16.]\n",
            "Finished episode 2, avg returns: [ 14.33333333 -14.33333333]\n",
            "Finished episode 3, avg returns: [ 12.75 -12.75]\n",
            "Finished episode 4, avg returns: [ 17.2 -17.2]\n",
            "Finished episode 5, avg returns: [ 11.66666667 -11.66666667]\n",
            "Finished episode 6, avg returns: [ 11.42857143 -11.42857143]\n",
            "Finished episode 7, avg returns: [ 10.25 -10.25]\n",
            "Finished episode 8, avg returns: [ 7.33333333 -7.33333333]\n",
            "Finished episode 9, avg returns: [ 9. -9.]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 35: shofar ===\n",
            "Finished episode 0, avg returns: [ 41. -41.]\n",
            "Finished episode 1, avg returns: [ 11. -11.]\n",
            "Finished episode 2, avg returns: [ 21. -21.]\n",
            "Finished episode 3, avg returns: [ 14. -14.]\n",
            "Finished episode 4, avg returns: [ 20.4 -20.4]\n",
            "Finished episode 5, avg returns: [ 16.16666667 -16.16666667]\n",
            "Finished episode 6, avg returns: [ 7.85714286 -7.85714286]\n",
            "Finished episode 7, avg returns: [ 5.25 -5.25]\n",
            "Finished episode 8, avg returns: [ 6.11111111 -6.11111111]\n",
            "Finished episode 9, avg returns: [ 5.5 -5.5]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 5: biopic ===\n",
            "Finished episode 0, avg returns: [ 8. -8.]\n",
            "Finished episode 1, avg returns: [ 2.5 -2.5]\n",
            "Finished episode 2, avg returns: [ 8.66666667 -8.66666667]\n",
            "Finished episode 3, avg returns: [ 11. -11.]\n",
            "Finished episode 4, avg returns: [ 15.2 -15.2]\n",
            "Finished episode 5, avg returns: [ 17.66666667 -17.66666667]\n",
            "Finished episode 6, avg returns: [ 18.14285714 -18.14285714]\n",
            "Finished episode 7, avg returns: [ 18.5 -18.5]\n",
            "Finished episode 8, avg returns: [ 17.44444444 -17.44444444]\n",
            "Finished episode 9, avg returns: [ 16.9 -16.9]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 6: boom ===\n",
            "Finished episode 0, avg returns: [ 2. -2.]\n",
            "Finished episode 1, avg returns: [ 2.5 -2.5]\n",
            "Finished episode 2, avg returns: [ 10.33333333 -10.33333333]\n",
            "Finished episode 3, avg returns: [ 13.5 -13.5]\n",
            "Finished episode 4, avg returns: [ 15.8 -15.8]\n",
            "Finished episode 5, avg returns: [ 17.5 -17.5]\n",
            "Finished episode 6, avg returns: [ 19.85714286 -19.85714286]\n",
            "Finished episode 7, avg returns: [ 17.5 -17.5]\n",
            "Finished episode 8, avg returns: [ 17. -17.]\n",
            "Finished episode 9, avg returns: [ 13.6 -13.6]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 22: mod1bot ===\n",
            "Finished episode 0, avg returns: [ 41. -41.]\n",
            "Finished episode 1, avg returns: [ 13.5 -13.5]\n",
            "Finished episode 2, avg returns: [ 27.33333333 -27.33333333]\n",
            "Finished episode 3, avg returns: [ 16.25 -16.25]\n",
            "Finished episode 4, avg returns: [ 25. -25.]\n",
            "Finished episode 5, avg returns: [ 14.33333333 -14.33333333]\n",
            "Finished episode 6, avg returns: [ 22. -22.]\n",
            "Finished episode 7, avg returns: [ 21. -21.]\n",
            "Finished episode 8, avg returns: [ 19. -19.]\n",
            "Finished episode 9, avg returns: [ 24.4 -24.4]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 10: flatbot3 ===\n",
            "Finished episode 0, avg returns: [ 44. -44.]\n",
            "Finished episode 1, avg returns: [ 53. -53.]\n",
            "Finished episode 2, avg returns: [ 52. -52.]\n",
            "Finished episode 3, avg returns: [ 55.25 -55.25]\n",
            "Finished episode 4, avg returns: [ 46.8 -46.8]\n",
            "Finished episode 5, avg returns: [ 47.83333333 -47.83333333]\n",
            "Finished episode 6, avg returns: [ 47.42857143 -47.42857143]\n",
            "Finished episode 7, avg returns: [ 48.125 -48.125]\n",
            "Finished episode 8, avg returns: [ 49.88888889 -49.88888889]\n",
            "Finished episode 9, avg returns: [ 50.9 -50.9]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 1: adddriftbot2 ===\n",
            "Finished episode 0, avg returns: [ 32. -32.]\n",
            "Finished episode 1, avg returns: [ 48. -48.]\n",
            "Finished episode 2, avg returns: [ 52.33333333 -52.33333333]\n",
            "Finished episode 3, avg returns: [ 55.75 -55.75]\n",
            "Finished episode 4, avg returns: [ 50.2 -50.2]\n",
            "Finished episode 5, avg returns: [ 49.5 -49.5]\n",
            "Finished episode 6, avg returns: [ 44. -44.]\n",
            "Finished episode 7, avg returns: [ 45.25 -45.25]\n",
            "Finished episode 8, avg returns: [ 47.77777778 -47.77777778]\n",
            "Finished episode 9, avg returns: [ 48.4 -48.4]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 31: robertot ===\n",
            "Finished episode 0, avg returns: [ 61. -61.]\n",
            "Finished episode 1, avg returns: [ 29.5 -29.5]\n",
            "Finished episode 2, avg returns: [ 16. -16.]\n",
            "Finished episode 3, avg returns: [ 23.75 -23.75]\n",
            "Finished episode 4, avg returns: [ 18.8 -18.8]\n",
            "Finished episode 5, avg returns: [ 25.16666667 -25.16666667]\n",
            "Finished episode 6, avg returns: [ 27.28571429 -27.28571429]\n",
            "Finished episode 7, avg returns: [ 31.25 -31.25]\n",
            "Finished episode 8, avg returns: [ 29.22222222 -29.22222222]\n",
            "Finished episode 9, avg returns: [ 29.8 -29.8]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 39: switchalot ===\n",
            "Finished episode 0, avg returns: [ 72. -72.]\n",
            "Finished episode 1, avg returns: [ 61.5 -61.5]\n",
            "Finished episode 2, avg returns: [ 72. -72.]\n",
            "Finished episode 3, avg returns: [ 72.5 -72.5]\n",
            "Finished episode 4, avg returns: [ 66.6 -66.6]\n",
            "Finished episode 5, avg returns: [ 60.16666667 -60.16666667]\n",
            "Finished episode 6, avg returns: [ 53.71428571 -53.71428571]\n",
            "Finished episode 7, avg returns: [ 48.25 -48.25]\n",
            "Finished episode 8, avg returns: [ 45.66666667 -45.66666667]\n",
            "Finished episode 9, avg returns: [ 47. -47.]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 16: inocencio ===\n",
            "Finished episode 0, avg returns: [ 95. -95.]\n",
            "Finished episode 1, avg returns: [ 61. -61.]\n",
            "Finished episode 2, avg returns: [ 58.33333333 -58.33333333]\n",
            "Finished episode 3, avg returns: [ 49.5 -49.5]\n",
            "Finished episode 4, avg returns: [ 53.2 -53.2]\n",
            "Finished episode 5, avg returns: [ 56. -56.]\n",
            "Finished episode 6, avg returns: [ 57.14285714 -57.14285714]\n",
            "Finished episode 7, avg returns: [ 58.5 -58.5]\n",
            "Finished episode 8, avg returns: [ 58.33333333 -58.33333333]\n",
            "Finished episode 9, avg returns: [ 58.4 -58.4]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 9: driftbot ===\n",
            "Finished episode 0, avg returns: [ 46. -46.]\n",
            "Finished episode 1, avg returns: [ 85.5 -85.5]\n",
            "Finished episode 2, avg returns: [ 73.33333333 -73.33333333]\n",
            "Finished episode 3, avg returns: [ 65.25 -65.25]\n",
            "Finished episode 4, avg returns: [ 62.8 -62.8]\n",
            "Finished episode 5, avg returns: [ 57.66666667 -57.66666667]\n",
            "Finished episode 6, avg returns: [ 64.14285714 -64.14285714]\n",
            "Finished episode 7, avg returns: [ 61.625 -61.625]\n",
            "Finished episode 8, avg returns: [ 62.33333333 -62.33333333]\n",
            "Finished episode 9, avg returns: [ 63.7 -63.7]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 27: piedra ===\n",
            "Finished episode 0, avg returns: [ 18. -18.]\n",
            "Finished episode 1, avg returns: [ 42.5 -42.5]\n",
            "Finished episode 2, avg returns: [ 41. -41.]\n",
            "Finished episode 3, avg returns: [ 38.25 -38.25]\n",
            "Finished episode 4, avg returns: [ 48.8 -48.8]\n",
            "Finished episode 5, avg returns: [ 52. -52.]\n",
            "Finished episode 6, avg returns: [ 49.28571429 -49.28571429]\n",
            "Finished episode 7, avg returns: [ 45.625 -45.625]\n",
            "Finished episode 8, avg returns: [ 42.66666667 -42.66666667]\n",
            "Finished episode 9, avg returns: [ 42.5 -42.5]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 37: sunNervebot ===\n",
            "Finished episode 0, avg returns: [ 52. -52.]\n",
            "Finished episode 1, avg returns: [ 21.5 -21.5]\n",
            "Finished episode 2, avg returns: [ 22.66666667 -22.66666667]\n",
            "Finished episode 3, avg returns: [ 19.75 -19.75]\n",
            "Finished episode 4, avg returns: [ 16.2 -16.2]\n",
            "Finished episode 5, avg returns: [ 18.33333333 -18.33333333]\n",
            "Finished episode 6, avg returns: [ 15.28571429 -15.28571429]\n",
            "Finished episode 7, avg returns: [ 16.75 -16.75]\n",
            "Finished episode 8, avg returns: [ 16.33333333 -16.33333333]\n",
            "Finished episode 9, avg returns: [ 19.9 -19.9]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 15: halbot ===\n",
            "Finished episode 0, avg returns: [ 76. -76.]\n",
            "Finished episode 1, avg returns: [ 88. -88.]\n",
            "Finished episode 2, avg returns: [ 78.66666667 -78.66666667]\n",
            "Finished episode 3, avg returns: [ 79. -79.]\n",
            "Finished episode 4, avg returns: [ 81. -81.]\n",
            "Finished episode 5, avg returns: [ 74.33333333 -74.33333333]\n",
            "Finished episode 6, avg returns: [ 73.28571429 -73.28571429]\n",
            "Finished episode 7, avg returns: [ 71.125 -71.125]\n",
            "Finished episode 8, avg returns: [ 67.44444444 -67.44444444]\n",
            "Finished episode 9, avg returns: [ 65.1 -65.1]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 38: sweetrock ===\n",
            "Finished episode 0, avg returns: [ 43. -43.]\n",
            "Finished episode 1, avg returns: [ 51.5 -51.5]\n",
            "Finished episode 2, avg returns: [ 41. -41.]\n",
            "Finished episode 3, avg returns: [ 43.75 -43.75]\n",
            "Finished episode 4, avg returns: [ 44. -44.]\n",
            "Finished episode 5, avg returns: [ 42.16666667 -42.16666667]\n",
            "Finished episode 6, avg returns: [ 43.71428571 -43.71428571]\n",
            "Finished episode 7, avg returns: [ 41.75 -41.75]\n",
            "Finished episode 8, avg returns: [ 40.55555556 -40.55555556]\n",
            "Finished episode 9, avg returns: [ 39.3 -39.3]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 11: foxtrotbot ===\n",
            "Finished episode 0, avg returns: [ 18. -18.]\n",
            "Finished episode 1, avg returns: [ 48. -48.]\n",
            "Finished episode 2, avg returns: [ 57.66666667 -57.66666667]\n",
            "Finished episode 3, avg returns: [ 48.75 -48.75]\n",
            "Finished episode 4, avg returns: [ 47.2 -47.2]\n",
            "Finished episode 5, avg returns: [ 49.5 -49.5]\n",
            "Finished episode 6, avg returns: [ 43.57142857 -43.57142857]\n",
            "Finished episode 7, avg returns: [ 45.25 -45.25]\n",
            "Finished episode 8, avg returns: [ 46. -46.]\n",
            "Finished episode 9, avg returns: [ 48.5 -48.5]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 29: r226bot ===\n",
            "Finished episode 0, avg returns: [ 98. -98.]\n",
            "Finished episode 1, avg returns: [ 73. -73.]\n",
            "Finished episode 2, avg returns: [ 69.33333333 -69.33333333]\n",
            "Finished episode 3, avg returns: [ 65.5 -65.5]\n",
            "Finished episode 4, avg returns: [ 62.8 -62.8]\n",
            "Finished episode 5, avg returns: [ 63. -63.]\n",
            "Finished episode 6, avg returns: [ 61.57142857 -61.57142857]\n",
            "Finished episode 7, avg returns: [ 63.5 -63.5]\n",
            "Finished episode 8, avg returns: [ 64. -64.]\n",
            "Finished episode 9, avg returns: [ 62.4 -62.4]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 13: granite ===\n",
            "Finished episode 0, avg returns: [ 47. -47.]\n",
            "Finished episode 1, avg returns: [ 74. -74.]\n",
            "Finished episode 2, avg returns: [ 65. -65.]\n",
            "Finished episode 3, avg returns: [ 57.5 -57.5]\n",
            "Finished episode 4, avg returns: [ 58.6 -58.6]\n",
            "Finished episode 5, avg returns: [ 64.16666667 -64.16666667]\n",
            "Finished episode 6, avg returns: [ 60.71428571 -60.71428571]\n",
            "Finished episode 7, avg returns: [ 65.75 -65.75]\n",
            "Finished episode 8, avg returns: [ 62.33333333 -62.33333333]\n",
            "Finished episode 9, avg returns: [ 60.3 -60.3]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 28: predbot ===\n",
            "Finished episode 0, avg returns: [ 77. -77.]\n",
            "Finished episode 1, avg returns: [ 66. -66.]\n",
            "Finished episode 2, avg returns: [ 59. -59.]\n",
            "Finished episode 3, avg returns: [ 68.25 -68.25]\n",
            "Finished episode 4, avg returns: [ 77.4 -77.4]\n",
            "Finished episode 5, avg returns: [ 77.83333333 -77.83333333]\n",
            "Finished episode 6, avg returns: [ 81.14285714 -81.14285714]\n",
            "Finished episode 7, avg returns: [ 81.625 -81.625]\n",
            "Finished episode 8, avg returns: [ 79.11111111 -79.11111111]\n",
            "Finished episode 9, avg returns: [ 80.1 -80.1]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 18: marble ===\n",
            "Finished episode 0, avg returns: [ 124. -124.]\n",
            "Finished episode 1, avg returns: [ 101. -101.]\n",
            "Finished episode 2, avg returns: [ 89.66666667 -89.66666667]\n",
            "Finished episode 3, avg returns: [ 99.25 -99.25]\n",
            "Finished episode 4, avg returns: [ 96.2 -96.2]\n",
            "Finished episode 5, avg returns: [ 87.33333333 -87.33333333]\n",
            "Finished episode 6, avg returns: [ 83.42857143 -83.42857143]\n",
            "Finished episode 7, avg returns: [ 87.125 -87.125]\n",
            "Finished episode 8, avg returns: [ 86.66666667 -86.66666667]\n",
            "Finished episode 9, avg returns: [ 83.5 -83.5]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 36: sunCrazybot ===\n",
            "Finished episode 0, avg returns: [ 92. -92.]\n",
            "Finished episode 1, avg returns: [ 79.5 -79.5]\n",
            "Finished episode 2, avg returns: [ 89. -89.]\n",
            "Finished episode 3, avg returns: [ 92.5 -92.5]\n",
            "Finished episode 4, avg returns: [ 94.6 -94.6]\n",
            "Finished episode 5, avg returns: [ 97.66666667 -97.66666667]\n",
            "Finished episode 6, avg returns: [ 92.85714286 -92.85714286]\n",
            "Finished episode 7, avg returns: [ 95.5 -95.5]\n",
            "Finished episode 8, avg returns: [ 92.33333333 -92.33333333]\n",
            "Finished episode 9, avg returns: [ 93. -93.]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 24: peterbot ===\n",
            "Finished episode 0, avg returns: [ 47. -47.]\n",
            "Finished episode 1, avg returns: [ 22.5 -22.5]\n",
            "Finished episode 2, avg returns: [ 15.33333333 -15.33333333]\n",
            "Finished episode 3, avg returns: [ 23. -23.]\n",
            "Finished episode 4, avg returns: [ 32.2 -32.2]\n",
            "Finished episode 5, avg returns: [ 37.83333333 -37.83333333]\n",
            "Finished episode 6, avg returns: [ 45.71428571 -45.71428571]\n",
            "Finished episode 7, avg returns: [ 51. -51.]\n",
            "Finished episode 8, avg returns: [ 45.66666667 -45.66666667]\n",
            "Finished episode 9, avg returns: [ 46.2 -46.2]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 4: antirotnbot ===\n",
            "Finished episode 0, avg returns: [ 44. -44.]\n",
            "Finished episode 1, avg returns: [ 44.5 -44.5]\n",
            "Finished episode 2, avg returns: [ 47.66666667 -47.66666667]\n",
            "Finished episode 3, avg returns: [ 48.75 -48.75]\n",
            "Finished episode 4, avg returns: [ 55. -55.]\n",
            "Finished episode 5, avg returns: [ 53.16666667 -53.16666667]\n",
            "Finished episode 6, avg returns: [ 53.14285714 -53.14285714]\n",
            "Finished episode 7, avg returns: [ 51.875 -51.875]\n",
            "Finished episode 8, avg returns: [ 50.55555556 -50.55555556]\n",
            "Finished episode 9, avg returns: [ 51.1 -51.1]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 40: switchbot ===\n",
            "Finished episode 0, avg returns: [ 84. -84.]\n",
            "Finished episode 1, avg returns: [ 85.5 -85.5]\n",
            "Finished episode 2, avg returns: [ 97.66666667 -97.66666667]\n",
            "Finished episode 3, avg returns: [ 104. -104.]\n",
            "Finished episode 4, avg returns: [ 101. -101.]\n",
            "Finished episode 5, avg returns: [ 91.16666667 -91.16666667]\n",
            "Finished episode 6, avg returns: [ 95.57142857 -95.57142857]\n",
            "Finished episode 7, avg returns: [ 94.625 -94.625]\n",
            "Finished episode 8, avg returns: [ 91.22222222 -91.22222222]\n",
            "Finished episode 9, avg returns: [ 94.9 -94.9]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 42: zq_move ===\n",
            "Finished episode 0, avg returns: [ 76. -76.]\n",
            "Finished episode 1, avg returns: [ 89. -89.]\n",
            "Finished episode 2, avg returns: [ 98. -98.]\n",
            "Finished episode 3, avg returns: [ 102.25 -102.25]\n",
            "Finished episode 4, avg returns: [ 105.6 -105.6]\n",
            "Finished episode 5, avg returns: [ 105.33333333 -105.33333333]\n",
            "Finished episode 6, avg returns: [ 106.28571429 -106.28571429]\n",
            "Finished episode 7, avg returns: [ 111.5 -111.5]\n",
            "Finished episode 8, avg returns: [ 115.88888889 -115.88888889]\n",
            "Finished episode 9, avg returns: [ 118.9 -118.9]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 21: mixed_strategy ===\n",
            "Finished episode 0, avg returns: [ 59. -59.]\n",
            "Finished episode 1, avg returns: [ 51.5 -51.5]\n",
            "Finished episode 2, avg returns: [ 42. -42.]\n",
            "Finished episode 3, avg returns: [ 48.5 -48.5]\n",
            "Finished episode 4, avg returns: [ 58.6 -58.6]\n",
            "Finished episode 5, avg returns: [ 62.66666667 -62.66666667]\n",
            "Finished episode 6, avg returns: [ 64.28571429 -64.28571429]\n",
            "Finished episode 7, avg returns: [ 67.875 -67.875]\n",
            "Finished episode 8, avg returns: [ 65.44444444 -65.44444444]\n",
            "Finished episode 9, avg returns: [ 62.1 -62.1]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 2: addshiftbot3 ===\n",
            "Finished episode 0, avg returns: [ 92. -92.]\n",
            "Finished episode 1, avg returns: [ 97. -97.]\n",
            "Finished episode 2, avg returns: [ 102. -102.]\n",
            "Finished episode 3, avg returns: [ 103.25 -103.25]\n",
            "Finished episode 4, avg returns: [ 99.8 -99.8]\n",
            "Finished episode 5, avg returns: [ 99.16666667 -99.16666667]\n",
            "Finished episode 6, avg returns: [ 100.14285714 -100.14285714]\n",
            "Finished episode 7, avg returns: [ 103.25 -103.25]\n",
            "Finished episode 8, avg returns: [ 101.88888889 -101.88888889]\n",
            "Finished episode 9, avg returns: [ 101.9 -101.9]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 23: multibot ===\n",
            "Finished episode 0, avg returns: [ 122. -122.]\n",
            "Finished episode 1, avg returns: [ 103. -103.]\n",
            "Finished episode 2, avg returns: [ 94. -94.]\n",
            "Finished episode 3, avg returns: [ 91.75 -91.75]\n",
            "Finished episode 4, avg returns: [ 86.4 -86.4]\n",
            "Finished episode 5, avg returns: [ 93.5 -93.5]\n",
            "Finished episode 6, avg returns: [ 91.71428571 -91.71428571]\n",
            "Finished episode 7, avg returns: [ 97.375 -97.375]\n",
            "Finished episode 8, avg returns: [ 97.11111111 -97.11111111]\n",
            "Finished episode 9, avg returns: [ 96.3 -96.3]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 7: copybot ===\n",
            "Finished episode 0, avg returns: [ 308. -308.]\n",
            "Finished episode 1, avg returns: [ 282. -282.]\n",
            "Finished episode 2, avg returns: [ 286.33333333 -286.33333333]\n",
            "Finished episode 3, avg returns: [ 278.5 -278.5]\n",
            "Finished episode 4, avg returns: [ 272.8 -272.8]\n",
            "Finished episode 5, avg returns: [ 279.66666667 -279.66666667]\n",
            "Finished episode 6, avg returns: [ 281. -281.]\n",
            "Finished episode 7, avg returns: [ 282.5 -282.5]\n",
            "Finished episode 8, avg returns: [ 281.55555556 -281.55555556]\n",
            "Finished episode 9, avg returns: [ 283.7 -283.7]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 3: antiflatbot ===\n",
            "Finished episode 0, avg returns: [ 405. -405.]\n",
            "Finished episode 1, avg returns: [ 314. -314.]\n",
            "Finished episode 2, avg returns: [ 277. -277.]\n",
            "Finished episode 3, avg returns: [ 253.5 -253.5]\n",
            "Finished episode 4, avg returns: [ 288.6 -288.6]\n",
            "Finished episode 5, avg returns: [ 307.33333333 -307.33333333]\n",
            "Finished episode 6, avg returns: [ 290.85714286 -290.85714286]\n",
            "Finished episode 7, avg returns: [ 284. -284.]\n",
            "Finished episode 8, avg returns: [ 276.33333333 -276.33333333]\n",
            "Finished episode 9, avg returns: [ 267.3 -267.3]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 33: rotatebot ===\n",
            "Finished episode 0, avg returns: [ 297. -297.]\n",
            "Finished episode 1, avg returns: [ 302.5 -302.5]\n",
            "Finished episode 2, avg returns: [ 305.66666667 -305.66666667]\n",
            "Finished episode 3, avg returns: [ 307.25 -307.25]\n",
            "Finished episode 4, avg returns: [ 304.4 -304.4]\n",
            "Finished episode 5, avg returns: [ 305.66666667 -305.66666667]\n",
            "Finished episode 6, avg returns: [ 307.71428571 -307.71428571]\n",
            "Finished episode 7, avg returns: [ 312. -312.]\n",
            "Finished episode 8, avg returns: [ 314. -314.]\n",
            "Finished episode 9, avg returns: [ 311.1 -311.1]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 12: freqbot2 ===\n",
            "Finished episode 0, avg returns: [ 208. -208.]\n",
            "Finished episode 1, avg returns: [ 217. -217.]\n",
            "Finished episode 2, avg returns: [ 228.66666667 -228.66666667]\n",
            "Finished episode 3, avg returns: [ 225.5 -225.5]\n",
            "Finished episode 4, avg returns: [ 228. -228.]\n",
            "Finished episode 5, avg returns: [ 225.33333333 -225.33333333]\n",
            "Finished episode 6, avg returns: [ 225.57142857 -225.57142857]\n",
            "Finished episode 7, avg returns: [ 225.375 -225.375]\n",
            "Finished episode 8, avg returns: [ 229.44444444 -229.44444444]\n",
            "Finished episode 9, avg returns: [ 228.5 -228.5]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 32: rockbot ===\n",
            "Finished episode 0, avg returns: [ 379. -379.]\n",
            "Finished episode 1, avg returns: [ 354. -354.]\n",
            "Finished episode 2, avg returns: [ 364. -364.]\n",
            "Finished episode 3, avg returns: [ 365.5 -365.5]\n",
            "Finished episode 4, avg returns: [ 360.8 -360.8]\n",
            "Finished episode 5, avg returns: [ 362.33333333 -362.33333333]\n",
            "Finished episode 6, avg returns: [ 358.85714286 -358.85714286]\n",
            "Finished episode 7, avg returns: [ 364.25 -364.25]\n",
            "Finished episode 8, avg returns: [ 367. -367.]\n",
            "Finished episode 9, avg returns: [ 364.5 -364.5]\n",
            "\n",
            "=== In-Sample Performance Ranking (high â†’ low) ===\n",
            "rockbot              score=364.50\n",
            "rotatebot            score=311.10\n",
            "copybot              score=283.70\n",
            "antiflatbot          score=267.30\n",
            "freqbot2             score=228.50\n",
            "zq_move              score=118.90\n",
            "addshiftbot3         score=101.90\n",
            "multibot             score=96.30\n",
            "switchbot            score=94.90\n",
            "sunCrazybot          score=93.00\n",
            "marble               score=83.50\n",
            "predbot              score=80.10\n",
            "halbot               score=65.10\n",
            "driftbot             score=63.70\n",
            "r226bot              score=62.40\n",
            "mixed_strategy       score=62.10\n",
            "granite              score=60.30\n",
            "inocencio            score=58.40\n",
            "antirotnbot          score=51.10\n",
            "flatbot3             score=50.90\n",
            "foxtrotbot           score=48.50\n",
            "adddriftbot2         score=48.40\n",
            "switchalot           score=47.00\n",
            "peterbot             score=46.20\n",
            "piedra               score=42.50\n",
            "sweetrock            score=39.30\n",
            "robertot             score=29.80\n",
            "mod1bot              score=24.40\n",
            "sunNervebot          score=19.90\n",
            "biopic               score=16.90\n",
            "textbot              score=16.00\n",
            "markov5              score=15.00\n",
            "boom                 score=13.60\n",
            "russrocker4          score=9.00\n",
            "iocainebot           score=7.90\n",
            "shofar               score=5.50\n",
            "randbot              score=4.60\n",
            "pibot                score=3.30\n",
            "greenberg            score=3.10\n",
            "phasenbott           score=0.40\n",
            "debruijn81           score=-0.90\n",
            "markovbails          score=-2.00\n",
            "actr_lag2_decay      score=-4.00\n"
          ]
        }
      ],
      "source": [
        "# In-sample bot IDs (0â€“19)\n",
        "oos_bot_ids = [x for x,_ in sorted(zip(roshambo_bot_ids.values(), bot_strength), key=lambda x: x[1], reverse=True)]\n",
        "\n",
        "results_oos = {}\n",
        "\n",
        "for bot_id in oos_bot_ids:\n",
        "    bot_name = roshambo_bot_names[bot_id]\n",
        "    opp = create_roshambo_bot_agent(1, num_actions, roshambo_bot_names, bot_id)\n",
        "\n",
        "    agents = [agent, opp]\n",
        "\n",
        "    print(f\"\\n=== Evaluating vs IN-SAMPLE bot {bot_id}: {bot_name} ===\")\n",
        "    avg = eval_agents(env, agents, num_players, 10, verbose=True)\n",
        "    results_oos[bot_name] = avg[0]\n",
        "\n",
        "# Print sorted results\n",
        "print(\"\\n=== In-Sample Performance Ranking (high â†’ low) ===\")\n",
        "for name, score in sorted(results_oos.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{name:20s} score={score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yuxKGD8l5Wc4",
        "outputId": "b57f778e-5e0f-43e5-abe5-6ee8e8809893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 0: actr_lag2_decay ===\n",
            "Finished episode 0, avg returns: [-18.  18.]\n",
            "Finished episode 1, avg returns: [-25.5  25.5]\n",
            "Finished episode 2, avg returns: [-17.33333333  17.33333333]\n",
            "Finished episode 3, avg returns: [-18.25  18.25]\n",
            "Finished episode 4, avg returns: [-15.4  15.4]\n",
            "Finished episode 5, avg returns: [-11.83333333  11.83333333]\n",
            "Finished episode 6, avg returns: [-10.28571429  10.28571429]\n",
            "Finished episode 7, avg returns: [-16.75  16.75]\n",
            "Finished episode 8, avg returns: [-20.11111111  20.11111111]\n",
            "Finished episode 9, avg returns: [-18.9  18.9]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 1: adddriftbot2 ===\n",
            "Finished episode 0, avg returns: [-33.  33.]\n",
            "Finished episode 1, avg returns: [-27.  27.]\n",
            "Finished episode 2, avg returns: [-11.66666667  11.66666667]\n",
            "Finished episode 3, avg returns: [-10.  10.]\n",
            "Finished episode 4, avg returns: [-15.4  15.4]\n",
            "Finished episode 5, avg returns: [-16.83333333  16.83333333]\n",
            "Finished episode 6, avg returns: [-15.14285714  15.14285714]\n",
            "Finished episode 7, avg returns: [-13.5  13.5]\n",
            "Finished episode 8, avg returns: [-7.22222222  7.22222222]\n",
            "Finished episode 9, avg returns: [-4.  4.]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 2: addshiftbot3 ===\n",
            "Finished episode 0, avg returns: [-76.  76.]\n",
            "Finished episode 1, avg returns: [-32.5  32.5]\n",
            "Finished episode 2, avg returns: [-26.  26.]\n",
            "Finished episode 3, avg returns: [-20.5  20.5]\n",
            "Finished episode 4, avg returns: [-18.2  18.2]\n",
            "Finished episode 5, avg returns: [-9.5  9.5]\n",
            "Finished episode 6, avg returns: [-12.14285714  12.14285714]\n",
            "Finished episode 7, avg returns: [-8.125  8.125]\n",
            "Finished episode 8, avg returns: [-7.44444444  7.44444444]\n",
            "Finished episode 9, avg returns: [-6.7  6.7]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 3: antiflatbot ===\n",
            "Finished episode 0, avg returns: [-203.  203.]\n",
            "Finished episode 1, avg returns: [-72.  72.]\n",
            "Finished episode 2, avg returns: [-36.33333333  36.33333333]\n",
            "Finished episode 3, avg returns: [-1.25  1.25]\n",
            "Finished episode 4, avg returns: [ 22.4 -22.4]\n",
            "Finished episode 5, avg returns: [-4.5  4.5]\n",
            "Finished episode 6, avg returns: [ 1.42857143 -1.42857143]\n",
            "Finished episode 7, avg returns: [ 7.375 -7.375]\n",
            "Finished episode 8, avg returns: [ 3.22222222 -3.22222222]\n",
            "Finished episode 9, avg returns: [-8.7  8.7]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 4: antirotnbot ===\n",
            "Finished episode 0, avg returns: [-31.  31.]\n",
            "Finished episode 1, avg returns: [ 1.5 -1.5]\n",
            "Finished episode 2, avg returns: [ 6. -6.]\n",
            "Finished episode 3, avg returns: [ 5.5 -5.5]\n",
            "Finished episode 4, avg returns: [-1.  1.]\n",
            "Finished episode 5, avg returns: [ 2.16666667 -2.16666667]\n",
            "Finished episode 6, avg returns: [ 9.28571429 -9.28571429]\n",
            "Finished episode 7, avg returns: [ 12.25 -12.25]\n",
            "Finished episode 8, avg returns: [ 13. -13.]\n",
            "Finished episode 9, avg returns: [ 11.7 -11.7]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 5: biopic ===\n",
            "Finished episode 0, avg returns: [-7.  7.]\n",
            "Finished episode 1, avg returns: [-3.  3.]\n",
            "Finished episode 2, avg returns: [ 7. -7.]\n",
            "Finished episode 3, avg returns: [-2.25  2.25]\n",
            "Finished episode 4, avg returns: [-5.4  5.4]\n",
            "Finished episode 5, avg returns: [-8.5  8.5]\n",
            "Finished episode 6, avg returns: [-3.57142857  3.57142857]\n",
            "Finished episode 7, avg returns: [ 1.375 -1.375]\n",
            "Finished episode 8, avg returns: [-0.22222222  0.22222222]\n",
            "Finished episode 9, avg returns: [ 0.9 -0.9]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 6: boom ===\n",
            "Finished episode 0, avg returns: [ 11. -11.]\n",
            "Finished episode 1, avg returns: [-23.5  23.5]\n",
            "Finished episode 2, avg returns: [-8.66666667  8.66666667]\n",
            "Finished episode 3, avg returns: [ 3.5 -3.5]\n",
            "Finished episode 4, avg returns: [ 10.6 -10.6]\n",
            "Finished episode 5, avg returns: [ 10.66666667 -10.66666667]\n",
            "Finished episode 6, avg returns: [ 4.28571429 -4.28571429]\n",
            "Finished episode 7, avg returns: [ 8.125 -8.125]\n",
            "Finished episode 8, avg returns: [ 4.55555556 -4.55555556]\n",
            "Finished episode 9, avg returns: [-0.1  0.1]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 7: copybot ===\n",
            "Finished episode 0, avg returns: [-32.  32.]\n",
            "Finished episode 1, avg returns: [-33.5  33.5]\n",
            "Finished episode 2, avg returns: [-34.66666667  34.66666667]\n",
            "Finished episode 3, avg returns: [-29.75  29.75]\n",
            "Finished episode 4, avg returns: [-26.2  26.2]\n",
            "Finished episode 5, avg returns: [-30.33333333  30.33333333]\n",
            "Finished episode 6, avg returns: [-29.57142857  29.57142857]\n",
            "Finished episode 7, avg returns: [-24.875  24.875]\n",
            "Finished episode 8, avg returns: [-24.11111111  24.11111111]\n",
            "Finished episode 9, avg returns: [-24.1  24.1]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 8: debruijn81 ===\n",
            "Finished episode 0, avg returns: [ 23. -23.]\n",
            "Finished episode 1, avg returns: [-9.  9.]\n",
            "Finished episode 2, avg returns: [-15.66666667  15.66666667]\n",
            "Finished episode 3, avg returns: [-16.5  16.5]\n",
            "Finished episode 4, avg returns: [-9.6  9.6]\n",
            "Finished episode 5, avg returns: [-10.  10.]\n",
            "Finished episode 6, avg returns: [-6.57142857  6.57142857]\n",
            "Finished episode 7, avg returns: [-1.25  1.25]\n",
            "Finished episode 8, avg returns: [-0.66666667  0.66666667]\n",
            "Finished episode 9, avg returns: [-3.5  3.5]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 9: driftbot ===\n",
            "Finished episode 0, avg returns: [-5.  5.]\n",
            "Finished episode 1, avg returns: [ 5.5 -5.5]\n",
            "Finished episode 2, avg returns: [ 20.66666667 -20.66666667]\n",
            "Finished episode 3, avg returns: [ 18. -18.]\n",
            "Finished episode 4, avg returns: [ 16.6 -16.6]\n",
            "Finished episode 5, avg returns: [ 23.83333333 -23.83333333]\n",
            "Finished episode 6, avg returns: [ 26.42857143 -26.42857143]\n",
            "Finished episode 7, avg returns: [ 22.75 -22.75]\n",
            "Finished episode 8, avg returns: [ 21.88888889 -21.88888889]\n",
            "Finished episode 9, avg returns: [ 20.1 -20.1]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 10: flatbot3 ===\n",
            "Finished episode 0, avg returns: [-5.  5.]\n",
            "Finished episode 1, avg returns: [ 21.5 -21.5]\n",
            "Finished episode 2, avg returns: [ 32.33333333 -32.33333333]\n",
            "Finished episode 3, avg returns: [ 29.5 -29.5]\n",
            "Finished episode 4, avg returns: [ 11.8 -11.8]\n",
            "Finished episode 5, avg returns: [ 0.16666667 -0.16666667]\n",
            "Finished episode 6, avg returns: [-4.85714286  4.85714286]\n",
            "Finished episode 7, avg returns: [-2.75  2.75]\n",
            "Finished episode 8, avg returns: [-4.22222222  4.22222222]\n",
            "Finished episode 9, avg returns: [-4.1  4.1]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 11: foxtrotbot ===\n",
            "Finished episode 0, avg returns: [ 23. -23.]\n",
            "Finished episode 1, avg returns: [ 12.5 -12.5]\n",
            "Finished episode 2, avg returns: [ 5.33333333 -5.33333333]\n",
            "Finished episode 3, avg returns: [ 3. -3.]\n",
            "Finished episode 4, avg returns: [ 0.4 -0.4]\n",
            "Finished episode 5, avg returns: [ 3.16666667 -3.16666667]\n",
            "Finished episode 6, avg returns: [ 4.42857143 -4.42857143]\n",
            "Finished episode 7, avg returns: [-0.875  0.875]\n",
            "Finished episode 8, avg returns: [-3.88888889  3.88888889]\n",
            "Finished episode 9, avg returns: [-2.9  2.9]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 12: freqbot2 ===\n",
            "Finished episode 0, avg returns: [-49.  49.]\n",
            "Finished episode 1, avg returns: [-27.  27.]\n",
            "Finished episode 2, avg returns: [-37.66666667  37.66666667]\n",
            "Finished episode 3, avg returns: [-41.75  41.75]\n",
            "Finished episode 4, avg returns: [-35.  35.]\n",
            "Finished episode 5, avg returns: [-43.5  43.5]\n",
            "Finished episode 6, avg returns: [-46.14285714  46.14285714]\n",
            "Finished episode 7, avg returns: [-50.625  50.625]\n",
            "Finished episode 8, avg returns: [-49.44444444  49.44444444]\n",
            "Finished episode 9, avg returns: [-48.6  48.6]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 13: granite ===\n",
            "Finished episode 0, avg returns: [ 25. -25.]\n",
            "Finished episode 1, avg returns: [ 2. -2.]\n",
            "Finished episode 2, avg returns: [ 6.33333333 -6.33333333]\n",
            "Finished episode 3, avg returns: [ 0.75 -0.75]\n",
            "Finished episode 4, avg returns: [-6.4  6.4]\n",
            "Finished episode 5, avg returns: [-2.16666667  2.16666667]\n",
            "Finished episode 6, avg returns: [-1.71428571  1.71428571]\n",
            "Finished episode 7, avg returns: [-2.375  2.375]\n",
            "Finished episode 8, avg returns: [-3.55555556  3.55555556]\n",
            "Finished episode 9, avg returns: [-6.9  6.9]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 14: greenberg ===\n",
            "Finished episode 0, avg returns: [-25.  25.]\n",
            "Finished episode 1, avg returns: [-27.5  27.5]\n",
            "Finished episode 2, avg returns: [-6.33333333  6.33333333]\n",
            "Finished episode 3, avg returns: [-22.  22.]\n",
            "Finished episode 4, avg returns: [-14.  14.]\n",
            "Finished episode 5, avg returns: [-11.66666667  11.66666667]\n",
            "Finished episode 6, avg returns: [-10.42857143  10.42857143]\n",
            "Finished episode 7, avg returns: [-11.875  11.875]\n",
            "Finished episode 8, avg returns: [-11.55555556  11.55555556]\n",
            "Finished episode 9, avg returns: [-10.9  10.9]\n",
            "\n",
            "=== Evaluating vs IN-SAMPLE bot 15: halbot ===\n",
            "Finished episode 0, avg returns: [-14.  14.]\n",
            "Finished episode 1, avg returns: [-16.  16.]\n",
            "Finished episode 2, avg returns: [-8.  8.]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1700656717.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n=== Evaluating vs IN-SAMPLE bot {bot_id}: {bot_name} ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_agents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_players\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mresults_oos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbot_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2314446399.py\u001b[0m in \u001b[0;36meval_agents\u001b[0;34m(env, agents, num_players, num_episodes, verbose)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m       agents_output = [\n\u001b[0;32m---> 59\u001b[0;31m           \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_evaluation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m       ]\n\u001b[1;32m     61\u001b[0m       \u001b[0maction_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magents_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-594180898.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, time_step, is_evaluation)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# --- policy forward ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         logits = self.policy(\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mh0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mpred_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1458581025.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h0, h1, hpred, pred_logits, my_b, opp_b)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# --- little linear head for flexibility ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1949\u001b[0m     \u001b[0;31m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m     \u001b[0;31m# https://github.com/pytorch/pytorch/pull/115074\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1951\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Module\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1952\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_parameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1953\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Flipped order\n",
        "agent.player_id = 1\n",
        "oos_bot_ids = list(range(0, 43))\n",
        "\n",
        "results_oos = {}\n",
        "\n",
        "for bot_id in oos_bot_ids:\n",
        "    bot_name = roshambo_bot_names[bot_id]\n",
        "    opp = create_roshambo_bot_agent(1, num_actions, roshambo_bot_names, bot_id)\n",
        "\n",
        "    agents = [opp, agent]\n",
        "\n",
        "    print(f\"\\n=== Evaluating vs IN-SAMPLE bot {bot_id}: {bot_name} ===\")\n",
        "    avg = eval_agents(env, agents, num_players, 10, verbose=True)\n",
        "    results_oos[bot_name] = avg[1]\n",
        "\n",
        "# Print sorted results\n",
        "print(\"\\n=== In-Sample Performance Ranking (high â†’ low) ===\")\n",
        "for name, score in sorted(results_oos.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{name:20s} score={score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOAT0Tf3WhHj",
        "outputId": "8ce511f4-9c69-4406-89a5-d05afdbee1c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 20: markovbails ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 21: mixed_strategy ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 22: mod1bot ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 23: multibot ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 24: peterbot ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 25: phasenbott ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 26: pibot ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 27: piedra ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 28: predbot ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 29: r226bot ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 30: randbot ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 31: robertot ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 32: rockbot ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 33: rotatebot ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 34: russrocker4 ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 35: shofar ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 36: sunCrazybot ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 37: sunNervebot ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 38: sweetrock ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 39: switchalot ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 40: switchbot ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 41: textbot ===\n",
            "\n",
            "=== Evaluating vs OUT-OF-SAMPLE bot 42: zq_move ===\n",
            "\n",
            "=== Out of Sample Performance Ranking (high â†’ low) ===\n",
            "rotatebot            score=338.00\n",
            "rockbot              score=157.25\n",
            "zq_move              score=106.25\n",
            "multibot             score=98.25\n",
            "r226bot              score=85.00\n",
            "sunCrazybot          score=74.25\n",
            "predbot              score=66.00\n",
            "switchbot            score=64.00\n",
            "piedra               score=50.50\n",
            "sweetrock            score=45.00\n",
            "mixed_strategy       score=35.75\n",
            "switchalot           score=27.50\n",
            "robertot             score=25.25\n",
            "textbot              score=21.75\n",
            "peterbot             score=21.50\n",
            "randbot              score=14.75\n",
            "russrocker4          score=11.25\n",
            "sunNervebot          score=11.25\n",
            "mod1bot              score=10.50\n",
            "shofar               score=6.50\n",
            "pibot                score=-10.00\n",
            "phasenbott           score=-13.00\n",
            "markovbails          score=-30.75\n"
          ]
        }
      ],
      "source": [
        "# Out-of-sample bot IDs (20â€“42)\n",
        "oos_bot_ids = list(range(20, 43))\n",
        "\n",
        "results_oos = {}\n",
        "\n",
        "for bot_id in oos_bot_ids:\n",
        "    bot_name = roshambo_bot_names[bot_id]\n",
        "    opp = create_roshambo_bot_agent(1, num_actions, roshambo_bot_names, bot_id)\n",
        "\n",
        "    agents = [agent, opp]\n",
        "\n",
        "    print(f\"\\n=== Evaluating vs OUT-OF-SAMPLE bot {bot_id}: {bot_name} ===\")\n",
        "    avg = eval_agents(env, agents, num_players, 4, verbose=False)\n",
        "    results_oos[bot_name] = avg[0]\n",
        "\n",
        "# Print sorted results\n",
        "print(\"\\n=== Out of Sample Performance Ranking (high â†’ low) ===\")\n",
        "for name, score in sorted(results_oos.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{name:20s} score={score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15sjtwWmQlv4"
      },
      "source": [
        "# Performance Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEVVdQroQlYG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_heatmap(history):\n",
        "    # history: list of lists (scores vs 43 bots)\n",
        "    arr = np.array(history)\n",
        "    plt.figure(figsize=(10,7))\n",
        "    plt.imshow(arr.T, aspect='auto', cmap='coolwarm', vmin=-1, vmax=1)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"Evaluation iteration\")\n",
        "    plt.ylabel(\"Bot ID\")\n",
        "    plt.title(\"Performance vs Bot Population Over Time\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFYAqHbcQ1b3"
      },
      "outputs": [],
      "source": [
        "def update_elo(my_elo, opp_elo, result, k=32):\n",
        "    # result: win=1, draw=0.5, lose=0\n",
        "    expect = 1 / (1 + 10 ** ((opp_elo - my_elo) / 400))\n",
        "    return my_elo + k * (result - expect)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7641XNaQ_pE"
      },
      "outputs": [],
      "source": [
        "def entropy(p):\n",
        "    return -(p * torch.log(p + 1e-12)).sum().item()\n",
        "\n",
        "id_entropies = []\n",
        "\n",
        "# Every episode:\n",
        "id_entropies.append(entropy(agent.opp_belief.belief))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy1BZENJRBBg"
      },
      "outputs": [],
      "source": [
        "plt.plot(id_entropies)\n",
        "plt.title(\"Opponent Identity Entropy Over Time\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Entropy\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_WoaBbAShpq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "class RPSPerformanceMonitor:\n",
        "    def __init__(self, env, agent, bot_names, num_bots=43, num_actions=3):\n",
        "        self.env = env\n",
        "        self.agent = agent\n",
        "        self.bot_names = bot_names\n",
        "        self.num_bots = num_bots\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # -----------------------------------------\n",
        "        # Fixed-size buffers\n",
        "        # -----------------------------------------\n",
        "        self.max_latent      = 5000\n",
        "        self.max_entropy     = 2000\n",
        "        self.max_cycles      = 2000\n",
        "        self.max_population  = 200\n",
        "        self.max_elo         = 2000\n",
        "\n",
        "        # tracking storage\n",
        "        self.performance_log = []          # small already\n",
        "        self.population_history = []       # (eval snapshots)\n",
        "        self.id_entropy_log = []\n",
        "        self.cycle_log = []\n",
        "        self.elo_log = []\n",
        "        self.latent_log = []\n",
        "\n",
        "        # ELO scores\n",
        "        self.agent_elo = 1500\n",
        "        self.bot_elos = [1500]*num_bots\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # Helper: ring buffer append\n",
        "    # -----------------------------------------\n",
        "    def _append_limited(self, arr, x, max_len):\n",
        "        arr.append(x)\n",
        "        if len(arr) > max_len:\n",
        "            arr.pop(0)\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # Utility functions\n",
        "    # -----------------------------------------\n",
        "    def _entropy(self, p):\n",
        "        return -(p * torch.log(p + 1e-12)).sum().item()\n",
        "\n",
        "    def _update_elo(self, my_elo, opp_elo, result, k=32):\n",
        "        expected = 1 / (1 + 10 ** ((opp_elo - my_elo) / 400))\n",
        "        return my_elo + k * (result - expected)\n",
        "\n",
        "    def _cycle_strength(self, actions):\n",
        "        if len(actions) < 2:\n",
        "            return 0\n",
        "        score = 0\n",
        "        for i in range(1, len(actions)):\n",
        "            prev = actions[i-1]; cur = actions[i]\n",
        "            if (prev + 1) % 3 == cur:\n",
        "                score += 1\n",
        "            elif (prev + 2) % 3 == cur:\n",
        "                score -= 1\n",
        "        return score / len(actions)\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # Episode collection + evaluations\n",
        "    # -----------------------------------------\n",
        "    def collect_and_log(self, iteration, bot_id, num_eps=5):\n",
        "        opp = create_roshambo_bot_agent(\n",
        "            1, self.num_actions, self.bot_names, bot_id\n",
        "        )\n",
        "\n",
        "        returns = []\n",
        "        latent_batch = []\n",
        "        actions_batch = []\n",
        "\n",
        "        for ep in range(num_eps):\n",
        "            self.agent.restart()\n",
        "            opp.restart()\n",
        "            ts = self.env.reset()\n",
        "            ep_return = 0\n",
        "\n",
        "            while not ts.last():\n",
        "\n",
        "                outs = [self.agent.step(ts), opp.step(ts)]\n",
        "                actions = [outs[0].action, outs[1].action]\n",
        "                ts_next = self.env.step(actions)\n",
        "                reward = ts_next.rewards[0]\n",
        "                ep_return += reward\n",
        "\n",
        "                # saved values from MyAgent.forward()\n",
        "                h0, h1, hpred, pred_logits, my_b, opp_b, last_probs = self.agent.saved_last\n",
        "                latent_batch.append(torch.cat([h0, h1, hpred]).cpu().numpy())\n",
        "                actions_batch.append(actions[0])\n",
        "\n",
        "                ts = ts_next\n",
        "\n",
        "            returns.append(ep_return)\n",
        "\n",
        "        avg_return = sum(returns) / len(returns)\n",
        "\n",
        "        # identity entropy\n",
        "        ent = self._entropy(self.agent.opp_belief.belief)\n",
        "        self._append_limited(self.id_entropy_log, ent, self.max_entropy)\n",
        "\n",
        "        # cycle score\n",
        "        cycle_s = self._cycle_strength(actions_batch)\n",
        "        self._append_limited(self.cycle_log, cycle_s, self.max_cycles)\n",
        "\n",
        "        # latent vectors\n",
        "        for v in latent_batch:\n",
        "            self._append_limited(self.latent_log, v, self.max_latent)\n",
        "\n",
        "        return avg_return\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # After PPO update\n",
        "    # -----------------------------------------\n",
        "    def after_update(self, iteration, bot_id, avg_return, loss):\n",
        "        self.performance_log.append((iteration, bot_id, avg_return, loss))\n",
        "\n",
        "        # ELO update\n",
        "        if avg_return > 0:\n",
        "            result = 1\n",
        "        elif avg_return < 0:\n",
        "            result = 0\n",
        "        else:\n",
        "            result = 0.5\n",
        "\n",
        "        self.agent_elo = self._update_elo(self.agent_elo, self.bot_elos[bot_id], result)\n",
        "        self.bot_elos[bot_id] = self._update_elo(self.bot_elos[bot_id], self.agent_elo, 1-result)\n",
        "\n",
        "        self._append_limited(self.elo_log, self.agent_elo, self.max_elo)\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # Population eval\n",
        "    # -----------------------------------------\n",
        "    def evaluate_population(self, num_eps=3):\n",
        "        scores = []\n",
        "        for b in range(self.num_bots):\n",
        "            avg_ret = self.collect_and_log(0, b, num_eps)\n",
        "            scores.append(avg_ret)\n",
        "        self._append_limited(self.population_history, scores, self.max_population)\n",
        "        return scores\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # Plots\n",
        "    # -----------------------------------------\n",
        "    def plot_heatmap(self):\n",
        "        if not self.population_history:\n",
        "            print(\"No population eval yet.\")\n",
        "            return\n",
        "        arr = np.array(self.population_history)\n",
        "        plt.figure(figsize=(10,6))\n",
        "        plt.imshow(arr.T, aspect='auto', cmap='coolwarm', vmin=-1, vmax=1)\n",
        "        plt.colorbar()\n",
        "        plt.title(\"Performance vs All Bots Over Time\")\n",
        "        plt.xlabel(\"Evaluation Index\")\n",
        "        plt.ylabel(\"Bot ID\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_elo(self):\n",
        "        if not self.elo_log:\n",
        "            print(\"No ELO data yet.\")\n",
        "            return\n",
        "        plt.plot(self.elo_log)\n",
        "        plt.title(\"Agent ELO Over Time\")\n",
        "        plt.xlabel(\"Iteration\")\n",
        "        plt.ylabel(\"ELO\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_identity_entropy(self):\n",
        "        if not self.id_entropy_log:\n",
        "            print(\"No entropy data yet.\")\n",
        "            return\n",
        "        plt.plot(self.id_entropy_log)\n",
        "        plt.title(\"Opponent Identity Entropy\")\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Entropy\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_cycle_strength(self):\n",
        "        if not self.cycle_log:\n",
        "            print(\"No cycle data yet.\")\n",
        "            return\n",
        "        plt.plot(self.cycle_log)\n",
        "        plt.title(\"Cycle Strength Over Time\")\n",
        "        plt.xlabel(\"Iteration\")\n",
        "        plt.ylabel(\"Cycle Score\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_latent_pca(self, sample=3000):\n",
        "        if not self.latent_log:\n",
        "            print(\"No latent vectors yet.\")\n",
        "            return\n",
        "        X = np.array(self.latent_log[-sample:])\n",
        "        if len(X) < 2:\n",
        "            print(\"Not enough latent data for PCA.\")\n",
        "            return\n",
        "        p = PCA(2).fit_transform(X)\n",
        "        plt.scatter(p[:,0], p[:,1], s=3)\n",
        "        plt.title(\"Latent Drift (PCA of h0,h1,hpred)\")\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23j1srrZSlvR"
      },
      "outputs": [],
      "source": [
        "evaluator = RPSPerformanceMonitor(env, agent, roshambo_bot_names)\n",
        "\n",
        "for it in range(200):\n",
        "    bot_id = it % 43\n",
        "\n",
        "    episodes, avg_return = evaluator.collect_and_log(\n",
        "        iteration=it,\n",
        "        bot_id=bot_id,\n",
        "        num_eps=5\n",
        "    )\n",
        "\n",
        "    loss = online_ppo_update(agent.policy, optimizer, episodes)\n",
        "\n",
        "    evaluator.after_update(it, bot_id, avg_return, loss)\n",
        "\n",
        "    print(f\"[{it}] vs {bot_id}: return={avg_return:.3f} loss={loss:.3f}\")\n",
        "\n",
        "    # periodic wide eval\n",
        "    if it % 20 == 0:\n",
        "        print(\"Population eval...\")\n",
        "        scores = evaluator.evaluate_population(num_eps=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbDlG4eJSm7f"
      },
      "outputs": [],
      "source": [
        "evaluator.plot_elo()\n",
        "evaluator.plot_heatmap()\n",
        "evaluator.plot_identity_entropy()\n",
        "evaluator.plot_cycle_strength()\n",
        "evaluator.plot_latent_pca()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
